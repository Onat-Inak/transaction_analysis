{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb6dc7e0cb0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.LSTM import LSTM\n",
    "from models.RNN import RNN\n",
    "from models.GRU import GRU\n",
    "from models.LSTM import LSTM\n",
    "from models.transformer import TransformerClassifier\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import Dataset\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from tools.adjust_learning_rate import adjust_learning_rate\n",
    "from tools.train import train\n",
    "from tools.test import test\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "torch.manual_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of negative sequence array (OHE):  (100000, 10, 312)\n",
      "Shape of positive sequence array (OHE):  (11979, 10, 312)\n",
      "Shape of negative sequence array:  (100000, 10)\n",
      "Shape of positive sequence array:  (11979, 10)\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load 3D numpy matrices (user, time, transaction type)\n",
    "ns = np.load('data/neg_sequences.npy')\n",
    "ps = np.load('data/pos_sequences.npy')\n",
    "\n",
    "transaction_size = ns.shape[-1]\n",
    "\n",
    "# Take a look at the given data with OHE (one-hot encodings)\n",
    "print('Shape of negative sequence array (OHE): ', ns.shape)\n",
    "print('Shape of positive sequence array (OHE): ', ps.shape)\n",
    "\n",
    "# Convert one-hot encodings to integers:\n",
    "ns = np.argmax(ns, axis=2)\n",
    "ps = np.argmax(ps, axis=2)\n",
    "\n",
    "# Take a look at the given data\n",
    "print('Shape of negative sequence array: ', ns.shape)\n",
    "print('Shape of positive sequence array: ', ps.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "ns_label = np.zeros_like(ns[:,0])\n",
    "ps_label = np.ones_like(ps[:,0])\n",
    "\n",
    "# Concetenate positive sequences with negative sequences regarding users with correponding labels (axis=0)\n",
    "X = np.concatenate((ns, ps), axis=0)\n",
    "y = np.concatenate((ns_label, ps_label), axis=0) \n",
    "\n",
    "# Shuffle data and labels, for reproductivity set random_state=0\n",
    "# dataset, labels = shuffle(dataset, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train, test and validation ratios\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Split the data / Shuffle it and maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=42, shuffle=True)\n",
    "\n",
    "# Further split train_data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=val_ratio, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (90702, 10)  - y_train.shape:  (90702,)\n",
      "X_test.shape:  (11198, 10)  - y_test.shape:  (11198,)\n",
      "X_val.shape:  (10079, 10)  - y_val.shape:  (10079,)\n",
      "Training data -> num_pos_seq:  tensor([9724], device='cuda:0')  num_neg_seq:  tensor([80978], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Print train, test and validation dataset and label shapes\n",
    "print('X_train.shape: ', X_train.shape, ' - y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape, ' - y_test.shape: ', y_test.shape)\n",
    "print('X_val.shape: ', X_val.shape, ' - y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Convert numpy arrays to torch.tensor\n",
    "X_train, y_train = torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "X_train, y_train = X_train.to(device, dtype=torch.int32), y_train.to(device, dtype=torch.float32)\n",
    "X_test, y_test = torch.from_numpy(X_test), torch.from_numpy(y_test)\n",
    "X_test, y_test = X_test.to(device, dtype=torch.int32), y_test.to(device, dtype=torch.float32)\n",
    "X_val, y_val = torch.from_numpy(X_val), torch.from_numpy(y_val)\n",
    "X_val, y_val = X_val.to(device, dtype=torch.int32), y_val.to(device, dtype=torch.float32)\n",
    "\n",
    "# Number of positive sequences in training data\n",
    "num_pos_seq = torch.count_nonzero(y_train).view(1)\n",
    "num_neg_seq = (y_train.shape[0] - num_pos_seq).view(1)\n",
    "print('Training data -> num_pos_seq: ', num_pos_seq, ' num_neg_seq: ', num_neg_seq)\n",
    "\n",
    "# Create a custom dataset\n",
    "train_dataset = Dataset(X_train, y_train, device)\n",
    "test_dataset = Dataset(X_test, y_test, device)\n",
    "val_dataset = Dataset(X_val, y_val, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = 'RNN-Classifier'\n",
    "\n",
    "# # Initialize W&B for RNN-Classifier\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project = project_name,\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config = dict(\n",
    "#     project_name = project_name,\n",
    "#     batch_size = 900,\n",
    "#     transaction_size = transaction_size,\n",
    "#     embedding_dim = 32,\n",
    "#     hidden_dim = 64,\n",
    "#     num_layers = 1,\n",
    "#     device = device,\n",
    "#     batch_first = True,\n",
    "#     fc_hidden_dim = 64,\n",
    "#     num_classes = 1,\n",
    "#     num_epochs = 150,\n",
    "#     learning_rate = 1e-4,\n",
    "#     weight_decay = 0.0, \n",
    "#     lr_update_step = None,\n",
    "#     log_step = 20,\n",
    "#     lr_step_decay = False,\n",
    "#     gamma = 0.95,\n",
    "#     #pos_weight = num_neg_seq/num_pos_seq,\n",
    "#     pos_weight = 6.0,\n",
    "#     grad_clip = 1.0,\n",
    "#     apply_grad_clip = False,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # initialize config\n",
    "# config = wandb.config\n",
    "\n",
    "# model = RNN(config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "#             num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = 'GRU-Classifier'\n",
    "\n",
    "# # Initialize W&B for GRU-Classifier\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project = project_name,\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config = dict(\n",
    "#     project_name = project_name,\n",
    "#     batch_size = 900,\n",
    "#     transaction_size = transaction_size,\n",
    "#     embedding_dim = 32,\n",
    "#     hidden_dim = 64,\n",
    "#     num_layers = 1,\n",
    "#     device = device,\n",
    "#     batch_first = True,\n",
    "#     fc_hidden_dim = 64,\n",
    "#     num_classes = 1,\n",
    "#     num_epochs = 25,\n",
    "#     learning_rate = 1e-2,\n",
    "#     weight_decay = 0.0, \n",
    "#     lr_update_step = None,\n",
    "#     log_step = 20,\n",
    "#     lr_step_decay = False,\n",
    "#     gamma = 0.8,\n",
    "#     pos_weight = num_neg_seq/num_pos_seq,\n",
    "#     # pos_weight = 6.0,\n",
    "#     grad_clip = 1.0,\n",
    "#     apply_grad_clip = False,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # initialize config\n",
    "# config = wandb.config\n",
    "\n",
    "# model = GRU(config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "#             num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = 'LSTM-Classifier'\n",
    "\n",
    "# # Initialize W&B for LSTM-Classifier\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project = project_name,\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config= dict(\n",
    "#     project_name = project_name,\n",
    "#     batch_size = 900,\n",
    "#     transaction_size = transaction_size,\n",
    "#     embedding_dim = 64,\n",
    "#     hidden_dim = 128,\n",
    "#     num_layers = 5,\n",
    "#     device = device,\n",
    "#     batch_first = True,\n",
    "#     fc_hidden_dim = 128,\n",
    "#     num_classes = 1,\n",
    "#     num_epochs = 25,\n",
    "#     learning_rate = 1e-3,\n",
    "#     weight_decay = 0.0, \n",
    "#     lr_update_step = None,\n",
    "#     log_step = 10,\n",
    "#     lr_step_decay = False,\n",
    "#     gamma = 0.90,\n",
    "#     pos_weight = num_neg_seq/num_pos_seq,\n",
    "#     # pos_weight = 6.0,\n",
    "#     grad_clip = 1.0,\n",
    "#     apply_grad_clip = False,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # initialize config\n",
    "# config = wandb.config\n",
    "\n",
    "# model = LSTM(config.batch_size, config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "#             num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "# model.to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:k6eghjjg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc6e5785a0f4663874f93ea9ae3c297",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.378 MB uploaded\\r'), FloatProgress(value=0.004824340947035809, max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>f1_score_test</td><td>▁</td></tr><tr><td>f1_score_val</td><td>▁▆▇▆▇▇▇▇▇█▇██</td></tr><tr><td>learning_rate</td><td>████▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▂▁▂▁▂▁▁</td></tr><tr><td>val_accuracy</td><td>▅█▅▂▇▆▄▁▃█▂▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>f1_score_test</td><td>0.11373</td></tr><tr><td>f1_score_val</td><td>0.47076</td></tr><tr><td>learning_rate</td><td>0.00014</td></tr><tr><td>test_accuracy</td><td>0.77056</td></tr><tr><td>train_loss</td><td>1.13464</td></tr><tr><td>val_accuracy</td><td>0.81535</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">still-music-39</strong> at: <a href='https://wandb.ai/onat-inak-/Transformer-Classifier/runs/k6eghjjg' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier/runs/k6eghjjg</a><br/> View project at: <a href='https://wandb.ai/onat-inak-/Transformer-Classifier' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240701_020759-k6eghjjg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:k6eghjjg). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5833271f7c34232a76022412e96bc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113336722094877, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/onatinak/workspace/transaction_analysis/wandb/run-20240701_021048-syjiyz8d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onat-inak-/Transformer-Classifier/runs/syjiyz8d' target=\"_blank\">zany-resonance-40</a></strong> to <a href='https://wandb.ai/onat-inak-/Transformer-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onat-inak-/Transformer-Classifier' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onat-inak-/Transformer-Classifier/runs/syjiyz8d' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier/runs/syjiyz8d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerClassifier(\n",
      "  (input_embedding): Embedding(312, 64)\n",
      "  (position_embedding): Embedding(10, 64)\n",
      "  (transformer_encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.2, inplace=False)\n",
      "        (dropout2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (4): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onatinak/anaconda3/envs/case_study/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "project_name = 'Transformer-Classifier'\n",
    "\n",
    "# Initialize W&B for LSTM-Classifier\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project = project_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config= dict(\n",
    "    project_name = project_name,\n",
    "    batch_size = 900,\n",
    "    transaction_size = transaction_size,\n",
    "    seq_len = 10,\n",
    "    embedding_dim = 32,\n",
    "    nhead = 8,\n",
    "    dim_feedforward_encoder = 32,\n",
    "    fc_hidden_dim = 32,\n",
    "    dropout = 0.2,\n",
    "    activation = \"relu\",\n",
    "    batch_first = True,\n",
    "    norm_first = True,\n",
    "    bias = True,\n",
    "    num_layers = 1,\n",
    "    is_causal = False,\n",
    "    learn_pos_embed = True,\n",
    "    device = device,\n",
    "    num_classes = 1,\n",
    "    num_epochs = 25,\n",
    "    learning_rate = 5e-4,\n",
    "    weight_decay = 0.0, \n",
    "    lr_update_step = None,\n",
    "    log_step = 10,\n",
    "    lr_step_decay = False,\n",
    "    gamma = 0.85,\n",
    "    pos_weight = num_neg_seq/num_pos_seq,\n",
    "    # pos_weight = 6.0,\n",
    "    grad_clip = 1.0,\n",
    "    apply_grad_clip = False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# initialize config\n",
    "config = wandb.config\n",
    "\n",
    "model = TransformerClassifier(transaction_size = config.transaction_size,\n",
    "                              seq_len = config.seq_len,\n",
    "                              embedding_dim = config.embedding_dim,\n",
    "                              nhead = config.nhead,\n",
    "                              dim_feedforward_encoder = config.dim_feedforward_encoder,\n",
    "                              fc_hidden_dim = config.fc_hidden_dim,\n",
    "                              dropout = config.dropout,\n",
    "                              device = config.device,\n",
    "                              activation = config.activation,\n",
    "                              batch_first = config.batch_first,\n",
    "                              norm_first = config.norm_first,\n",
    "                              bias = config.bias,\n",
    "                              num_layers = config.num_layers,\n",
    "                              is_causal = config.is_causal,\n",
    "                              learn_pos_embed = config.learn_pos_embed)\n",
    "model.to(device)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name and param.data.dim() == 2:\n",
    "        nn.init.kaiming_uniform_(param)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZE DATA LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embedding.weight tensor([[-0.0731,  0.1758,  0.1826,  ..., -0.1579, -0.1309, -0.1317],\n",
      "        [ 0.2799, -0.2075, -0.2589,  ...,  0.1938,  0.2945,  0.1080],\n",
      "        [ 0.1538,  0.1774,  0.1628,  ..., -0.0589,  0.0864,  0.0440],\n",
      "        ...,\n",
      "        [-0.1487, -0.2545,  0.1325,  ..., -0.1814, -0.1959, -0.1694],\n",
      "        [-0.2318, -0.0891,  0.0402,  ...,  0.2102, -0.0559, -0.1363],\n",
      "        [ 0.1436, -0.1477, -0.1953,  ..., -0.2754,  0.0799, -0.1059]],\n",
      "       device='cuda:0')\n",
      "position_embedding.weight tensor([[-1.7167e-01, -2.6797e-01,  1.5269e-01,  1.9346e-02, -2.9452e-01,\n",
      "          1.3225e-01,  1.7002e-01,  3.5961e-02,  1.0037e-02,  1.4809e-01,\n",
      "         -2.2775e-01,  2.8105e-01, -2.0843e-01, -1.8877e-01,  2.0119e-01,\n",
      "          6.0791e-02,  3.0253e-01, -5.2485e-03, -1.9569e-01, -9.6379e-02,\n",
      "         -2.6869e-01,  7.5520e-02, -9.5899e-02, -8.9925e-02,  1.4588e-01,\n",
      "         -4.5316e-02, -4.6514e-02, -1.3297e-01,  4.5517e-02, -6.0995e-02,\n",
      "         -1.0585e-01, -1.9514e-01, -2.8376e-02, -1.4868e-01, -1.7527e-01,\n",
      "          3.6268e-02,  2.2681e-01,  4.7026e-02,  3.3829e-02,  1.0145e-02,\n",
      "         -2.5609e-01,  7.5008e-03, -6.5081e-02, -4.8406e-02, -3.7862e-02,\n",
      "         -1.4452e-01, -8.4057e-02, -2.8230e-01, -2.0998e-01,  1.8582e-01,\n",
      "         -4.1469e-02, -2.7235e-02, -9.2339e-02,  1.8881e-02,  1.8503e-01,\n",
      "          2.7967e-01, -1.8000e-01, -9.0465e-02,  1.1330e-01, -4.4355e-03,\n",
      "         -1.6470e-01, -1.0145e-01,  1.0520e-01, -1.9939e-01],\n",
      "        [-1.1640e-02,  6.5422e-02, -2.0108e-01,  2.3611e-01,  2.5532e-01,\n",
      "         -1.9416e-01,  6.5787e-02, -9.4030e-02,  1.1807e-01,  1.7632e-01,\n",
      "         -2.1259e-01, -1.8299e-01, -1.5235e-01, -2.3281e-01,  1.4002e-01,\n",
      "         -9.8304e-02,  4.1707e-02, -1.9370e-01,  6.4399e-02,  6.0402e-02,\n",
      "          1.1118e-01, -8.4654e-02, -2.1884e-01, -2.7347e-01,  1.4120e-01,\n",
      "          2.2162e-01,  1.9317e-01, -2.6680e-02,  1.2129e-01,  2.9641e-01,\n",
      "         -2.7671e-01,  1.0614e-01,  1.9691e-01,  6.5212e-02, -1.3679e-01,\n",
      "          1.4521e-01, -1.4815e-01,  2.0089e-01,  1.4936e-02,  2.0494e-01,\n",
      "          3.0178e-01,  2.2850e-01,  1.9087e-01,  7.1609e-02,  7.0940e-02,\n",
      "          3.7383e-03, -7.6953e-02,  1.3043e-01,  1.7088e-01,  1.2574e-01,\n",
      "          2.5948e-02, -4.0700e-02,  5.0990e-02, -2.6279e-01, -9.4761e-02,\n",
      "          1.9904e-01,  2.6266e-01,  2.2877e-01,  1.1872e-01,  2.4049e-01,\n",
      "         -7.6917e-02,  1.0152e-01, -1.0046e-01,  1.2586e-01],\n",
      "        [ 2.4568e-02, -3.0341e-01, -3.0374e-01,  2.8423e-01, -1.6454e-01,\n",
      "          2.9270e-01, -2.8474e-01, -1.3912e-01, -1.8864e-01,  1.1779e-01,\n",
      "         -2.6359e-01, -6.9219e-02, -1.3352e-02,  2.1921e-01, -9.5229e-02,\n",
      "          3.8555e-02,  1.3206e-01,  8.7760e-02,  1.5886e-01, -2.1964e-01,\n",
      "          1.2346e-01,  1.2713e-01,  1.4757e-01,  2.1654e-01, -1.9170e-01,\n",
      "          1.6097e-03,  1.6243e-01, -2.8081e-01,  2.3667e-02,  4.2938e-02,\n",
      "          1.6806e-01, -1.1505e-01, -2.2675e-01,  1.3397e-01, -2.6597e-01,\n",
      "          3.0162e-01, -1.8514e-01,  5.5468e-02, -8.1096e-02,  2.0242e-01,\n",
      "         -2.3921e-01,  2.1287e-01, -2.5087e-01,  3.5650e-02, -8.4567e-02,\n",
      "          9.7835e-02, -6.6135e-02, -2.0829e-01, -7.8317e-02,  1.8845e-01,\n",
      "         -2.6208e-01,  1.0138e-01, -2.8520e-01, -1.2021e-02, -1.7694e-01,\n",
      "          1.4250e-01, -2.2654e-01,  2.0403e-01,  2.1041e-01, -3.6916e-02,\n",
      "          3.0679e-02,  1.4401e-01,  2.6970e-01, -3.9844e-02],\n",
      "        [-1.1294e-01,  2.9252e-01, -2.0148e-01,  2.4152e-01, -5.4614e-02,\n",
      "         -2.3324e-01,  1.3932e-01, -2.3712e-01,  1.2981e-01, -2.3125e-01,\n",
      "          5.9114e-02,  4.3234e-02,  1.0208e-01, -2.3435e-01,  2.0272e-01,\n",
      "         -3.3329e-02, -1.5323e-01, -2.6448e-01, -1.6236e-02, -1.9396e-01,\n",
      "          2.3030e-01,  1.4348e-01, -3.5116e-02, -1.1163e-01,  1.3675e-01,\n",
      "          2.2696e-01, -1.7820e-01,  9.4418e-02,  6.6015e-02,  1.0297e-01,\n",
      "         -1.8419e-01, -2.1875e-01, -2.2910e-01, -1.4892e-01,  8.1247e-02,\n",
      "         -1.3533e-01,  2.6162e-01,  1.2095e-01, -8.7969e-02, -5.7050e-02,\n",
      "         -1.9458e-01, -6.3819e-04,  2.8477e-01, -3.0214e-01,  9.9057e-03,\n",
      "         -2.4900e-01,  9.8053e-02,  1.4180e-01,  2.3519e-01, -7.3627e-02,\n",
      "          2.8150e-01, -9.3870e-02,  1.8637e-01, -2.8682e-01, -2.6703e-01,\n",
      "          2.6604e-01, -1.0045e-01, -2.5167e-01,  2.4530e-01,  7.4014e-02,\n",
      "          5.6806e-02,  6.1762e-02,  3.0084e-01, -1.3039e-01],\n",
      "        [ 1.0427e-01,  1.6970e-01, -8.3287e-02,  1.4607e-01, -1.1102e-01,\n",
      "          2.6070e-01, -2.7543e-01,  2.2726e-01,  2.1922e-01,  1.9012e-01,\n",
      "         -1.4092e-01, -7.3476e-02,  2.2113e-01, -2.3341e-01,  1.1005e-02,\n",
      "         -1.7789e-01, -9.1841e-02, -2.0630e-01,  4.7893e-02, -1.2310e-01,\n",
      "          1.1501e-01, -3.0266e-01,  2.3238e-01, -1.0332e-02, -1.7693e-01,\n",
      "          2.8995e-01, -2.3247e-01,  1.9053e-01, -9.1210e-03, -2.8775e-01,\n",
      "         -2.7632e-01, -3.7821e-02, -1.2686e-01,  6.4325e-02,  1.6479e-01,\n",
      "         -1.6940e-01,  2.3300e-01, -9.0598e-02, -1.4876e-01, -2.9125e-01,\n",
      "          1.9888e-01,  8.3669e-02,  1.6377e-01, -1.0618e-01, -4.8946e-02,\n",
      "          2.1363e-01, -3.0350e-01,  2.5381e-01,  1.3430e-01, -3.0828e-02,\n",
      "          6.2896e-02, -1.5769e-01,  1.9224e-01, -2.4720e-02,  1.8816e-01,\n",
      "          1.6757e-01,  2.2970e-01,  8.2847e-02,  2.4813e-01,  2.4341e-01,\n",
      "          2.2877e-01,  1.6619e-01, -1.4921e-01,  2.0777e-01],\n",
      "        [ 1.0662e-01, -9.8943e-02,  7.4674e-03, -6.7272e-02, -1.1562e-01,\n",
      "          2.3235e-01, -1.0996e-01,  2.9826e-01,  7.1452e-02,  2.4335e-01,\n",
      "          5.8710e-02,  5.4659e-02, -1.3577e-01,  7.8047e-02,  1.7647e-02,\n",
      "          1.6981e-01,  2.9934e-01, -2.9866e-01, -2.6922e-02, -9.0262e-03,\n",
      "         -8.1166e-03,  1.0008e-01, -2.2231e-01, -8.6801e-02,  2.6514e-01,\n",
      "         -1.3752e-01, -2.9577e-01,  2.8693e-01,  1.1949e-02, -7.0694e-03,\n",
      "          1.3084e-01, -1.0013e-01, -8.5074e-02,  2.8541e-02, -1.8024e-01,\n",
      "          1.9896e-01, -1.3544e-01, -2.6345e-01, -1.6237e-01,  2.7289e-01,\n",
      "         -1.7436e-01,  1.9135e-01, -1.8529e-01, -3.0017e-01,  1.2440e-01,\n",
      "          2.7313e-01, -2.5307e-01, -1.3577e-01, -6.0590e-02,  2.7185e-01,\n",
      "         -2.4225e-01, -2.7620e-01,  1.9805e-01,  2.5438e-01, -8.0057e-02,\n",
      "         -1.8657e-01, -1.6753e-01,  2.5325e-01, -2.3255e-01,  1.8548e-02,\n",
      "         -1.8972e-01,  2.7134e-01,  2.3918e-01, -2.8011e-01],\n",
      "        [-2.0742e-01,  1.4295e-01, -2.4523e-01, -1.5222e-01, -6.6768e-02,\n",
      "         -2.1518e-01,  2.0697e-01,  3.1883e-03, -8.2364e-02,  2.6242e-01,\n",
      "          8.6959e-02,  1.8953e-01,  1.0057e-01,  1.4999e-01, -9.0800e-02,\n",
      "          2.2836e-01, -5.6495e-02,  3.0192e-01, -2.0370e-01, -2.1208e-01,\n",
      "          5.9784e-02,  7.2905e-02, -7.9484e-02,  1.4155e-01, -2.2093e-01,\n",
      "         -2.1306e-01,  1.3142e-01, -2.3787e-01, -9.5887e-02,  2.9735e-01,\n",
      "          1.8817e-01,  3.6287e-02,  1.8390e-03,  2.2211e-01,  1.9899e-01,\n",
      "         -1.2924e-02,  5.5469e-02,  2.3885e-02,  2.7733e-01,  2.0427e-01,\n",
      "         -2.6676e-01,  6.2711e-02, -6.7646e-02,  1.9401e-01,  2.1865e-01,\n",
      "         -6.2741e-02, -1.2242e-01,  1.6122e-02, -1.3018e-01,  2.7009e-01,\n",
      "         -1.7065e-01,  2.7431e-01, -1.4668e-01,  1.3587e-01,  1.8425e-01,\n",
      "          6.4639e-02,  4.2691e-02,  1.8275e-01, -1.5773e-01, -2.8053e-01,\n",
      "          2.5131e-01, -1.9808e-01, -1.2990e-01, -9.2024e-02],\n",
      "        [ 1.7197e-01, -1.2950e-01, -2.9592e-01, -7.1338e-02, -7.8422e-02,\n",
      "         -1.2747e-01, -2.8245e-01,  2.1099e-02,  1.5406e-01,  1.0159e-01,\n",
      "         -1.2829e-01,  1.1400e-01,  2.6244e-01, -2.0976e-02,  2.4350e-01,\n",
      "         -7.9062e-02, -3.0613e-01,  3.3061e-02, -4.0426e-02,  8.4275e-02,\n",
      "         -2.9870e-01, -1.1967e-01, -2.2331e-01,  7.7269e-02,  8.0305e-02,\n",
      "         -1.1809e-01, -1.1579e-01, -9.1839e-02,  2.4389e-01, -2.3204e-01,\n",
      "         -2.9759e-01, -3.4447e-02,  2.9093e-02, -4.2614e-02,  4.7575e-02,\n",
      "         -2.4504e-02, -2.6764e-01,  2.6566e-01,  7.0373e-02,  1.0014e-01,\n",
      "         -1.1102e-01,  8.4889e-02, -3.3025e-02, -1.1790e-01, -2.9933e-01,\n",
      "          2.2788e-01,  7.3734e-02, -2.3888e-01, -7.8551e-03, -1.9232e-01,\n",
      "         -2.5590e-01,  9.9501e-02,  1.2093e-01,  1.4273e-01,  2.5354e-01,\n",
      "         -2.6189e-01,  3.8282e-03, -2.6553e-01, -2.4064e-02,  1.4850e-01,\n",
      "         -6.7424e-02,  2.2933e-01, -2.1377e-01,  1.2405e-01],\n",
      "        [-1.1208e-01,  1.3643e-01,  2.1744e-02, -1.1295e-01,  2.9525e-01,\n",
      "         -1.5827e-01, -1.2214e-03,  2.7356e-01,  1.9157e-01,  1.4061e-01,\n",
      "         -2.6466e-01, -4.9884e-02, -2.9418e-01,  2.3303e-01, -1.6177e-01,\n",
      "         -1.7720e-01,  2.3692e-01,  1.4980e-04,  1.7727e-01, -1.7517e-01,\n",
      "          2.2771e-01, -4.3673e-02,  2.0070e-01,  1.6997e-01,  2.1776e-01,\n",
      "         -4.0283e-03, -2.3061e-01, -6.9619e-02, -2.4754e-01,  2.0261e-01,\n",
      "         -2.4608e-02,  2.0966e-01,  1.2109e-01,  5.9202e-02,  2.0742e-01,\n",
      "          6.0593e-02, -1.5784e-01, -1.6122e-01, -2.4823e-02,  2.6749e-01,\n",
      "          1.2497e-01,  6.1653e-02, -6.6556e-02,  3.9868e-02, -1.7588e-01,\n",
      "         -1.5819e-01, -2.5590e-01, -1.1138e-01,  2.8941e-01,  1.6566e-01,\n",
      "         -1.9329e-01,  1.1064e-02, -4.0291e-02,  3.6131e-03, -1.4623e-01,\n",
      "          4.8256e-02, -2.8255e-01,  1.1709e-01,  1.9678e-01, -1.1410e-01,\n",
      "          1.9416e-01, -9.5668e-02,  2.2426e-01,  1.7652e-01],\n",
      "        [ 1.3608e-01,  2.4648e-01,  1.0137e-01, -8.3128e-02,  1.8250e-01,\n",
      "          5.1809e-02, -1.9977e-01,  1.1270e-02, -3.0099e-01,  1.6313e-01,\n",
      "         -1.3113e-01,  2.4377e-01,  2.4644e-01, -2.0078e-01, -4.3041e-02,\n",
      "          5.3387e-02, -1.9033e-01, -8.4028e-02,  2.1882e-01, -2.7254e-01,\n",
      "          1.3136e-01, -1.1674e-01, -2.8244e-01, -1.3625e-01,  2.4584e-01,\n",
      "         -1.6237e-01, -2.3134e-01,  1.3569e-02,  2.6249e-01,  3.0242e-01,\n",
      "          9.7691e-02, -2.3466e-01, -2.2303e-01,  2.6758e-03, -3.7668e-02,\n",
      "          2.4219e-01,  7.9550e-02,  1.8064e-01,  6.7612e-02,  2.9284e-01,\n",
      "          5.2759e-02,  8.8078e-02, -2.0364e-01, -1.1444e-01, -1.0019e-01,\n",
      "          2.7836e-01, -1.2256e-01, -1.6163e-01,  1.3212e-01,  2.8948e-01,\n",
      "         -1.1531e-01, -9.2947e-02,  4.5073e-02, -2.9247e-01, -2.0077e-01,\n",
      "          2.6946e-01,  2.8411e-01, -1.8842e-01, -3.2517e-02,  5.0695e-02,\n",
      "          2.8716e-01,  5.6089e-02,  2.4890e-01,  1.6146e-01]], device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.in_proj_weight tensor([[-0.1476, -0.3005, -0.1814,  ..., -0.3016, -0.2980,  0.2851],\n",
      "        [-0.0148, -0.0467, -0.0183,  ...,  0.1441, -0.0968, -0.1944],\n",
      "        [-0.0273, -0.0155, -0.2919,  ..., -0.2319, -0.2750, -0.1578],\n",
      "        ...,\n",
      "        [-0.1497,  0.3032, -0.1352,  ...,  0.0533, -0.0370,  0.1758],\n",
      "        [-0.2468,  0.0602, -0.2959,  ..., -0.0460,  0.2507,  0.1000],\n",
      "        [ 0.1012, -0.2901, -0.1751,  ...,  0.1554, -0.2481,  0.2198]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.in_proj_bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.out_proj.weight tensor([[ 0.1487,  0.2320, -0.2907,  ..., -0.0907,  0.2265, -0.2240],\n",
      "        [-0.1337, -0.2690, -0.0590,  ...,  0.2205,  0.0799,  0.0518],\n",
      "        [ 0.2777,  0.1541,  0.0477,  ..., -0.0406, -0.2324, -0.1632],\n",
      "        ...,\n",
      "        [-0.1204,  0.2462,  0.2462,  ..., -0.0922,  0.0175, -0.0494],\n",
      "        [-0.0247, -0.2246, -0.0724,  ..., -0.1446, -0.0796,  0.2754],\n",
      "        [ 0.0392, -0.2127,  0.1141,  ...,  0.0046, -0.1321,  0.2621]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.out_proj.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear1.weight tensor([[ 2.5749e-01, -2.7425e-01, -1.2951e-01,  ...,  2.2394e-01,\n",
      "          2.2446e-01,  1.5474e-02],\n",
      "        [ 1.4055e-01, -1.0659e-01, -5.3375e-02,  ...,  2.7899e-01,\n",
      "          2.4619e-04, -2.3291e-01],\n",
      "        [ 1.2942e-01, -3.8130e-02,  1.9349e-01,  ...,  2.9204e-01,\n",
      "          3.6153e-02, -9.9486e-02],\n",
      "        ...,\n",
      "        [-7.1937e-03, -2.7092e-01, -1.6035e-01,  ...,  2.4438e-02,\n",
      "          2.0488e-01, -1.7477e-01],\n",
      "        [ 3.0598e-01, -5.6677e-02, -2.5635e-01,  ...,  2.3766e-01,\n",
      "         -3.0514e-01, -2.7449e-02],\n",
      "        [-2.6852e-01, -1.1278e-01, -1.4628e-02,  ...,  1.1191e-01,\n",
      "          1.1213e-01, -1.0149e-01]], device='cuda:0')\n",
      "transformer_encoder_layer.linear1.bias tensor([-0.0463, -0.0482,  0.0280, -0.0686, -0.0988, -0.0673, -0.0733, -0.1123,\n",
      "        -0.0716,  0.0571,  0.1165, -0.0762, -0.0144, -0.1066, -0.0080,  0.1027,\n",
      "        -0.0611,  0.0536,  0.1062,  0.0387, -0.0515, -0.0200, -0.0940,  0.0399,\n",
      "         0.0301, -0.0883,  0.1065,  0.0029, -0.1145,  0.0316, -0.0973,  0.0511,\n",
      "         0.0636, -0.0207, -0.0341, -0.0081, -0.0718,  0.0273,  0.0333,  0.0698,\n",
      "        -0.0483, -0.0552,  0.0185, -0.1165, -0.1079,  0.0433, -0.0879, -0.0317,\n",
      "         0.0933,  0.0965,  0.0585, -0.0246, -0.0218, -0.0746, -0.1119,  0.0030,\n",
      "         0.0556, -0.0229, -0.0357,  0.0844, -0.0087, -0.0609, -0.0826, -0.0518,\n",
      "        -0.0676,  0.0238, -0.0467,  0.0871, -0.0747, -0.0232,  0.1203, -0.0364,\n",
      "        -0.0027,  0.0726, -0.0455,  0.1057, -0.0288,  0.0978,  0.1038,  0.0072,\n",
      "         0.0761,  0.0497, -0.0066,  0.0953,  0.0793, -0.0723, -0.0546, -0.1200,\n",
      "        -0.0167, -0.0212, -0.0566, -0.0174, -0.0520, -0.1052,  0.0992, -0.0654,\n",
      "         0.0116, -0.0828,  0.0085,  0.0465, -0.0938, -0.0972, -0.1148,  0.0231,\n",
      "         0.0670, -0.0639,  0.0211, -0.0451, -0.0958,  0.0288, -0.0856,  0.0533,\n",
      "        -0.0920, -0.1119,  0.1213,  0.0934,  0.0118,  0.1027, -0.0153,  0.0982,\n",
      "        -0.1152,  0.1039,  0.0972, -0.0193,  0.0395, -0.0322,  0.0492,  0.0763],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear2.weight tensor([[-0.1747,  0.0845, -0.0688,  ...,  0.1898, -0.1528, -0.0901],\n",
      "        [ 0.0593,  0.0659, -0.1302,  ...,  0.0719,  0.0838, -0.1401],\n",
      "        [ 0.0562, -0.2106, -0.0843,  ...,  0.1209, -0.0598,  0.0050],\n",
      "        ...,\n",
      "        [-0.1998,  0.1394,  0.0405,  ...,  0.0565,  0.1653, -0.0085],\n",
      "        [ 0.1939,  0.0709, -0.0508,  ...,  0.1547, -0.0090, -0.1602],\n",
      "        [ 0.0446,  0.1169,  0.0941,  ..., -0.1367, -0.0798,  0.0024]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear2.bias tensor([-0.0250, -0.0356, -0.0302, -0.0182,  0.0239, -0.0150, -0.0436,  0.0533,\n",
      "        -0.0392,  0.0825,  0.0443,  0.0853,  0.0649,  0.0379, -0.0486, -0.0697,\n",
      "        -0.0690,  0.0348, -0.0701, -0.0048,  0.0772, -0.0179, -0.0242, -0.0384,\n",
      "         0.0057, -0.0359,  0.0193, -0.0722, -0.0200,  0.0483, -0.0269,  0.0178,\n",
      "        -0.0670, -0.0004, -0.0875, -0.0856, -0.0217, -0.0125,  0.0412,  0.0806,\n",
      "        -0.0735,  0.0319, -0.0293, -0.0366,  0.0524,  0.0366,  0.0078,  0.0342,\n",
      "        -0.0061, -0.0867, -0.0822, -0.0341, -0.0865,  0.0645, -0.0124, -0.0784,\n",
      "        -0.0080, -0.0482, -0.0390,  0.0607,  0.0062,  0.0677, -0.0767,  0.0464],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.norm1.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder_layer.norm1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.norm2.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder_layer.norm2.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_weight tensor([[ 0.1110,  0.1688, -0.1857,  ...,  0.2486, -0.0365, -0.1471],\n",
      "        [ 0.2076,  0.0981, -0.0174,  ..., -0.2691, -0.2346,  0.2717],\n",
      "        [ 0.1860, -0.2262,  0.2148,  ...,  0.2652,  0.2638,  0.2989],\n",
      "        ...,\n",
      "        [-0.0798,  0.1316,  0.2604,  ..., -0.2675,  0.2043,  0.1248],\n",
      "        [-0.0409, -0.0631, -0.0226,  ..., -0.0108,  0.1856, -0.1746],\n",
      "        [ 0.2335,  0.1489,  0.2719,  ..., -0.2168,  0.2568,  0.2435]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight tensor([[-0.1053,  0.0350,  0.1426,  ...,  0.2725,  0.2334,  0.2767],\n",
      "        [ 0.2795, -0.2375, -0.0216,  ..., -0.1198, -0.2255,  0.0024],\n",
      "        [-0.0586, -0.0427,  0.1478,  ...,  0.0061, -0.2536,  0.0661],\n",
      "        ...,\n",
      "        [-0.0750,  0.0353, -0.1200,  ..., -0.1665,  0.0916,  0.2977],\n",
      "        [ 0.2774,  0.2777, -0.1212,  ..., -0.0311, -0.2346, -0.1048],\n",
      "        [ 0.1100,  0.0729,  0.2655,  ..., -0.0167, -0.1430, -0.1107]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.weight tensor([[-0.0980,  0.1235,  0.2954,  ...,  0.1815,  0.2452,  0.0859],\n",
      "        [-0.1335, -0.0814,  0.3038,  ...,  0.0110, -0.2713,  0.0551],\n",
      "        [-0.0809, -0.1269,  0.1066,  ...,  0.0765, -0.0246, -0.0731],\n",
      "        ...,\n",
      "        [ 0.2217,  0.0837,  0.2274,  ..., -0.2783, -0.2180,  0.1761],\n",
      "        [-0.0370, -0.2996,  0.2268,  ..., -0.2469,  0.2933,  0.0464],\n",
      "        [ 0.1646,  0.2283,  0.1266,  ..., -0.3052,  0.2933,  0.0923]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.bias tensor([-0.0463, -0.0482,  0.0280, -0.0686, -0.0988, -0.0673, -0.0733, -0.1123,\n",
      "        -0.0716,  0.0571,  0.1165, -0.0762, -0.0144, -0.1066, -0.0080,  0.1027,\n",
      "        -0.0611,  0.0536,  0.1062,  0.0387, -0.0515, -0.0200, -0.0940,  0.0399,\n",
      "         0.0301, -0.0883,  0.1065,  0.0029, -0.1145,  0.0316, -0.0973,  0.0511,\n",
      "         0.0636, -0.0207, -0.0341, -0.0081, -0.0718,  0.0273,  0.0333,  0.0698,\n",
      "        -0.0483, -0.0552,  0.0185, -0.1165, -0.1079,  0.0433, -0.0879, -0.0317,\n",
      "         0.0933,  0.0965,  0.0585, -0.0246, -0.0218, -0.0746, -0.1119,  0.0030,\n",
      "         0.0556, -0.0229, -0.0357,  0.0844, -0.0087, -0.0609, -0.0826, -0.0518,\n",
      "        -0.0676,  0.0238, -0.0467,  0.0871, -0.0747, -0.0232,  0.1203, -0.0364,\n",
      "        -0.0027,  0.0726, -0.0455,  0.1057, -0.0288,  0.0978,  0.1038,  0.0072,\n",
      "         0.0761,  0.0497, -0.0066,  0.0953,  0.0793, -0.0723, -0.0546, -0.1200,\n",
      "        -0.0167, -0.0212, -0.0566, -0.0174, -0.0520, -0.1052,  0.0992, -0.0654,\n",
      "         0.0116, -0.0828,  0.0085,  0.0465, -0.0938, -0.0972, -0.1148,  0.0231,\n",
      "         0.0670, -0.0639,  0.0211, -0.0451, -0.0958,  0.0288, -0.0856,  0.0533,\n",
      "        -0.0920, -0.1119,  0.1213,  0.0934,  0.0118,  0.1027, -0.0153,  0.0982,\n",
      "        -0.1152,  0.1039,  0.0972, -0.0193,  0.0395, -0.0322,  0.0492,  0.0763],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.weight tensor([[-0.0534,  0.1602, -0.0908,  ...,  0.0026, -0.1909, -0.1146],\n",
      "        [-0.0101, -0.0414, -0.1003,  ...,  0.1056, -0.1499, -0.0657],\n",
      "        [ 0.1481, -0.0103,  0.1871,  ..., -0.1393, -0.0458,  0.0958],\n",
      "        ...,\n",
      "        [-0.1944, -0.0858,  0.1364,  ...,  0.0940, -0.0417,  0.1211],\n",
      "        [ 0.1111,  0.2147,  0.0229,  ..., -0.1494, -0.0239, -0.1866],\n",
      "        [-0.1761,  0.0009, -0.1259,  ..., -0.1410, -0.0275, -0.1847]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.bias tensor([-0.0250, -0.0356, -0.0302, -0.0182,  0.0239, -0.0150, -0.0436,  0.0533,\n",
      "        -0.0392,  0.0825,  0.0443,  0.0853,  0.0649,  0.0379, -0.0486, -0.0697,\n",
      "        -0.0690,  0.0348, -0.0701, -0.0048,  0.0772, -0.0179, -0.0242, -0.0384,\n",
      "         0.0057, -0.0359,  0.0193, -0.0722, -0.0200,  0.0483, -0.0269,  0.0178,\n",
      "        -0.0670, -0.0004, -0.0875, -0.0856, -0.0217, -0.0125,  0.0412,  0.0806,\n",
      "        -0.0735,  0.0319, -0.0293, -0.0366,  0.0524,  0.0366,  0.0078,  0.0342,\n",
      "        -0.0061, -0.0867, -0.0822, -0.0341, -0.0865,  0.0645, -0.0124, -0.0784,\n",
      "        -0.0080, -0.0482, -0.0390,  0.0607,  0.0062,  0.0677, -0.0767,  0.0464],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "fc.0.weight tensor([[ 0.0804,  0.0466, -0.0792,  ..., -0.0019, -0.0651, -0.0470],\n",
      "        [-0.0727,  0.0600,  0.0582,  ..., -0.0821,  0.0313,  0.0306],\n",
      "        [-0.0225,  0.0370, -0.0344,  ..., -0.0640,  0.0152, -0.0234],\n",
      "        ...,\n",
      "        [-0.0828,  0.0932, -0.0431,  ...,  0.0566, -0.0433, -0.0455],\n",
      "        [ 0.0750,  0.0518, -0.0456,  ..., -0.0688,  0.0917, -0.0793],\n",
      "        [ 0.0528,  0.0505,  0.0244,  ...,  0.0749, -0.0388, -0.0214]],\n",
      "       device='cuda:0')\n",
      "fc.0.bias tensor([-1.8472e-02, -3.3557e-02, -1.6331e-02,  1.9784e-04,  2.7500e-03,\n",
      "         8.5102e-06, -3.0190e-02, -1.9152e-02,  3.3277e-02, -3.4147e-02,\n",
      "         4.6923e-03, -1.7847e-02, -9.8912e-03,  2.1519e-02, -6.4126e-03,\n",
      "         1.8312e-02,  3.1748e-03,  2.5110e-02, -3.1591e-02, -1.0388e-02,\n",
      "        -1.0326e-02, -2.2462e-02,  3.8821e-02,  2.7882e-02, -4.1372e-03,\n",
      "         1.7457e-02,  3.1906e-02, -2.6164e-02,  2.3668e-02, -2.1636e-02,\n",
      "        -1.0737e-02, -6.0408e-03,  1.6248e-02,  3.2399e-02,  1.6552e-02,\n",
      "        -1.1423e-02,  3.7610e-02,  2.1017e-03,  3.7001e-03,  1.2535e-02,\n",
      "         7.3908e-03, -3.4918e-02, -1.7059e-02,  1.6962e-02,  3.6828e-03,\n",
      "         3.2356e-02,  2.1142e-02,  2.5013e-02, -3.8319e-02, -7.4356e-03,\n",
      "        -8.0683e-03, -8.1947e-05,  3.9080e-02, -6.2053e-03,  3.7695e-02,\n",
      "        -3.2804e-02,  3.6398e-03,  2.8133e-03, -3.4132e-02, -8.8591e-03,\n",
      "         1.1089e-02,  1.1957e-02,  3.5045e-02,  1.5559e-02], device='cuda:0')\n",
      "fc.3.weight tensor([[-0.1255,  0.0206,  0.2594,  0.2208,  0.1053,  0.2971,  0.1988, -0.2428,\n",
      "          0.0054,  0.0514, -0.2774, -0.1287,  0.1039, -0.1757,  0.2939, -0.0563,\n",
      "         -0.0528, -0.2026,  0.0679,  0.1293, -0.2068, -0.3044,  0.1651,  0.1331,\n",
      "          0.2011,  0.2400,  0.1995,  0.0418, -0.2110,  0.1782, -0.2551,  0.0179,\n",
      "          0.1931, -0.1427, -0.2638,  0.0367,  0.2448, -0.3010,  0.1306, -0.0178,\n",
      "         -0.0341,  0.0967,  0.2758,  0.0669, -0.0412,  0.0354,  0.0623,  0.2313,\n",
      "          0.2391,  0.1522, -0.1458, -0.2913,  0.0594,  0.1089,  0.2111,  0.0818,\n",
      "          0.0113, -0.0782,  0.1456, -0.1420,  0.1472, -0.3049, -0.2023,  0.1918]],\n",
      "       device='cuda:0')\n",
      "fc.3.bias tensor([-0.0432], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Divide train and test dataset into batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Check whether data is splitted correctly -> X_.shape: (batch, seq, encoding), y_.shape: (batch)\n",
    "# for i, (X_, y_) in enumerate(train_loader): \n",
    "#     print(X_.shape, y_.shape)\n",
    "#     print(X_[:10,:])\n",
    "#     print(y_[:10])\n",
    "\n",
    "# Print trainable model parameters with their corresponding initial values\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on 10800 transactions in test data: 47.712963%\n",
      "f1_score_test:  0.18666282586778052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.46      0.61      9689\n",
      "         1.0       0.11      0.58      0.19      1111\n",
      "\n",
      "    accuracy                           0.48     10800\n",
      "   macro avg       0.51      0.52      0.40     10800\n",
      "weighted avg       0.82      0.48      0.57     10800\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (input_embedding): Embedding(312, 64)\n",
       "  (position_embedding): Embedding(10, 64)\n",
       "  (transformer_encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.2, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=640, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Observe initial performance of the model without any training\n",
    "model.eval()\n",
    "test(model, test_loader, device, config.project_name, save_model = False)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer:  AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "\n",
      "loss function:  BCEWithLogitsLoss()\n",
      "\n",
      "scheduler <torch.optim.lr_scheduler.ExponentialLR object at 0x7fb688505eb0>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(float(config.pos_weight)).to(dtype=torch.long, device=config.device) * torch.ones([1]).to(config.device))\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight = 5 * torch.ones([1]).to(config.device))\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.999),\n",
    "#                              eps=1e-8, weight_decay=config.weight_decay, amsgrad=False)  \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01,\n",
    "                             amsgrad=False, maximize=False, foreach=None, capturable=False, \n",
    "                             differentiable=False, fused=None)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.gamma)\n",
    "\n",
    "print('optimizer: ', optimizer)\n",
    "print('')\n",
    "print('loss function: ', criterion)\n",
    "print('')\n",
    "print('scheduler', scheduler)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62061f7b7d714af2a13dca53555f98a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.47      0.62      8817\n",
      "         1.0       0.12      0.61      0.21      1083\n",
      "\n",
      "    accuracy                           0.48      9900\n",
      "   macro avg       0.52      0.54      0.41      9900\n",
      "weighted avg       0.82      0.48      0.57      9900\n",
      "\n",
      "Epoch [1/25], Step [10/100], Train Loss: 1.2245\n",
      "learning_rate:  0.001\n",
      "Train Loss after 10 batches: 1.224\n",
      "Epoch [1/25], Step [20/100], Train Loss: 1.2235\n",
      "learning_rate:  0.001\n",
      "Train Loss after 20 batches: 1.223\n",
      "Epoch [1/25], Step [30/100], Train Loss: 1.2233\n",
      "learning_rate:  0.001\n",
      "Train Loss after 30 batches: 1.223\n",
      "Epoch [1/25], Step [40/100], Train Loss: 1.1694\n",
      "learning_rate:  0.001\n",
      "Train Loss after 40 batches: 1.169\n",
      "Epoch [1/25], Step [50/100], Train Loss: 1.1552\n",
      "learning_rate:  0.001\n",
      "Train Loss after 50 batches: 1.155\n",
      "Epoch [1/25], Step [60/100], Train Loss: 1.1276\n",
      "learning_rate:  0.001\n",
      "Train Loss after 60 batches: 1.128\n",
      "Epoch [1/25], Step [70/100], Train Loss: 1.1041\n",
      "learning_rate:  0.001\n",
      "Train Loss after 70 batches: 1.104\n",
      "Epoch [1/25], Step [80/100], Train Loss: 1.0938\n",
      "learning_rate:  0.001\n",
      "Train Loss after 80 batches: 1.094\n",
      "Epoch [1/25], Step [90/100], Train Loss: 1.0841\n",
      "learning_rate:  0.001\n",
      "Train Loss after 90 batches: 1.084\n",
      "Epoch [1/25], Step [100/100], Train Loss: 1.1012\n",
      "learning_rate:  0.001\n",
      "Train Loss after 100 batches: 1.101\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.87      0.91      8818\n",
      "         1.0       0.37      0.62      0.46      1082\n",
      "\n",
      "    accuracy                           0.84      9900\n",
      "   macro avg       0.66      0.74      0.68      9900\n",
      "weighted avg       0.89      0.84      0.86      9900\n",
      "\n",
      "Validation Accuracy: 84.090912%\n",
      "\n",
      "Epoch [2/25], Step [10/100], Train Loss: 1.0880\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 110 batches: 1.088\n",
      "Epoch [2/25], Step [20/100], Train Loss: 1.0707\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 120 batches: 1.071\n",
      "Epoch [2/25], Step [30/100], Train Loss: 1.0665\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 130 batches: 1.067\n",
      "Epoch [2/25], Step [40/100], Train Loss: 1.0825\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 140 batches: 1.082\n",
      "Epoch [2/25], Step [50/100], Train Loss: 1.0809\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 150 batches: 1.081\n",
      "Epoch [2/25], Step [60/100], Train Loss: 1.0737\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 160 batches: 1.074\n",
      "Epoch [2/25], Step [70/100], Train Loss: 1.0753\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 170 batches: 1.075\n",
      "Epoch [2/25], Step [80/100], Train Loss: 1.0773\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 180 batches: 1.077\n",
      "Epoch [2/25], Step [90/100], Train Loss: 1.0656\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 190 batches: 1.066\n",
      "Epoch [2/25], Step [100/100], Train Loss: 1.0715\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 200 batches: 1.072\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8818\n",
      "         1.0       0.38      0.68      0.49      1082\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.67      0.77      0.70      9900\n",
      "weighted avg       0.89      0.85      0.86      9900\n",
      "\n",
      "Validation Accuracy: 84.636366%\n",
      "\n",
      "Epoch [3/25], Step [10/100], Train Loss: 1.0691\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 210 batches: 1.069\n",
      "Epoch [3/25], Step [20/100], Train Loss: 1.0472\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 220 batches: 1.047\n",
      "Epoch [3/25], Step [30/100], Train Loss: 1.0673\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 230 batches: 1.067\n",
      "Epoch [3/25], Step [40/100], Train Loss: 1.0502\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 240 batches: 1.050\n",
      "Epoch [3/25], Step [50/100], Train Loss: 1.0779\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 250 batches: 1.078\n",
      "Epoch [3/25], Step [60/100], Train Loss: 1.0789\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 260 batches: 1.079\n",
      "Epoch [3/25], Step [70/100], Train Loss: 1.0505\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 270 batches: 1.050\n",
      "Epoch [3/25], Step [80/100], Train Loss: 1.0531\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 280 batches: 1.053\n",
      "Epoch [3/25], Step [90/100], Train Loss: 1.0658\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 290 batches: 1.066\n",
      "Epoch [3/25], Step [100/100], Train Loss: 1.0460\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 300 batches: 1.046\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.85      0.90      8821\n",
      "         1.0       0.37      0.72      0.49      1079\n",
      "\n",
      "    accuracy                           0.84      9900\n",
      "   macro avg       0.67      0.79      0.70      9900\n",
      "weighted avg       0.90      0.84      0.86      9900\n",
      "\n",
      "Validation Accuracy: 83.656567%\n",
      "\n",
      "Epoch [4/25], Step [10/100], Train Loss: 1.0599\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 310 batches: 1.060\n",
      "Epoch [4/25], Step [20/100], Train Loss: 1.0484\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 320 batches: 1.048\n",
      "Epoch [4/25], Step [30/100], Train Loss: 1.0647\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 330 batches: 1.065\n",
      "Epoch [4/25], Step [40/100], Train Loss: 1.0534\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 340 batches: 1.053\n",
      "Epoch [4/25], Step [50/100], Train Loss: 1.0400\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 350 batches: 1.040\n",
      "Epoch [4/25], Step [60/100], Train Loss: 1.0624\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 360 batches: 1.062\n",
      "Epoch [4/25], Step [70/100], Train Loss: 1.0595\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 370 batches: 1.060\n",
      "Epoch [4/25], Step [80/100], Train Loss: 1.0494\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 380 batches: 1.049\n",
      "Epoch [4/25], Step [90/100], Train Loss: 1.0697\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 390 batches: 1.070\n",
      "Epoch [4/25], Step [100/100], Train Loss: 1.0622\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 400 batches: 1.062\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.84      0.90      8820\n",
      "         1.0       0.37      0.74      0.49      1080\n",
      "\n",
      "    accuracy                           0.83      9900\n",
      "   macro avg       0.67      0.79      0.70      9900\n",
      "weighted avg       0.90      0.83      0.85      9900\n",
      "\n",
      "Validation Accuracy: 83.212125%\n",
      "\n",
      "Epoch [5/25], Step [10/100], Train Loss: 1.0521\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 410 batches: 1.052\n",
      "Epoch [5/25], Step [20/100], Train Loss: 1.0421\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 420 batches: 1.042\n",
      "Epoch [5/25], Step [30/100], Train Loss: 1.0603\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 430 batches: 1.060\n",
      "Epoch [5/25], Step [40/100], Train Loss: 1.0576\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 440 batches: 1.058\n",
      "Epoch [5/25], Step [50/100], Train Loss: 1.0577\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 450 batches: 1.058\n",
      "Epoch [5/25], Step [60/100], Train Loss: 1.0471\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 460 batches: 1.047\n",
      "Epoch [5/25], Step [70/100], Train Loss: 1.0520\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 470 batches: 1.052\n",
      "Epoch [5/25], Step [80/100], Train Loss: 1.0535\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 480 batches: 1.054\n",
      "Epoch [5/25], Step [90/100], Train Loss: 1.0478\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 490 batches: 1.048\n",
      "Epoch [5/25], Step [100/100], Train Loss: 1.0623\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 500 batches: 1.062\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.88      0.92      8827\n",
      "         1.0       0.41      0.67      0.51      1073\n",
      "\n",
      "    accuracy                           0.86      9900\n",
      "   macro avg       0.68      0.78      0.71      9900\n",
      "weighted avg       0.90      0.86      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.848486%\n",
      "\n",
      "Epoch [6/25], Step [10/100], Train Loss: 1.0389\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 510 batches: 1.039\n",
      "Epoch [6/25], Step [20/100], Train Loss: 1.0626\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 520 batches: 1.063\n",
      "Epoch [6/25], Step [30/100], Train Loss: 1.0464\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 530 batches: 1.046\n",
      "Epoch [6/25], Step [40/100], Train Loss: 1.0547\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 540 batches: 1.055\n",
      "Epoch [6/25], Step [50/100], Train Loss: 1.0512\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 550 batches: 1.051\n",
      "Epoch [6/25], Step [60/100], Train Loss: 1.0567\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 560 batches: 1.057\n",
      "Epoch [6/25], Step [70/100], Train Loss: 1.0508\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 570 batches: 1.051\n",
      "Epoch [6/25], Step [80/100], Train Loss: 1.0468\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 580 batches: 1.047\n",
      "Epoch [6/25], Step [90/100], Train Loss: 1.0572\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 590 batches: 1.057\n",
      "Epoch [6/25], Step [100/100], Train Loss: 1.0549\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 600 batches: 1.055\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8814\n",
      "         1.0       0.40      0.71      0.51      1086\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.111111%\n",
      "\n",
      "Epoch [7/25], Step [10/100], Train Loss: 1.0548\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 610 batches: 1.055\n",
      "Epoch [7/25], Step [20/100], Train Loss: 1.0471\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 620 batches: 1.047\n",
      "Epoch [7/25], Step [30/100], Train Loss: 1.0509\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 630 batches: 1.051\n",
      "Epoch [7/25], Step [40/100], Train Loss: 1.0582\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 640 batches: 1.058\n",
      "Epoch [7/25], Step [50/100], Train Loss: 1.0608\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 650 batches: 1.061\n",
      "Epoch [7/25], Step [60/100], Train Loss: 1.0369\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 660 batches: 1.037\n",
      "Epoch [7/25], Step [70/100], Train Loss: 1.0412\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 670 batches: 1.041\n",
      "Epoch [7/25], Step [80/100], Train Loss: 1.0485\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 680 batches: 1.049\n",
      "Epoch [7/25], Step [90/100], Train Loss: 1.0362\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 690 batches: 1.036\n",
      "Epoch [7/25], Step [100/100], Train Loss: 1.0553\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 700 batches: 1.055\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8815\n",
      "         1.0       0.39      0.70      0.50      1085\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.78      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 84.888893%\n",
      "\n",
      "Epoch [8/25], Step [10/100], Train Loss: 1.0539\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 710 batches: 1.054\n",
      "Epoch [8/25], Step [20/100], Train Loss: 1.0624\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 720 batches: 1.062\n",
      "Epoch [8/25], Step [30/100], Train Loss: 1.0357\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 730 batches: 1.036\n",
      "Epoch [8/25], Step [40/100], Train Loss: 1.0378\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 740 batches: 1.038\n",
      "Epoch [8/25], Step [50/100], Train Loss: 1.0461\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 750 batches: 1.046\n",
      "Epoch [8/25], Step [60/100], Train Loss: 1.0587\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 760 batches: 1.059\n",
      "Epoch [8/25], Step [70/100], Train Loss: 1.0505\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 770 batches: 1.051\n",
      "Epoch [8/25], Step [80/100], Train Loss: 1.0231\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 780 batches: 1.023\n",
      "Epoch [8/25], Step [90/100], Train Loss: 1.0397\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 790 batches: 1.040\n",
      "Epoch [8/25], Step [100/100], Train Loss: 1.0652\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 800 batches: 1.065\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8816\n",
      "         1.0       0.40      0.71      0.51      1084\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.010105%\n",
      "\n",
      "Epoch [9/25], Step [10/100], Train Loss: 1.0441\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 810 batches: 1.044\n",
      "Epoch [9/25], Step [20/100], Train Loss: 1.0626\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 820 batches: 1.063\n",
      "Epoch [9/25], Step [30/100], Train Loss: 1.0390\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 830 batches: 1.039\n",
      "Epoch [9/25], Step [40/100], Train Loss: 1.0257\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 840 batches: 1.026\n",
      "Epoch [9/25], Step [50/100], Train Loss: 1.0670\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 850 batches: 1.067\n",
      "Epoch [9/25], Step [60/100], Train Loss: 1.0371\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 860 batches: 1.037\n",
      "Epoch [9/25], Step [70/100], Train Loss: 1.0585\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 870 batches: 1.059\n",
      "Epoch [9/25], Step [80/100], Train Loss: 1.0446\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 880 batches: 1.045\n",
      "Epoch [9/25], Step [90/100], Train Loss: 1.0379\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 890 batches: 1.038\n",
      "Epoch [9/25], Step [100/100], Train Loss: 1.0481\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 900 batches: 1.048\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8808\n",
      "         1.0       0.39      0.72      0.51      1092\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 84.666669%\n",
      "\n",
      "Epoch [10/25], Step [10/100], Train Loss: 1.0509\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 910 batches: 1.051\n",
      "Epoch [10/25], Step [20/100], Train Loss: 1.0425\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 920 batches: 1.043\n",
      "Epoch [10/25], Step [30/100], Train Loss: 1.0618\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 930 batches: 1.062\n",
      "Epoch [10/25], Step [40/100], Train Loss: 1.0291\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 940 batches: 1.029\n",
      "Epoch [10/25], Step [50/100], Train Loss: 1.0404\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 950 batches: 1.040\n",
      "Epoch [10/25], Step [60/100], Train Loss: 1.0516\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 960 batches: 1.052\n",
      "Epoch [10/25], Step [70/100], Train Loss: 1.0497\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 970 batches: 1.050\n",
      "Epoch [10/25], Step [80/100], Train Loss: 1.0430\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 980 batches: 1.043\n",
      "Epoch [10/25], Step [90/100], Train Loss: 1.0464\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 990 batches: 1.046\n",
      "Epoch [10/25], Step [100/100], Train Loss: 1.0320\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 1000 batches: 1.032\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8816\n",
      "         1.0       0.41      0.71      0.52      1084\n",
      "\n",
      "    accuracy                           0.86      9900\n",
      "   macro avg       0.68      0.79      0.72      9900\n",
      "weighted avg       0.90      0.86      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.515153%\n",
      "\n",
      "Epoch [11/25], Step [10/100], Train Loss: 1.0431\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1010 batches: 1.043\n",
      "Epoch [11/25], Step [20/100], Train Loss: 1.0482\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1020 batches: 1.048\n",
      "Epoch [11/25], Step [30/100], Train Loss: 1.0528\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1030 batches: 1.053\n",
      "Epoch [11/25], Step [40/100], Train Loss: 1.0362\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1040 batches: 1.036\n",
      "Epoch [11/25], Step [50/100], Train Loss: 1.0344\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1050 batches: 1.034\n",
      "Epoch [11/25], Step [60/100], Train Loss: 1.0464\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1060 batches: 1.046\n",
      "Epoch [11/25], Step [70/100], Train Loss: 1.0373\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1070 batches: 1.037\n",
      "Epoch [11/25], Step [80/100], Train Loss: 1.0405\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1080 batches: 1.041\n",
      "Epoch [11/25], Step [90/100], Train Loss: 1.0482\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1090 batches: 1.048\n",
      "Epoch [11/25], Step [100/100], Train Loss: 1.0390\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1100 batches: 1.039\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8813\n",
      "         1.0       0.39      0.73      0.51      1087\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 84.767681%\n",
      "\n",
      "Epoch [12/25], Step [10/100], Train Loss: 1.0409\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1110 batches: 1.041\n",
      "Epoch [12/25], Step [20/100], Train Loss: 1.0344\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1120 batches: 1.034\n",
      "Epoch [12/25], Step [30/100], Train Loss: 1.0342\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1130 batches: 1.034\n",
      "Epoch [12/25], Step [40/100], Train Loss: 1.0484\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1140 batches: 1.048\n",
      "Epoch [12/25], Step [50/100], Train Loss: 1.0453\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1150 batches: 1.045\n",
      "Epoch [12/25], Step [60/100], Train Loss: 1.0337\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1160 batches: 1.034\n",
      "Epoch [12/25], Step [70/100], Train Loss: 1.0389\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1170 batches: 1.039\n",
      "Epoch [12/25], Step [80/100], Train Loss: 1.0587\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1180 batches: 1.059\n",
      "Epoch [12/25], Step [90/100], Train Loss: 1.0446\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1190 batches: 1.045\n",
      "Epoch [12/25], Step [100/100], Train Loss: 1.0407\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1200 batches: 1.041\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8812\n",
      "         1.0       0.40      0.72      0.51      1088\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 84.919196%\n",
      "\n",
      "Epoch [13/25], Step [10/100], Train Loss: 1.0266\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1210 batches: 1.027\n",
      "Epoch [13/25], Step [20/100], Train Loss: 1.0549\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1220 batches: 1.055\n",
      "Epoch [13/25], Step [30/100], Train Loss: 1.0394\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1230 batches: 1.039\n",
      "Epoch [13/25], Step [40/100], Train Loss: 1.0457\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1240 batches: 1.046\n",
      "Epoch [13/25], Step [50/100], Train Loss: 1.0315\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1250 batches: 1.031\n",
      "Epoch [13/25], Step [60/100], Train Loss: 1.0411\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1260 batches: 1.041\n",
      "Epoch [13/25], Step [70/100], Train Loss: 1.0563\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1270 batches: 1.056\n",
      "Epoch [13/25], Step [80/100], Train Loss: 1.0411\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1280 batches: 1.041\n",
      "Epoch [13/25], Step [90/100], Train Loss: 1.0406\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1290 batches: 1.041\n",
      "Epoch [13/25], Step [100/100], Train Loss: 1.0355\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1300 batches: 1.035\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8819\n",
      "         1.0       0.40      0.72      0.51      1081\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.020202%\n",
      "\n",
      "Epoch [14/25], Step [10/100], Train Loss: 1.0247\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1310 batches: 1.025\n",
      "Epoch [14/25], Step [20/100], Train Loss: 1.0403\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1320 batches: 1.040\n",
      "Epoch [14/25], Step [30/100], Train Loss: 1.0408\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1330 batches: 1.041\n",
      "Epoch [14/25], Step [40/100], Train Loss: 1.0495\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1340 batches: 1.049\n",
      "Epoch [14/25], Step [50/100], Train Loss: 1.0442\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1350 batches: 1.044\n",
      "Epoch [14/25], Step [60/100], Train Loss: 1.0469\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1360 batches: 1.047\n",
      "Epoch [14/25], Step [70/100], Train Loss: 1.0461\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1370 batches: 1.046\n",
      "Epoch [14/25], Step [80/100], Train Loss: 1.0424\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1380 batches: 1.042\n",
      "Epoch [14/25], Step [90/100], Train Loss: 1.0354\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1390 batches: 1.035\n",
      "Epoch [14/25], Step [100/100], Train Loss: 1.0482\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1400 batches: 1.048\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8818\n",
      "         1.0       0.40      0.72      0.51      1082\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.151517%\n",
      "\n",
      "Epoch [15/25], Step [10/100], Train Loss: 1.0455\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1410 batches: 1.046\n",
      "Epoch [15/25], Step [20/100], Train Loss: 1.0478\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1420 batches: 1.048\n",
      "Epoch [15/25], Step [30/100], Train Loss: 1.0434\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1430 batches: 1.043\n",
      "Epoch [15/25], Step [40/100], Train Loss: 1.0302\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1440 batches: 1.030\n",
      "Epoch [15/25], Step [50/100], Train Loss: 1.0311\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1450 batches: 1.031\n",
      "Epoch [15/25], Step [60/100], Train Loss: 1.0363\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1460 batches: 1.036\n",
      "Epoch [15/25], Step [70/100], Train Loss: 1.0470\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1470 batches: 1.047\n",
      "Epoch [15/25], Step [80/100], Train Loss: 1.0417\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1480 batches: 1.042\n",
      "Epoch [15/25], Step [90/100], Train Loss: 1.0495\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1490 batches: 1.050\n",
      "Epoch [15/25], Step [100/100], Train Loss: 1.0447\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1500 batches: 1.045\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8812\n",
      "         1.0       0.40      0.72      0.52      1088\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.010105%\n",
      "\n",
      "Epoch [16/25], Step [10/100], Train Loss: 1.0282\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1510 batches: 1.028\n",
      "Epoch [16/25], Step [20/100], Train Loss: 1.0389\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1520 batches: 1.039\n",
      "Epoch [16/25], Step [30/100], Train Loss: 1.0549\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1530 batches: 1.055\n",
      "Epoch [16/25], Step [40/100], Train Loss: 1.0442\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1540 batches: 1.044\n",
      "Epoch [16/25], Step [50/100], Train Loss: 1.0418\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1550 batches: 1.042\n",
      "Epoch [16/25], Step [60/100], Train Loss: 1.0369\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1560 batches: 1.037\n",
      "Epoch [16/25], Step [70/100], Train Loss: 1.0586\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1570 batches: 1.059\n",
      "Epoch [16/25], Step [80/100], Train Loss: 1.0393\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1580 batches: 1.039\n",
      "Epoch [16/25], Step [90/100], Train Loss: 1.0445\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1590 batches: 1.044\n",
      "Epoch [16/25], Step [100/100], Train Loss: 1.0209\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1600 batches: 1.021\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.88      0.92      8816\n",
      "         1.0       0.42      0.69      0.52      1084\n",
      "\n",
      "    accuracy                           0.86      9900\n",
      "   macro avg       0.69      0.78      0.72      9900\n",
      "weighted avg       0.90      0.86      0.87      9900\n",
      "\n",
      "Validation Accuracy: 86.020207%\n",
      "\n",
      "Epoch [17/25], Step [10/100], Train Loss: 1.0410\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1610 batches: 1.041\n",
      "Epoch [17/25], Step [20/100], Train Loss: 1.0358\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1620 batches: 1.036\n",
      "Epoch [17/25], Step [30/100], Train Loss: 1.0400\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1630 batches: 1.040\n",
      "Epoch [17/25], Step [40/100], Train Loss: 1.0383\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1640 batches: 1.038\n",
      "Epoch [17/25], Step [50/100], Train Loss: 1.0436\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1650 batches: 1.044\n",
      "Epoch [17/25], Step [60/100], Train Loss: 1.0401\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1660 batches: 1.040\n",
      "Epoch [17/25], Step [70/100], Train Loss: 1.0531\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1670 batches: 1.053\n",
      "Epoch [17/25], Step [80/100], Train Loss: 1.0596\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1680 batches: 1.060\n",
      "Epoch [17/25], Step [90/100], Train Loss: 1.0341\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1690 batches: 1.034\n",
      "Epoch [17/25], Step [100/100], Train Loss: 1.0267\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1700 batches: 1.027\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8814\n",
      "         1.0       0.40      0.71      0.51      1086\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.303032%\n",
      "\n",
      "Epoch [18/25], Step [10/100], Train Loss: 1.0450\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1710 batches: 1.045\n",
      "Epoch [18/25], Step [20/100], Train Loss: 1.0355\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1720 batches: 1.035\n",
      "Epoch [18/25], Step [30/100], Train Loss: 1.0463\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1730 batches: 1.046\n",
      "Epoch [18/25], Step [40/100], Train Loss: 1.0511\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1740 batches: 1.051\n",
      "Epoch [18/25], Step [50/100], Train Loss: 1.0455\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1750 batches: 1.046\n",
      "Epoch [18/25], Step [60/100], Train Loss: 1.0262\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1760 batches: 1.026\n",
      "Epoch [18/25], Step [70/100], Train Loss: 1.0328\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1770 batches: 1.033\n",
      "Epoch [18/25], Step [80/100], Train Loss: 1.0507\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1780 batches: 1.051\n",
      "Epoch [18/25], Step [90/100], Train Loss: 1.0304\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1790 batches: 1.030\n",
      "Epoch [18/25], Step [100/100], Train Loss: 1.0391\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1800 batches: 1.039\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8814\n",
      "         1.0       0.40      0.71      0.51      1086\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.373741%\n",
      "\n",
      "Epoch [19/25], Step [10/100], Train Loss: 1.0560\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1810 batches: 1.056\n",
      "Epoch [19/25], Step [20/100], Train Loss: 1.0467\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1820 batches: 1.047\n",
      "Epoch [19/25], Step [30/100], Train Loss: 1.0239\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1830 batches: 1.024\n",
      "Epoch [19/25], Step [40/100], Train Loss: 1.0317\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1840 batches: 1.032\n",
      "Epoch [19/25], Step [50/100], Train Loss: 1.0409\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1850 batches: 1.041\n",
      "Epoch [19/25], Step [60/100], Train Loss: 1.0333\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1860 batches: 1.033\n",
      "Epoch [19/25], Step [70/100], Train Loss: 1.0309\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1870 batches: 1.031\n",
      "Epoch [19/25], Step [80/100], Train Loss: 1.0429\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1880 batches: 1.043\n",
      "Epoch [19/25], Step [90/100], Train Loss: 1.0446\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1890 batches: 1.045\n",
      "Epoch [19/25], Step [100/100], Train Loss: 1.0550\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1900 batches: 1.055\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8818\n",
      "         1.0       0.40      0.71      0.51      1082\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.393941%\n",
      "\n",
      "Epoch [20/25], Step [10/100], Train Loss: 1.0276\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1910 batches: 1.028\n",
      "Epoch [20/25], Step [20/100], Train Loss: 1.0290\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1920 batches: 1.029\n",
      "Epoch [20/25], Step [30/100], Train Loss: 1.0460\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1930 batches: 1.046\n",
      "Epoch [20/25], Step [40/100], Train Loss: 1.0328\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1940 batches: 1.033\n",
      "Epoch [20/25], Step [50/100], Train Loss: 1.0429\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1950 batches: 1.043\n",
      "Epoch [20/25], Step [60/100], Train Loss: 1.0337\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1960 batches: 1.034\n",
      "Epoch [20/25], Step [70/100], Train Loss: 1.0468\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1970 batches: 1.047\n",
      "Epoch [20/25], Step [80/100], Train Loss: 1.0471\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1980 batches: 1.047\n",
      "Epoch [20/25], Step [90/100], Train Loss: 1.0530\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1990 batches: 1.053\n",
      "Epoch [20/25], Step [100/100], Train Loss: 1.0361\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 2000 batches: 1.036\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8826\n",
      "         1.0       0.40      0.71      0.51      1074\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.323232%\n",
      "\n",
      "Epoch [21/25], Step [10/100], Train Loss: 1.0388\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2010 batches: 1.039\n",
      "Epoch [21/25], Step [20/100], Train Loss: 1.0403\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2020 batches: 1.040\n",
      "Epoch [21/25], Step [30/100], Train Loss: 1.0318\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2030 batches: 1.032\n",
      "Epoch [21/25], Step [40/100], Train Loss: 1.0365\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2040 batches: 1.036\n",
      "Epoch [21/25], Step [50/100], Train Loss: 1.0399\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2050 batches: 1.040\n",
      "Epoch [21/25], Step [60/100], Train Loss: 1.0380\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2060 batches: 1.038\n",
      "Epoch [21/25], Step [70/100], Train Loss: 1.0362\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2070 batches: 1.036\n",
      "Epoch [21/25], Step [80/100], Train Loss: 1.0390\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2080 batches: 1.039\n",
      "Epoch [21/25], Step [90/100], Train Loss: 1.0348\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2090 batches: 1.035\n",
      "Epoch [21/25], Step [100/100], Train Loss: 1.0529\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2100 batches: 1.053\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8821\n",
      "         1.0       0.40      0.71      0.51      1079\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.323232%\n",
      "\n",
      "Epoch [22/25], Step [10/100], Train Loss: 1.0554\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2110 batches: 1.055\n",
      "Epoch [22/25], Step [20/100], Train Loss: 1.0231\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2120 batches: 1.023\n",
      "Epoch [22/25], Step [30/100], Train Loss: 1.0522\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2130 batches: 1.052\n",
      "Epoch [22/25], Step [40/100], Train Loss: 1.0410\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2140 batches: 1.041\n",
      "Epoch [22/25], Step [50/100], Train Loss: 1.0415\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2150 batches: 1.041\n",
      "Epoch [22/25], Step [60/100], Train Loss: 1.0406\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2160 batches: 1.041\n",
      "Epoch [22/25], Step [70/100], Train Loss: 1.0282\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2170 batches: 1.028\n",
      "Epoch [22/25], Step [80/100], Train Loss: 1.0369\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2180 batches: 1.037\n",
      "Epoch [22/25], Step [90/100], Train Loss: 1.0428\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2190 batches: 1.043\n",
      "Epoch [22/25], Step [100/100], Train Loss: 1.0397\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2200 batches: 1.040\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8822\n",
      "         1.0       0.40      0.72      0.51      1078\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 84.939396%\n",
      "\n",
      "Epoch [23/25], Step [10/100], Train Loss: 1.0560\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2210 batches: 1.056\n",
      "Epoch [23/25], Step [20/100], Train Loss: 1.0300\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2220 batches: 1.030\n",
      "Epoch [23/25], Step [30/100], Train Loss: 1.0361\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2230 batches: 1.036\n",
      "Epoch [23/25], Step [40/100], Train Loss: 1.0444\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2240 batches: 1.044\n",
      "Epoch [23/25], Step [50/100], Train Loss: 1.0318\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2250 batches: 1.032\n",
      "Epoch [23/25], Step [60/100], Train Loss: 1.0333\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2260 batches: 1.033\n",
      "Epoch [23/25], Step [70/100], Train Loss: 1.0339\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2270 batches: 1.034\n",
      "Epoch [23/25], Step [80/100], Train Loss: 1.0445\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2280 batches: 1.045\n",
      "Epoch [23/25], Step [90/100], Train Loss: 1.0331\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2290 batches: 1.033\n",
      "Epoch [23/25], Step [100/100], Train Loss: 1.0455\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2300 batches: 1.046\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8820\n",
      "         1.0       0.40      0.71      0.51      1080\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.303032%\n",
      "\n",
      "Epoch [24/25], Step [10/100], Train Loss: 1.0410\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2310 batches: 1.041\n",
      "Epoch [24/25], Step [20/100], Train Loss: 1.0352\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2320 batches: 1.035\n",
      "Epoch [24/25], Step [30/100], Train Loss: 1.0353\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2330 batches: 1.035\n",
      "Epoch [24/25], Step [40/100], Train Loss: 1.0261\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2340 batches: 1.026\n",
      "Epoch [24/25], Step [50/100], Train Loss: 1.0283\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2350 batches: 1.028\n",
      "Epoch [24/25], Step [60/100], Train Loss: 1.0425\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2360 batches: 1.043\n",
      "Epoch [24/25], Step [70/100], Train Loss: 1.0467\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2370 batches: 1.047\n",
      "Epoch [24/25], Step [80/100], Train Loss: 1.0339\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2380 batches: 1.034\n",
      "Epoch [24/25], Step [90/100], Train Loss: 1.0673\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2390 batches: 1.067\n",
      "Epoch [24/25], Step [100/100], Train Loss: 1.0421\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2400 batches: 1.042\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8811\n",
      "         1.0       0.41      0.71      0.52      1089\n",
      "\n",
      "    accuracy                           0.86      9900\n",
      "   macro avg       0.68      0.79      0.72      9900\n",
      "weighted avg       0.90      0.86      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.505050%\n",
      "\n",
      "Epoch [25/25], Step [10/100], Train Loss: 1.0366\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2410 batches: 1.037\n",
      "Epoch [25/25], Step [20/100], Train Loss: 1.0474\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2420 batches: 1.047\n",
      "Epoch [25/25], Step [30/100], Train Loss: 1.0531\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2430 batches: 1.053\n",
      "Epoch [25/25], Step [40/100], Train Loss: 1.0355\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2440 batches: 1.036\n",
      "Epoch [25/25], Step [50/100], Train Loss: 1.0214\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2450 batches: 1.021\n",
      "Epoch [25/25], Step [60/100], Train Loss: 1.0347\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2460 batches: 1.035\n",
      "Epoch [25/25], Step [70/100], Train Loss: 1.0342\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2470 batches: 1.034\n",
      "Epoch [25/25], Step [80/100], Train Loss: 1.0343\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2480 batches: 1.034\n",
      "Epoch [25/25], Step [90/100], Train Loss: 1.0442\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2490 batches: 1.044\n",
      "Epoch [25/25], Step [100/100], Train Loss: 1.0442\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2500 batches: 1.044\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8819\n",
      "         1.0       0.41      0.71      0.52      1081\n",
      "\n",
      "    accuracy                           0.86      9900\n",
      "   macro avg       0.68      0.79      0.72      9900\n",
      "weighted avg       0.90      0.86      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.505050%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(model, train_loader, val_loader, criterion, optimizer, scheduler, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on 10800 transactions in test data: 85.314815%\n",
      "f1_score_test:  0.5012578616352201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      9687\n",
      "         1.0       0.39      0.72      0.50      1113\n",
      "\n",
      "    accuracy                           0.85     10800\n",
      "   macro avg       0.67      0.79      0.71     10800\n",
      "weighted avg       0.90      0.85      0.87     10800\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model -> no need to compute gradients (for memory efficiency)\n",
    "test(model, test_loader, device, config.project_name, save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embedding.weight tensor([[ 0.0021,  0.2726,  0.1501,  ..., -0.1496, -0.1056, -0.0765],\n",
      "        [ 0.2669, -0.2252, -0.2506,  ...,  0.1740,  0.2594,  0.0882],\n",
      "        [ 0.1301,  0.1836,  0.1049,  ..., -0.0616,  0.0688,  0.0646],\n",
      "        ...,\n",
      "        [-0.1037, -0.2203,  0.1484,  ..., -0.1795, -0.2250, -0.1400],\n",
      "        [-0.2979, -0.1337,  0.0458,  ...,  0.1881, -0.0184, -0.1529],\n",
      "        [ 0.1631, -0.1515, -0.2128,  ..., -0.2663,  0.0965, -0.1101]],\n",
      "       device='cuda:0')\n",
      "position_embedding.weight tensor([[-1.4740e-01, -3.1109e-01,  1.6756e-01,  4.7371e-02, -3.2034e-01,\n",
      "          1.3223e-01,  1.6921e-01,  4.4159e-02,  1.2460e-02,  1.6204e-01,\n",
      "         -2.5367e-01,  2.6116e-01, -2.0303e-01, -1.6858e-01,  2.0410e-01,\n",
      "          9.3142e-02,  2.9821e-01,  4.1964e-03, -1.7384e-01, -1.3079e-01,\n",
      "         -2.6247e-01,  1.1906e-01, -7.3606e-02, -1.3022e-01,  1.4187e-01,\n",
      "         -1.4397e-02, -6.0359e-02, -1.6356e-01,  4.1786e-02, -6.8856e-02,\n",
      "         -1.4135e-01, -2.0822e-01, -5.7330e-02, -1.4183e-01, -1.4752e-01,\n",
      "         -6.6964e-03,  2.0546e-01,  3.9232e-02,  2.7112e-02,  2.8150e-02,\n",
      "         -3.0273e-01,  4.8103e-02, -6.5909e-02, -5.2739e-02, -3.4491e-02,\n",
      "         -1.0939e-01, -9.0499e-02, -2.5707e-01, -2.1300e-01,  1.8914e-01,\n",
      "         -4.2433e-02, -4.9401e-02, -7.9835e-02,  6.1928e-02,  2.0895e-01,\n",
      "          2.6310e-01, -2.2664e-01, -4.5310e-02,  1.0921e-01,  1.9405e-02,\n",
      "         -1.7482e-01, -9.5707e-02,  1.1078e-01, -2.1243e-01],\n",
      "        [ 2.0090e-02,  8.8184e-02, -2.0891e-01,  2.5500e-01,  2.4425e-01,\n",
      "         -2.1032e-01,  8.0797e-02, -6.3644e-02,  1.1724e-01,  1.6434e-01,\n",
      "         -2.3782e-01, -1.8368e-01, -1.6980e-01, -2.1618e-01,  1.3534e-01,\n",
      "         -9.0029e-02,  7.2198e-02, -2.1267e-01,  8.6169e-02,  7.5972e-02,\n",
      "          7.9676e-02, -9.0447e-02, -2.1655e-01, -2.7295e-01,  1.3859e-01,\n",
      "          2.5725e-01,  1.8058e-01,  7.2849e-03,  1.0260e-01,  2.9426e-01,\n",
      "         -3.2665e-01,  1.2164e-01,  2.2247e-01,  3.6230e-02, -1.3878e-01,\n",
      "          1.0574e-01, -1.2586e-01,  1.8936e-01,  1.9804e-02,  1.5116e-01,\n",
      "          2.8400e-01,  2.5355e-01,  2.1833e-01,  3.5845e-02,  5.8405e-02,\n",
      "          1.8210e-02, -4.0151e-02,  1.3844e-01,  1.4943e-01,  9.0017e-02,\n",
      "          1.7041e-02, -1.7427e-02,  6.4998e-02, -2.3068e-01, -6.9655e-02,\n",
      "          1.7349e-01,  2.3823e-01,  2.3774e-01,  1.5057e-01,  2.8592e-01,\n",
      "         -1.0705e-01,  9.4894e-02, -1.0064e-01,  1.4458e-01],\n",
      "        [ 3.3501e-02, -3.3996e-01, -3.2906e-01,  3.0537e-01, -1.7488e-01,\n",
      "          3.1829e-01, -2.3598e-01, -1.5279e-01, -1.5358e-01,  1.2599e-01,\n",
      "         -2.8890e-01, -8.2083e-02, -3.6543e-02,  1.9877e-01, -9.0021e-02,\n",
      "          4.8540e-02,  1.5642e-01,  1.1009e-01,  1.4852e-01, -1.6428e-01,\n",
      "          6.5708e-02,  1.4043e-01,  1.6656e-01,  2.6473e-01, -2.2603e-01,\n",
      "          1.3275e-02,  1.6380e-01, -2.7371e-01, -2.9536e-02,  2.1612e-02,\n",
      "          1.7104e-01, -1.0467e-01, -2.2926e-01,  1.2754e-01, -2.8264e-01,\n",
      "          3.0600e-01, -1.6266e-01,  6.9477e-02, -9.0017e-02,  2.0109e-01,\n",
      "         -2.7455e-01,  2.4168e-01, -2.8017e-01,  5.3000e-02, -5.8456e-02,\n",
      "          1.4160e-01, -8.3387e-02, -1.9497e-01, -1.0484e-01,  1.6463e-01,\n",
      "         -2.2001e-01,  1.0037e-01, -2.5597e-01,  2.8729e-02, -1.7682e-01,\n",
      "          1.0849e-01, -2.4757e-01,  2.2118e-01,  2.8561e-01, -1.5091e-02,\n",
      "         -2.2077e-02,  1.2993e-01,  2.7189e-01, -7.6410e-02],\n",
      "        [-1.2035e-01,  3.0721e-01, -2.0523e-01,  2.5849e-01, -7.5045e-02,\n",
      "         -2.5231e-01,  1.5680e-01, -2.7941e-01,  1.3775e-01, -2.2961e-01,\n",
      "          2.8763e-02,  3.7969e-02,  9.0686e-02, -2.4637e-01,  2.2430e-01,\n",
      "         -1.8248e-02, -1.3905e-01, -2.6491e-01, -7.3202e-03, -2.3779e-01,\n",
      "          2.2774e-01,  1.7072e-01, -2.5503e-02, -7.0059e-02,  9.9834e-02,\n",
      "          2.4250e-01, -1.9578e-01,  9.3230e-02,  8.2636e-02,  7.5726e-02,\n",
      "         -1.9721e-01, -2.0504e-01, -2.2942e-01, -1.5014e-01,  9.7913e-02,\n",
      "         -1.4272e-01,  2.4137e-01,  1.6145e-01, -1.1804e-01, -5.9725e-02,\n",
      "         -2.3812e-01, -4.4468e-03,  3.0145e-01, -3.2926e-01,  4.0942e-03,\n",
      "         -2.3378e-01,  1.0122e-01,  1.5558e-01,  2.4369e-01, -6.3757e-02,\n",
      "          2.9050e-01, -8.3356e-02,  1.4600e-01, -2.5060e-01, -2.6908e-01,\n",
      "          2.5374e-01, -1.0666e-01, -2.0496e-01,  2.4570e-01,  8.8608e-02,\n",
      "          5.3959e-02,  6.8855e-02,  2.9206e-01, -1.4222e-01],\n",
      "        [ 1.0459e-01,  1.6788e-01, -1.0260e-01,  1.5855e-01, -1.6911e-01,\n",
      "          2.8129e-01, -3.0475e-01,  2.4438e-01,  1.8897e-01,  1.9541e-01,\n",
      "         -1.1986e-01, -1.0972e-01,  2.3262e-01, -2.2353e-01,  6.6084e-03,\n",
      "         -1.5360e-01, -7.0531e-02, -1.8370e-01,  5.0590e-02, -1.3195e-01,\n",
      "          9.2659e-02, -3.0999e-01,  2.3217e-01, -2.5151e-02, -1.5647e-01,\n",
      "          3.1624e-01, -2.3341e-01,  2.0550e-01, -2.7200e-02, -3.0765e-01,\n",
      "         -3.0818e-01, -2.2695e-02, -1.3636e-01,  5.4958e-02,  1.4276e-01,\n",
      "         -1.6350e-01,  2.5582e-01, -6.5353e-02, -1.4166e-01, -2.9081e-01,\n",
      "          1.7297e-01,  1.1590e-01,  1.6292e-01, -1.0232e-01, -4.6824e-02,\n",
      "          2.7791e-01, -3.1812e-01,  2.3062e-01,  1.3948e-01,  5.9243e-03,\n",
      "          3.6545e-02, -1.5231e-01,  2.0229e-01,  3.3882e-04,  2.0981e-01,\n",
      "          1.6760e-01,  2.3171e-01,  6.7181e-02,  2.8695e-01,  2.8298e-01,\n",
      "          1.8516e-01,  1.5914e-01, -1.9791e-01,  2.2697e-01],\n",
      "        [ 1.3621e-01, -1.2162e-01,  1.3500e-03, -6.6944e-02, -1.3557e-01,\n",
      "          2.4633e-01, -1.4956e-01,  2.8160e-01,  6.4785e-02,  2.5613e-01,\n",
      "          4.9777e-02,  4.3654e-02, -1.3936e-01,  9.9083e-02,  4.1111e-02,\n",
      "          2.2624e-01,  3.2828e-01, -2.7722e-01, -4.2717e-02, -3.3706e-02,\n",
      "         -7.4578e-03,  1.4762e-01, -1.8838e-01, -8.3640e-02,  2.4171e-01,\n",
      "         -1.2256e-01, -2.8687e-01,  2.9836e-01,  2.0231e-02, -4.8457e-02,\n",
      "          1.5147e-01, -9.7464e-02, -1.0171e-01,  2.3736e-02, -1.7208e-01,\n",
      "          1.6753e-01, -1.2456e-01, -2.8417e-01, -1.6590e-01,  2.5922e-01,\n",
      "         -2.0221e-01,  2.0791e-01, -1.8645e-01, -3.2223e-01,  9.6339e-02,\n",
      "          2.8029e-01, -2.8773e-01, -9.8982e-02, -3.2766e-02,  2.9393e-01,\n",
      "         -2.4541e-01, -2.7262e-01,  1.9623e-01,  2.7733e-01, -8.9881e-02,\n",
      "         -2.0215e-01, -1.7271e-01,  2.7564e-01, -2.1712e-01,  1.4765e-02,\n",
      "         -1.7506e-01,  2.2249e-01,  2.2030e-01, -2.9846e-01],\n",
      "        [-2.2024e-01,  1.6957e-01, -2.2624e-01, -1.2032e-01, -1.3319e-01,\n",
      "         -2.0497e-01,  2.3346e-01,  1.3636e-02, -1.0775e-01,  2.4730e-01,\n",
      "          4.6715e-02,  1.9990e-01,  8.0319e-02,  1.5574e-01, -1.1868e-01,\n",
      "          2.1361e-01, -2.9579e-02,  3.4058e-01, -2.1492e-01, -1.9334e-01,\n",
      "          2.0685e-02,  7.9781e-02, -4.6726e-02,  1.5322e-01, -2.0176e-01,\n",
      "         -1.7338e-01,  1.4381e-01, -2.2637e-01, -8.9083e-02,  2.4645e-01,\n",
      "          1.4811e-01,  7.2266e-02,  2.0647e-02,  2.0347e-01,  1.6845e-01,\n",
      "         -4.7100e-02,  7.6761e-02,  1.2907e-02,  2.6807e-01,  2.2641e-01,\n",
      "         -2.9045e-01,  9.8531e-02, -6.3247e-02,  1.8199e-01,  2.3789e-01,\n",
      "         -5.9468e-02, -1.3259e-01,  1.4394e-02, -1.1698e-01,  2.7948e-01,\n",
      "         -1.8614e-01,  2.5628e-01, -9.5931e-02,  1.9634e-01,  1.8954e-01,\n",
      "          4.0014e-02,  9.6675e-03,  1.8092e-01, -1.6025e-01, -2.6364e-01,\n",
      "          1.9747e-01, -2.2572e-01, -1.6460e-01, -9.6800e-02],\n",
      "        [ 1.7910e-01, -1.0398e-01, -2.5627e-01, -5.6019e-02, -8.7821e-02,\n",
      "         -8.1229e-02, -3.1179e-01,  4.3909e-02,  1.0784e-01,  8.2522e-02,\n",
      "         -1.0506e-01,  1.0506e-01,  2.3395e-01, -4.2609e-03,  1.9900e-01,\n",
      "         -8.0655e-02, -2.6097e-01,  3.5551e-02, -5.0550e-02,  8.1591e-02,\n",
      "         -2.9157e-01, -1.7880e-01, -2.0559e-01,  8.6192e-02,  4.7263e-02,\n",
      "         -1.1380e-01, -1.3926e-01, -8.9054e-02,  2.1411e-01, -2.2853e-01,\n",
      "         -3.2776e-01, -1.5232e-02,  3.3522e-02, -3.8263e-02,  1.8078e-02,\n",
      "         -3.0101e-02, -2.4197e-01,  2.2249e-01,  8.6280e-02,  9.7492e-02,\n",
      "         -9.9061e-02,  8.6831e-02, -2.9695e-02, -1.5299e-01, -2.5366e-01,\n",
      "          2.0803e-01,  8.4778e-02, -2.3536e-01,  3.8086e-04, -1.5364e-01,\n",
      "         -2.4591e-01,  8.4935e-02,  1.5185e-01,  1.3062e-01,  2.5836e-01,\n",
      "         -2.2298e-01, -2.9922e-03, -2.2595e-01, -2.8082e-02,  1.4786e-01,\n",
      "         -8.0935e-02,  2.0359e-01, -2.1781e-01,  1.3513e-01],\n",
      "        [-9.6172e-02,  1.2752e-01,  5.1315e-02, -9.8932e-02,  2.5486e-01,\n",
      "         -1.1276e-01, -3.9929e-02,  2.5038e-01,  1.5087e-01,  1.3929e-01,\n",
      "         -2.3431e-01, -3.6788e-02, -2.3150e-01,  2.0836e-01, -1.7071e-01,\n",
      "         -1.3427e-01,  2.0457e-01,  3.7844e-02,  1.4954e-01, -1.4510e-01,\n",
      "          1.9699e-01, -3.9547e-02,  1.5603e-01,  1.3099e-01,  2.3981e-01,\n",
      "         -3.0317e-02, -2.1563e-01, -8.7475e-02, -2.7362e-01,  2.3668e-01,\n",
      "         -1.9159e-02,  2.5696e-01,  1.1791e-01,  2.1919e-02,  1.7605e-01,\n",
      "          6.9509e-02, -1.5985e-01, -1.8021e-01,  2.9206e-02,  2.6249e-01,\n",
      "          7.6298e-02,  3.7252e-02, -4.3357e-02,  3.5606e-02, -1.3393e-01,\n",
      "         -1.0360e-01, -2.7198e-01, -6.5941e-02,  2.4954e-01,  1.8207e-01,\n",
      "         -1.6277e-01, -8.6996e-03, -2.9910e-02, -4.1383e-02, -1.5496e-01,\n",
      "          5.1517e-02, -2.4914e-01,  1.3907e-01,  1.6683e-01, -1.5841e-01,\n",
      "          2.1770e-01, -1.1775e-01,  2.2571e-01,  1.7251e-01],\n",
      "        [ 1.6389e-01,  2.7211e-01,  8.9366e-02, -8.1583e-02,  1.5145e-01,\n",
      "          1.0633e-01, -1.9676e-01, -3.0069e-03, -2.6928e-01,  1.4856e-01,\n",
      "         -1.4121e-01,  2.1818e-01,  2.1768e-01, -1.6743e-01, -8.5488e-02,\n",
      "          4.5539e-02, -1.5385e-01, -4.4563e-02,  2.0130e-01, -3.0438e-01,\n",
      "          1.0817e-01, -1.2614e-01, -2.3400e-01, -1.3624e-01,  1.9598e-01,\n",
      "         -1.2547e-01, -1.7673e-01,  1.4956e-02,  2.7276e-01,  2.2685e-01,\n",
      "          1.0402e-01, -1.8647e-01, -2.1926e-01,  6.5033e-03, -5.2701e-02,\n",
      "          2.2459e-01,  8.9664e-02,  1.5949e-01,  6.2983e-02,  3.4788e-01,\n",
      "          5.5448e-02,  8.5405e-02, -1.9240e-01, -1.0998e-01, -7.5260e-02,\n",
      "          1.9555e-01, -1.0067e-01, -1.4871e-01,  1.0054e-01,  3.0525e-01,\n",
      "         -4.7165e-02, -7.8835e-02,  4.3678e-02, -3.0254e-01, -1.5187e-01,\n",
      "          2.4459e-01,  2.3390e-01, -1.6028e-01, -7.9794e-02,  4.2347e-02,\n",
      "          2.8085e-01,  5.5859e-02,  2.1567e-01,  1.4427e-01]], device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.in_proj_weight tensor([[-0.1476, -0.3005, -0.1814,  ..., -0.3016, -0.2980,  0.2851],\n",
      "        [-0.0148, -0.0467, -0.0183,  ...,  0.1441, -0.0968, -0.1944],\n",
      "        [-0.0273, -0.0155, -0.2919,  ..., -0.2319, -0.2750, -0.1578],\n",
      "        ...,\n",
      "        [-0.1497,  0.3032, -0.1352,  ...,  0.0533, -0.0370,  0.1758],\n",
      "        [-0.2468,  0.0602, -0.2959,  ..., -0.0460,  0.2507,  0.1000],\n",
      "        [ 0.1012, -0.2901, -0.1751,  ...,  0.1554, -0.2481,  0.2198]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.in_proj_bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.out_proj.weight tensor([[ 0.1487,  0.2320, -0.2907,  ..., -0.0907,  0.2265, -0.2240],\n",
      "        [-0.1337, -0.2690, -0.0590,  ...,  0.2205,  0.0799,  0.0518],\n",
      "        [ 0.2777,  0.1541,  0.0477,  ..., -0.0406, -0.2324, -0.1632],\n",
      "        ...,\n",
      "        [-0.1204,  0.2462,  0.2462,  ..., -0.0922,  0.0175, -0.0494],\n",
      "        [-0.0247, -0.2246, -0.0724,  ..., -0.1446, -0.0796,  0.2754],\n",
      "        [ 0.0392, -0.2127,  0.1141,  ...,  0.0046, -0.1321,  0.2621]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.out_proj.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear1.weight tensor([[ 2.5749e-01, -2.7425e-01, -1.2951e-01,  ...,  2.2394e-01,\n",
      "          2.2446e-01,  1.5474e-02],\n",
      "        [ 1.4055e-01, -1.0659e-01, -5.3375e-02,  ...,  2.7899e-01,\n",
      "          2.4619e-04, -2.3291e-01],\n",
      "        [ 1.2942e-01, -3.8130e-02,  1.9349e-01,  ...,  2.9204e-01,\n",
      "          3.6153e-02, -9.9486e-02],\n",
      "        ...,\n",
      "        [-7.1937e-03, -2.7092e-01, -1.6035e-01,  ...,  2.4438e-02,\n",
      "          2.0488e-01, -1.7477e-01],\n",
      "        [ 3.0598e-01, -5.6677e-02, -2.5635e-01,  ...,  2.3766e-01,\n",
      "         -3.0514e-01, -2.7449e-02],\n",
      "        [-2.6852e-01, -1.1278e-01, -1.4628e-02,  ...,  1.1191e-01,\n",
      "          1.1213e-01, -1.0149e-01]], device='cuda:0')\n",
      "transformer_encoder_layer.linear1.bias tensor([-0.0463, -0.0482,  0.0280, -0.0686, -0.0988, -0.0673, -0.0733, -0.1123,\n",
      "        -0.0716,  0.0571,  0.1165, -0.0762, -0.0144, -0.1066, -0.0080,  0.1027,\n",
      "        -0.0611,  0.0536,  0.1062,  0.0387, -0.0515, -0.0200, -0.0940,  0.0399,\n",
      "         0.0301, -0.0883,  0.1065,  0.0029, -0.1145,  0.0316, -0.0973,  0.0511,\n",
      "         0.0636, -0.0207, -0.0341, -0.0081, -0.0718,  0.0273,  0.0333,  0.0698,\n",
      "        -0.0483, -0.0552,  0.0185, -0.1165, -0.1079,  0.0433, -0.0879, -0.0317,\n",
      "         0.0933,  0.0965,  0.0585, -0.0246, -0.0218, -0.0746, -0.1119,  0.0030,\n",
      "         0.0556, -0.0229, -0.0357,  0.0844, -0.0087, -0.0609, -0.0826, -0.0518,\n",
      "        -0.0676,  0.0238, -0.0467,  0.0871, -0.0747, -0.0232,  0.1203, -0.0364,\n",
      "        -0.0027,  0.0726, -0.0455,  0.1057, -0.0288,  0.0978,  0.1038,  0.0072,\n",
      "         0.0761,  0.0497, -0.0066,  0.0953,  0.0793, -0.0723, -0.0546, -0.1200,\n",
      "        -0.0167, -0.0212, -0.0566, -0.0174, -0.0520, -0.1052,  0.0992, -0.0654,\n",
      "         0.0116, -0.0828,  0.0085,  0.0465, -0.0938, -0.0972, -0.1148,  0.0231,\n",
      "         0.0670, -0.0639,  0.0211, -0.0451, -0.0958,  0.0288, -0.0856,  0.0533,\n",
      "        -0.0920, -0.1119,  0.1213,  0.0934,  0.0118,  0.1027, -0.0153,  0.0982,\n",
      "        -0.1152,  0.1039,  0.0972, -0.0193,  0.0395, -0.0322,  0.0492,  0.0763],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear2.weight tensor([[-0.1747,  0.0845, -0.0688,  ...,  0.1898, -0.1528, -0.0901],\n",
      "        [ 0.0593,  0.0659, -0.1302,  ...,  0.0719,  0.0838, -0.1401],\n",
      "        [ 0.0562, -0.2106, -0.0843,  ...,  0.1209, -0.0598,  0.0050],\n",
      "        ...,\n",
      "        [-0.1998,  0.1394,  0.0405,  ...,  0.0565,  0.1653, -0.0085],\n",
      "        [ 0.1939,  0.0709, -0.0508,  ...,  0.1547, -0.0090, -0.1602],\n",
      "        [ 0.0446,  0.1169,  0.0941,  ..., -0.1367, -0.0798,  0.0024]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear2.bias tensor([-0.0250, -0.0356, -0.0302, -0.0182,  0.0239, -0.0150, -0.0436,  0.0533,\n",
      "        -0.0392,  0.0825,  0.0443,  0.0853,  0.0649,  0.0379, -0.0486, -0.0697,\n",
      "        -0.0690,  0.0348, -0.0701, -0.0048,  0.0772, -0.0179, -0.0242, -0.0384,\n",
      "         0.0057, -0.0359,  0.0193, -0.0722, -0.0200,  0.0483, -0.0269,  0.0178,\n",
      "        -0.0670, -0.0004, -0.0875, -0.0856, -0.0217, -0.0125,  0.0412,  0.0806,\n",
      "        -0.0735,  0.0319, -0.0293, -0.0366,  0.0524,  0.0366,  0.0078,  0.0342,\n",
      "        -0.0061, -0.0867, -0.0822, -0.0341, -0.0865,  0.0645, -0.0124, -0.0784,\n",
      "        -0.0080, -0.0482, -0.0390,  0.0607,  0.0062,  0.0677, -0.0767,  0.0464],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.norm1.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder_layer.norm1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.norm2.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder_layer.norm2.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_weight tensor([[ 0.1025,  0.1264, -0.1988,  ...,  0.2389, -0.0629, -0.1665],\n",
      "        [ 0.1649,  0.1012, -0.0624,  ..., -0.2861, -0.2606,  0.2793],\n",
      "        [ 0.1598, -0.2582,  0.1704,  ...,  0.2550,  0.2512,  0.3078],\n",
      "        ...,\n",
      "        [-0.0935,  0.1576,  0.2713,  ..., -0.3310,  0.2422,  0.1320],\n",
      "        [-0.0540, -0.0925, -0.0590,  ..., -0.0276,  0.1853, -0.1996],\n",
      "        [ 0.2521,  0.1786,  0.2645,  ..., -0.2329,  0.2861,  0.2561]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_bias tensor([ 3.6983e-02,  8.7501e-02, -4.7895e-03, -3.5819e-03, -5.9956e-03,\n",
      "         8.7751e-02,  6.0653e-02,  2.3519e-02,  2.9230e-02,  8.3561e-03,\n",
      "        -2.5806e-02, -2.5991e-02,  1.0706e-04, -4.1082e-02, -1.3936e-02,\n",
      "        -2.4643e-02, -7.3256e-02, -4.2004e-02, -5.2883e-02,  1.2392e-02,\n",
      "        -7.0122e-02, -1.5555e-02, -9.5075e-03,  4.0848e-03,  8.3145e-03,\n",
      "         4.0624e-03,  5.9968e-02, -1.0938e-02,  4.0401e-02,  2.0859e-02,\n",
      "        -1.2697e-03, -3.0177e-02,  6.1642e-02,  1.5981e-02, -1.9508e-02,\n",
      "         1.6702e-02, -2.0775e-02,  6.7612e-02,  2.1024e-02,  5.7406e-02,\n",
      "         3.3444e-02, -4.3222e-04,  8.4380e-03, -5.1988e-02,  3.0644e-02,\n",
      "         7.4070e-03, -5.9920e-03, -3.0404e-02, -3.3036e-02, -4.4215e-02,\n",
      "        -4.8119e-02,  4.7516e-02, -2.6947e-02, -2.1347e-02,  5.0255e-02,\n",
      "         7.6368e-02, -6.7322e-02, -4.6951e-02, -5.2340e-02, -1.4094e-02,\n",
      "         5.5670e-02, -4.4673e-02,  2.4158e-03,  4.0099e-03,  4.8877e-04,\n",
      "        -5.7743e-04, -6.7292e-05,  1.1630e-04,  2.5911e-04,  5.1797e-04,\n",
      "         4.2662e-04, -8.8963e-04,  9.3199e-04, -6.7800e-04,  3.6492e-04,\n",
      "        -4.3524e-04,  3.3203e-04,  9.3067e-05,  3.3997e-04,  2.8274e-04,\n",
      "         6.1416e-05, -1.3872e-03,  1.0798e-03,  3.6314e-04, -4.2809e-04,\n",
      "         9.0516e-04,  5.0723e-04,  5.0747e-05,  5.9730e-04,  1.0231e-04,\n",
      "         3.0754e-04, -7.1501e-04,  2.6460e-04, -1.5670e-04, -1.1357e-04,\n",
      "        -1.4042e-04,  6.6592e-04, -7.4643e-04,  6.2638e-04, -7.0013e-04,\n",
      "         1.1737e-04, -5.1749e-04,  1.3249e-04,  1.4746e-04, -8.7961e-04,\n",
      "        -2.3373e-04,  1.5550e-03,  1.2083e-03, -3.3559e-04, -8.0784e-04,\n",
      "         9.9157e-04, -9.4549e-04,  4.8760e-05, -3.9849e-05,  3.3228e-04,\n",
      "         1.8102e-04, -8.2568e-04, -2.1001e-04, -1.7043e-05,  1.1687e-03,\n",
      "        -1.5322e-03, -1.6713e-04,  1.9133e-03,  1.1436e-03, -5.5900e-04,\n",
      "        -8.1326e-04,  1.3330e-03, -6.4583e-04,  3.6259e-02,  5.1429e-03,\n",
      "        -4.5448e-03, -1.4784e-02, -1.9094e-05, -1.1975e-02,  7.5191e-03,\n",
      "        -4.1068e-03, -9.8319e-03,  8.9196e-03,  1.0608e-02, -3.0381e-03,\n",
      "        -1.0395e-02,  1.2135e-02,  2.5261e-03, -7.0622e-03,  1.0196e-02,\n",
      "         1.3824e-02,  7.5847e-03, -1.0547e-02,  2.3488e-02,  4.0648e-03,\n",
      "         1.8967e-02, -4.2823e-03, -1.1458e-02, -2.5326e-02, -2.7343e-03,\n",
      "         1.3164e-02,  9.0439e-03, -1.6398e-02, -1.5172e-02,  2.5437e-03,\n",
      "        -3.0301e-03, -6.0817e-03, -2.9245e-02,  2.8400e-02,  6.9606e-03,\n",
      "        -1.3091e-02, -1.7827e-02, -1.4533e-02,  2.8686e-03,  3.0327e-02,\n",
      "        -4.0967e-03, -4.1520e-03, -1.7083e-03, -3.1481e-02,  3.0469e-02,\n",
      "        -7.1949e-03, -1.8066e-03,  1.4290e-02, -5.9843e-03,  5.2322e-03,\n",
      "         1.3945e-03,  3.1862e-03, -4.8505e-03,  8.6167e-04, -9.6072e-04,\n",
      "        -9.7103e-03, -7.7208e-03, -6.3208e-03, -1.3829e-02, -1.1587e-02,\n",
      "         1.6536e-03,  4.1766e-03], device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight tensor([[-0.0993,  0.0510,  0.1572,  ...,  0.2870,  0.1862,  0.3262],\n",
      "        [ 0.2749, -0.2380, -0.0307,  ..., -0.1219, -0.2021, -0.0015],\n",
      "        [-0.0692,  0.0052,  0.1759,  ...,  0.0037, -0.2858,  0.1298],\n",
      "        ...,\n",
      "        [-0.0738,  0.0549, -0.1185,  ..., -0.2054,  0.0930,  0.2648],\n",
      "        [ 0.2627,  0.2955, -0.1006,  ...,  0.0453, -0.2318, -0.0764],\n",
      "        [ 0.1647,  0.0897,  0.2396,  ..., -0.0105, -0.1322, -0.1361]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias tensor([-0.0085, -0.0069,  0.0005,  0.0095, -0.0005, -0.0034, -0.0130, -0.0017,\n",
      "         0.0306, -0.0035,  0.0007,  0.0395,  0.0017, -0.0357,  0.0015,  0.0004,\n",
      "         0.0154, -0.0103,  0.0016,  0.0061, -0.0234, -0.0061, -0.0028,  0.0013,\n",
      "         0.0268, -0.0055, -0.0002,  0.0192, -0.0075,  0.0059,  0.0161, -0.0118,\n",
      "         0.0027, -0.0120, -0.0037,  0.0150,  0.0082, -0.0202, -0.0008, -0.0114,\n",
      "         0.0154, -0.0065,  0.0109,  0.0017, -0.0044,  0.0190,  0.0263,  0.0136,\n",
      "        -0.0174, -0.0013,  0.0002,  0.0117,  0.0020,  0.0022,  0.0091, -0.0028,\n",
      "         0.0162, -0.0055,  0.0177,  0.0138, -0.0153,  0.0108,  0.0186, -0.0044],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.weight tensor([[-0.1045,  0.1381,  0.3689,  ...,  0.2046,  0.2584,  0.0749],\n",
      "        [-0.1438, -0.0743,  0.3001,  ...,  0.0333, -0.2617,  0.0343],\n",
      "        [-0.0996, -0.1020,  0.1206,  ...,  0.0994, -0.0008, -0.0449],\n",
      "        ...,\n",
      "        [ 0.2103,  0.0918,  0.2420,  ..., -0.2548, -0.1959,  0.1781],\n",
      "        [-0.0516, -0.2855,  0.2488,  ..., -0.2372,  0.3127,  0.0751],\n",
      "        [ 0.1590,  0.2369,  0.1238,  ..., -0.2887,  0.3115,  0.1009]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.bias tensor([-0.0568, -0.0454,  0.0072, -0.0859, -0.1270, -0.0835, -0.0768, -0.1502,\n",
      "        -0.0732,  0.0251,  0.0807, -0.0662, -0.0053, -0.1188,  0.0055,  0.1052,\n",
      "        -0.0778,  0.0314,  0.1081,  0.0268, -0.0645, -0.0402, -0.1055,  0.0453,\n",
      "         0.0024, -0.1099,  0.1139, -0.0127, -0.1238,  0.0176, -0.1185,  0.0665,\n",
      "         0.0576, -0.0211, -0.0791, -0.0102, -0.0838,  0.0020,  0.0101,  0.0719,\n",
      "        -0.0646, -0.0692,  0.0170, -0.1020, -0.1237,  0.0275, -0.0946, -0.0358,\n",
      "         0.0430,  0.0805,  0.0321, -0.0266, -0.0514, -0.0850, -0.1091, -0.0149,\n",
      "         0.0422, -0.0109, -0.0263,  0.0833, -0.0143, -0.0835, -0.1151, -0.0486,\n",
      "        -0.0994,  0.0359, -0.0683,  0.1043, -0.0808, -0.0191,  0.1200, -0.0572,\n",
      "        -0.0292,  0.0742, -0.0553,  0.1185, -0.0014,  0.0867,  0.0863, -0.0274,\n",
      "         0.0426,  0.0344, -0.0155,  0.0902,  0.0393, -0.0500, -0.0520, -0.1158,\n",
      "        -0.0372, -0.0477, -0.0499, -0.0291, -0.0686, -0.1355,  0.0650, -0.0566,\n",
      "         0.0237, -0.0799,  0.0190,  0.0238, -0.1288, -0.0790, -0.1605,  0.0263,\n",
      "         0.0444, -0.0792,  0.0239, -0.0761, -0.0954,  0.0361, -0.0719,  0.0437,\n",
      "        -0.1018, -0.1016,  0.1260,  0.0762,  0.0233,  0.0693, -0.0047,  0.0874,\n",
      "        -0.1413,  0.0930,  0.0537, -0.0234,  0.0558, -0.0562,  0.0358,  0.0759],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.weight tensor([[-0.0276,  0.1673, -0.0556,  ..., -0.0056, -0.1561, -0.1081],\n",
      "        [-0.0262, -0.0673, -0.1149,  ...,  0.1077, -0.1582, -0.0718],\n",
      "        [ 0.1268,  0.0029,  0.2075,  ..., -0.1670, -0.0314,  0.1017],\n",
      "        ...,\n",
      "        [-0.1926, -0.0909,  0.1284,  ...,  0.0625, -0.0490,  0.1226],\n",
      "        [ 0.1155,  0.2097,  0.0096,  ..., -0.1327, -0.0209, -0.1855],\n",
      "        [-0.1805,  0.0021, -0.1456,  ..., -0.1351, -0.0359, -0.1657]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.bias tensor([-0.0158, -0.0350, -0.0404, -0.0093,  0.0223,  0.0008, -0.0499,  0.0563,\n",
      "        -0.0253,  0.0742,  0.0395,  0.0807,  0.0631,  0.0354, -0.0512, -0.0544,\n",
      "        -0.0551,  0.0302, -0.0528, -0.0114,  0.0771, -0.0124, -0.0051, -0.0362,\n",
      "         0.0212, -0.0012,  0.0201, -0.0768, -0.0042,  0.0563, -0.0027, -0.0059,\n",
      "        -0.0682,  0.0083, -0.0766, -0.0626, -0.0063, -0.0056,  0.0355,  0.0799,\n",
      "        -0.0695,  0.0296, -0.0051, -0.0529,  0.0481,  0.0230, -0.0019,  0.0402,\n",
      "        -0.0030, -0.0754, -0.0695, -0.0329, -0.0865,  0.0478, -0.0207, -0.1016,\n",
      "         0.0045, -0.0511, -0.0296,  0.0604,  0.0022,  0.0501, -0.0451,  0.0482],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.weight tensor([1.0273, 0.9871, 0.9739, 1.0207, 1.0053, 1.0334, 1.0073, 1.0017, 1.0003,\n",
      "        1.0031, 1.0275, 1.0295, 1.0073, 0.9787, 0.9926, 0.9935, 0.9487, 1.0319,\n",
      "        0.9810, 1.0392, 0.9475, 1.0692, 0.9895, 1.0231, 1.0016, 0.9682, 1.0150,\n",
      "        0.9836, 0.9921, 0.9985, 1.0057, 1.0294, 0.9519, 0.9820, 0.9864, 1.0165,\n",
      "        0.9608, 1.0121, 0.9325, 1.0358, 1.0242, 1.0385, 0.9916, 1.0198, 0.9487,\n",
      "        0.9958, 1.0157, 0.9466, 0.9938, 1.0092, 1.0065, 0.9939, 0.9931, 1.0569,\n",
      "        0.9687, 1.0146, 1.0026, 0.9710, 1.0171, 0.9998, 1.0220, 1.0187, 1.0159,\n",
      "        1.0028], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.bias tensor([ 0.0362,  0.0165,  0.0140,  0.0160, -0.0552,  0.0539, -0.0356,  0.0022,\n",
      "        -0.0156, -0.0014, -0.0091, -0.0007, -0.0181,  0.0218, -0.0550,  0.0234,\n",
      "         0.0287,  0.0364, -0.0103, -0.0224, -0.0327, -0.0065,  0.0262,  0.0039,\n",
      "        -0.0233,  0.0486,  0.0006,  0.0156, -0.0220, -0.0586, -0.0326,  0.0612,\n",
      "         0.0038, -0.0258, -0.0191, -0.0136,  0.0257, -0.0200, -0.0044,  0.0368,\n",
      "        -0.0417,  0.0341,  0.0102, -0.0125,  0.0347,  0.0152, -0.0139,  0.0351,\n",
      "         0.0017,  0.0403,  0.0357, -0.0090,  0.0399,  0.0239,  0.0239, -0.0103,\n",
      "        -0.0332,  0.0370,  0.0022,  0.0149, -0.0258, -0.0423, -0.0425,  0.0060],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.weight tensor([1.0205, 0.9279, 1.0228, 0.9812, 0.9616, 0.9813, 1.0166, 0.9802, 0.9811,\n",
      "        1.0427, 1.0163, 0.9892, 0.9860, 0.9597, 0.9772, 1.0282, 0.9768, 0.9643,\n",
      "        0.9718, 0.9802, 0.9867, 0.9533, 0.9488, 0.9787, 0.9715, 0.9785, 1.0247,\n",
      "        0.9614, 0.9691, 1.0201, 0.9408, 0.9554, 0.9432, 0.9626, 0.9873, 0.9497,\n",
      "        0.9383, 0.9882, 1.0005, 0.9701, 0.9857, 0.9836, 0.9894, 0.9886, 0.9422,\n",
      "        0.9825, 0.9539, 1.0060, 0.9761, 0.9771, 0.9993, 0.9755, 0.9816, 1.0130,\n",
      "        0.9883, 0.9537, 0.9983, 0.9440, 0.9709, 0.9370, 0.9620, 0.9600, 0.9688,\n",
      "        0.9811], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.bias tensor([ 6.8556e-03, -3.4467e-03,  6.7542e-03,  4.0159e-03,  8.9714e-05,\n",
      "         1.0809e-03, -2.8506e-02,  1.0426e-02,  1.7746e-02, -5.0111e-03,\n",
      "         1.8626e-02,  3.0858e-02,  8.7067e-03, -2.4058e-02,  1.6699e-02,\n",
      "         8.9508e-03,  6.6683e-03, -2.2050e-02,  1.8057e-03,  7.1449e-03,\n",
      "        -3.3013e-02, -1.5675e-02, -3.6295e-02,  1.0558e-02,  3.5855e-02,\n",
      "        -2.1956e-02,  1.8433e-02,  1.8743e-02,  6.4633e-03,  1.4192e-02,\n",
      "         4.4362e-03, -8.7530e-03,  5.6485e-03, -2.5154e-02, -1.5132e-02,\n",
      "         9.8275e-03,  9.7043e-03, -2.3628e-02, -5.6726e-03, -1.1722e-02,\n",
      "        -1.6426e-03, -1.6090e-02,  1.0884e-03, -4.3445e-03, -1.8852e-02,\n",
      "         4.2173e-03,  3.9066e-02,  1.1663e-02, -2.2194e-02, -9.7917e-03,\n",
      "        -1.4484e-02,  1.4051e-02,  1.6354e-02,  1.2424e-03,  8.0984e-03,\n",
      "         1.8425e-02,  2.1460e-02,  1.7695e-03, -8.4957e-04,  1.5059e-02,\n",
      "        -1.1739e-02,  3.5471e-03,  1.3209e-02, -1.5135e-02], device='cuda:0')\n",
      "fc.0.weight tensor([[ 0.0846,  0.0309, -0.0625,  ..., -0.0008, -0.0553, -0.0589],\n",
      "        [-0.0600,  0.0655,  0.0561,  ..., -0.0508,  0.0358,  0.0402],\n",
      "        [-0.0221,  0.0593, -0.0484,  ..., -0.0275,  0.0215, -0.0165],\n",
      "        ...,\n",
      "        [-0.0675,  0.0946, -0.0453,  ...,  0.0337, -0.0435, -0.0562],\n",
      "        [ 0.0620,  0.0295, -0.0506,  ..., -0.0705,  0.0847, -0.1022],\n",
      "        [ 0.0574,  0.0761,  0.0153,  ...,  0.0800,  0.0006, -0.0144]],\n",
      "       device='cuda:0')\n",
      "fc.0.bias tensor([-2.0231e-02, -4.2599e-02, -3.5553e-02, -1.8199e-02,  4.3881e-03,\n",
      "        -1.0274e-02, -4.2364e-02, -1.7541e-02,  3.4556e-02, -3.6096e-02,\n",
      "        -5.0736e-03, -2.0129e-02, -2.1567e-02,  1.2739e-02, -3.0663e-02,\n",
      "         2.7379e-02,  7.0247e-03,  2.3546e-02, -3.9810e-02, -3.6286e-02,\n",
      "        -2.4280e-02, -1.4658e-02,  3.6634e-02,  1.5518e-02, -1.4005e-02,\n",
      "         6.4229e-03,  2.5108e-02, -2.4132e-02,  2.2802e-02, -1.8543e-02,\n",
      "        -8.2553e-03, -2.5317e-02, -5.1978e-03,  3.3165e-02,  1.4538e-02,\n",
      "        -3.0755e-03,  1.9961e-02,  2.9805e-03, -8.9141e-03,  2.3680e-02,\n",
      "         9.0210e-03, -3.7487e-02, -3.3234e-02,  6.7439e-03,  1.4034e-02,\n",
      "         3.2764e-02, -5.9643e-03,  2.7647e-03, -5.2590e-02, -3.8830e-02,\n",
      "        -8.2534e-03,  9.7385e-05,  4.2711e-02, -8.9833e-03,  1.8310e-02,\n",
      "        -3.7747e-02, -1.0995e-02,  1.4277e-02, -4.9453e-02, -2.8536e-03,\n",
      "        -3.2061e-03,  1.4934e-02,  4.1352e-02, -1.5953e-04], device='cuda:0')\n",
      "fc.3.weight tensor([[-0.1375,  0.0172,  0.2536,  0.1852,  0.0893,  0.2821,  0.1745, -0.2556,\n",
      "         -0.0403,  0.0567, -0.2560, -0.1384,  0.1014, -0.1683,  0.2419, -0.0821,\n",
      "         -0.0704, -0.2245,  0.0657,  0.1065, -0.1787, -0.3017,  0.1660,  0.1412,\n",
      "          0.1923,  0.2223,  0.1488,  0.0680, -0.2172,  0.1503, -0.2543,  0.0121,\n",
      "          0.1727, -0.1448, -0.2503,  0.0591,  0.2198, -0.2845,  0.1145, -0.0518,\n",
      "         -0.0329,  0.0920,  0.2365,  0.0589, -0.0678,  0.0480,  0.0393,  0.2162,\n",
      "          0.2254,  0.1240, -0.1331, -0.2701,  0.0568,  0.1097,  0.1884,  0.1006,\n",
      "          0.0260, -0.1411,  0.1377, -0.1398,  0.1460, -0.2812, -0.2028,  0.1564]],\n",
      "       device='cuda:0')\n",
      "fc.3.bias tensor([-0.0593], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Print learnable parameter values after training\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741fe2f694f14353b26948c31da61728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.504 MB of 0.680 MB uploaded\\r'), FloatProgress(value=0.7406896377655972, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>f1_score_test</td><td>▁█</td></tr><tr><td>f1_score_val</td><td>▁▇▇▇▇█████████████████████</td></tr><tr><td>learning_rate</td><td>██▇▇▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▄▂▂▃▂▃▂▁▂▂▁▂▁▁▁▂▂▂▁▂▂▁▂▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁███▇█████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>f1_score_test</td><td>0.50126</td></tr><tr><td>f1_score_val</td><td>0.51732</td></tr><tr><td>learning_rate</td><td>2e-05</td></tr><tr><td>test_accuracy</td><td>0.85315</td></tr><tr><td>train_loss</td><td>1.04421</td></tr><tr><td>val_accuracy</td><td>0.85505</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-resonance-40</strong> at: <a href='https://wandb.ai/onat-inak-/Transformer-Classifier/runs/syjiyz8d' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier/runs/syjiyz8d</a><br/> View project at: <a href='https://wandb.ai/onat-inak-/Transformer-Classifier' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240701_021048-syjiyz8d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
