{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.LSTM import LSTM\n",
    "from models.RNN import RNN\n",
    "from models.GRU import GRU\n",
    "from models.LSTM import LSTM\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import Dataset\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from tools.adjust_learning_rate import adjust_learning_rate\n",
    "from tools.train import train\n",
    "from tools.test import test\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "torch.manual_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load 3D numpy matrices (user, time, transaction type)\n",
    "ns = np.load('data/neg_sequences.npy')\n",
    "ps = np.load('data/pos_sequences.npy')\n",
    "\n",
    "transaction_size = ns.shape[-1]\n",
    "\n",
    "# Take a look at the given data with OHE (one-hot encodings)\n",
    "print('Shape of negative sequence array (OHE): ', ns.shape)\n",
    "print('Shape of positive sequence array (OHE): ', ps.shape)\n",
    "\n",
    "# Convert one-hot encodings to integers:\n",
    "ns = np.argmax(ns, axis=2)\n",
    "ps = np.argmax(ps, axis=2)\n",
    "\n",
    "# Take a look at the given data\n",
    "print('Shape of negative sequence array: ', ns.shape)\n",
    "print('Shape of positive sequence array: ', ps.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "ns_label = np.zeros_like(ns[:,0])\n",
    "ps_label = np.ones_like(ps[:,0])\n",
    "\n",
    "# Concetenate positive sequences with negative sequences regarding users with correponding labels (axis=0)\n",
    "X = np.concatenate((ns, ps), axis=0)\n",
    "y = np.concatenate((ns_label, ps_label), axis=0) \n",
    "\n",
    "# Shuffle data and labels, for reproductivity set random_state=0\n",
    "# dataset, labels = shuffle(dataset, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train, test and validation ratios\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Split the data / Shuffle it and maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=42, shuffle=True)\n",
    "\n",
    "# Further split train_data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=val_ratio, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train, test and validation dataset and label shapes\n",
    "print('X_train.shape: ', X_train.shape, ' - y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape, ' - y_test.shape: ', y_test.shape)\n",
    "print('X_val.shape: ', X_val.shape, ' - y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Convert numpy arrays to torch.tensor\n",
    "X_train, y_train = torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "X_train, y_train = X_train.to(device, dtype=torch.int32), y_train.to(device, dtype=torch.float32)\n",
    "X_test, y_test = torch.from_numpy(X_test), torch.from_numpy(y_test)\n",
    "X_test, y_test = X_test.to(device, dtype=torch.int32), y_test.to(device, dtype=torch.float32)\n",
    "X_val, y_val = torch.from_numpy(X_val), torch.from_numpy(y_val)\n",
    "X_val, y_val = X_val.to(device, dtype=torch.int32), y_val.to(device, dtype=torch.float32)\n",
    "\n",
    "# Number of positive sequences in training data\n",
    "num_pos_seq = torch.count_nonzero(y_train).view(1)\n",
    "num_neg_seq = (y_train.shape[0] - num_pos_seq).view(1)\n",
    "print('Training data -> num_pos_seq: ', num_pos_seq, ' num_neg_seq: ', num_neg_seq)\n",
    "\n",
    "# Create a custom dataset\n",
    "train_dataset = Dataset(X_train, y_train, device)\n",
    "test_dataset = Dataset(X_test, y_test, device)\n",
    "val_dataset = Dataset(X_val, y_val, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = 'RNN-Classifier'\n",
    "\n",
    "# # Initialize W&B for RNN-Classifier\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project = project_name,\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config = dict(\n",
    "#     project_name = project_name,\n",
    "#     batch_size = 900,\n",
    "#     transaction_size = transaction_size,\n",
    "#     embedding_dim = 32,\n",
    "#     hidden_dim = 64,\n",
    "#     num_layers = 1,\n",
    "#     device = device,\n",
    "#     batch_first = True,\n",
    "#     fc_hidden_dim = 64,\n",
    "#     num_classes = 1,\n",
    "#     num_epochs = 150,\n",
    "#     learning_rate = 1e-4,\n",
    "#     weight_decay = 0.0, \n",
    "#     lr_update_step = None,\n",
    "#     log_step = 20,\n",
    "#     lr_step_decay = False,\n",
    "#     gamma = 0.95,\n",
    "#     #pos_weight = num_neg_seq/num_pos_seq,\n",
    "#     pos_weight = 6.0,\n",
    "#     grad_clip = 1.0,\n",
    "#     apply_grad_clip = False,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # initialize config\n",
    "# config = wandb.config\n",
    "\n",
    "# model = RNN(config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "#             num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'GRU-Classifier'\n",
    "\n",
    "# Initialize W&B for GRU-Classifier\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project = project_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config = dict(\n",
    "    project_name = project_name,\n",
    "    batch_size = 900,\n",
    "    transaction_size = transaction_size,\n",
    "    embedding_dim = 32,\n",
    "    hidden_dim = 64,\n",
    "    num_layers = 1,\n",
    "    device = device,\n",
    "    batch_first = True,\n",
    "    fc_hidden_dim = 64,\n",
    "    num_classes = 1,\n",
    "    num_epochs = 25,\n",
    "    learning_rate = 1e-2,\n",
    "    weight_decay = 0.0, \n",
    "    lr_update_step = None,\n",
    "    log_step = 20,\n",
    "    lr_step_decay = False,\n",
    "    gamma = 0.90,\n",
    "    #pos_weight = num_neg_seq/num_pos_seq,\n",
    "    pos_weight = 6.0,\n",
    "    grad_clip = 1.0,\n",
    "    apply_grad_clip = False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# initialize config\n",
    "config = wandb.config\n",
    "\n",
    "model = GRU(config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "            num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = 'LSTM-Classifier'\n",
    "\n",
    "# # Initialize W&B for LSTM-Classifier\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project = project_name,\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config= dict(\n",
    "#     project_name = project_name,\n",
    "#     batch_size = 900,\n",
    "#     transaction_size = transaction_size,\n",
    "#     embedding_dim = 64,\n",
    "#     hidden_dim = 128,\n",
    "#     num_layers = 5,\n",
    "#     device = device,\n",
    "#     batch_first = True,\n",
    "#     fc_hidden_dim = 128,\n",
    "#     num_classes = 1,\n",
    "#     num_epochs = 25,\n",
    "#     learning_rate = 1e-3,\n",
    "#     weight_decay = 0.0, \n",
    "#     lr_update_step = None,\n",
    "#     log_step = 10,\n",
    "#     lr_step_decay = False,\n",
    "#     gamma = 0.90,\n",
    "#     pos_weight = num_neg_seq/num_pos_seq,\n",
    "#     # pos_weight = 6.0,\n",
    "#     grad_clip = 1.0,\n",
    "#     apply_grad_clip = False,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # initialize config\n",
    "# config = wandb.config\n",
    "\n",
    "# model = LSTM(config.batch_size, config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "#             num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "# model.to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZE DATA LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide train and test dataset into batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Check whether data is splitted correctly -> X_.shape: (batch, seq, encoding), y_.shape: (batch)\n",
    "# for i, (X_, y_) in enumerate(train_loader): \n",
    "#     print(X_.shape, y_.shape)\n",
    "#     print(X_[:10,:])\n",
    "#     print(y_[:10])\n",
    "\n",
    "# Print trainable model parameters with their corresponding initial values\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe initial performance of the model without any training\n",
    "model.eval()\n",
    "test(model, test_loader, device, config.project_name, save_model = False)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(float(config.pos_weight)).to(dtype=torch.long, device=config.device) * torch.ones([1]).to(config.device))\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight = 5 * torch.ones([1]).to(config.device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.999),\n",
    "                             eps=1e-8, weight_decay=config.weight_decay, amsgrad=False)  \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(model, train_loader, val_loader, criterion, optimizer, scheduler, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model -> no need to compute gradients (for memory efficiency)\n",
    "test(model, test_loader, device, config.project_name, save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
