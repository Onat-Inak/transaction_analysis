{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f874748bcb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.RNN import RNN\n",
    "from models.GRU import GRU\n",
    "from models.LSTM import LSTM\n",
    "from models.transformer import TransformerClassifier\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import Dataset\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from tools.adjust_learning_rate import adjust_learning_rate\n",
    "from tools.train import train\n",
    "from tools.test import test\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "torch.manual_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of negative sequence array (OHE):  (100000, 10, 312)\n",
      "Shape of positive sequence array (OHE):  (11979, 10, 312)\n",
      "Shape of negative sequence array:  (100000, 10)\n",
      "Shape of positive sequence array:  (11979, 10)\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load 3D numpy matrices (user, time, transaction type)\n",
    "ns = np.load('data/neg_sequences.npy')\n",
    "ps = np.load('data/pos_sequences.npy')\n",
    "\n",
    "transaction_size = ns.shape[-1]\n",
    "\n",
    "# Take a look at the given data with OHE (one-hot encodings)\n",
    "print('Shape of negative sequence array (OHE): ', ns.shape)\n",
    "print('Shape of positive sequence array (OHE): ', ps.shape)\n",
    "\n",
    "# Convert one-hot encodings to integers:\n",
    "ns = np.argmax(ns, axis=2)\n",
    "ps = np.argmax(ps, axis=2)\n",
    "\n",
    "# Take a look at the given data\n",
    "print('Shape of negative sequence array: ', ns.shape)\n",
    "print('Shape of positive sequence array: ', ps.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "ns_label = np.zeros_like(ns[:,0])\n",
    "ps_label = np.ones_like(ps[:,0])\n",
    "\n",
    "# Concetenate positive sequences with negative sequences regarding users with correponding labels (axis=0)\n",
    "X = np.concatenate((ns, ps), axis=0)\n",
    "y = np.concatenate((ns_label, ps_label), axis=0) \n",
    "\n",
    "# Shuffle data and labels, for reproductivity set random_state=0\n",
    "# dataset, labels = shuffle(dataset, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train, test and validation ratios\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Split the data / Shuffle it and maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=42, shuffle=True)\n",
    "\n",
    "# Further split train_data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=val_ratio, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (90702, 10)  - y_train.shape:  (90702,)\n",
      "X_test.shape:  (11198, 10)  - y_test.shape:  (11198,)\n",
      "X_val.shape:  (10079, 10)  - y_val.shape:  (10079,)\n",
      "Training data -> num_pos_seq:  tensor([9724], device='cuda:0')  num_neg_seq:  tensor([80978], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Print train, test and validation dataset and label shapes\n",
    "print('X_train.shape: ', X_train.shape, ' - y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape, ' - y_test.shape: ', y_test.shape)\n",
    "print('X_val.shape: ', X_val.shape, ' - y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Convert numpy arrays to torch.tensor\n",
    "X_train, y_train = torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "X_train, y_train = X_train.to(device, dtype=torch.int32), y_train.to(device, dtype=torch.float32)\n",
    "X_test, y_test = torch.from_numpy(X_test), torch.from_numpy(y_test)\n",
    "X_test, y_test = X_test.to(device, dtype=torch.int32), y_test.to(device, dtype=torch.float32)\n",
    "X_val, y_val = torch.from_numpy(X_val), torch.from_numpy(y_val)\n",
    "X_val, y_val = X_val.to(device, dtype=torch.int32), y_val.to(device, dtype=torch.float32)\n",
    "\n",
    "# Number of positive sequences in training data\n",
    "num_pos_seq = torch.count_nonzero(y_train).view(1)\n",
    "num_neg_seq = (y_train.shape[0] - num_pos_seq).view(1)\n",
    "print('Training data -> num_pos_seq: ', num_pos_seq, ' num_neg_seq: ', num_neg_seq)\n",
    "\n",
    "# Create a custom dataset\n",
    "train_dataset = Dataset(X_train, y_train, device)\n",
    "test_dataset = Dataset(X_test, y_test, device)\n",
    "val_dataset = Dataset(X_val, y_val, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = 'RNN-Classifier'\n",
    "\n",
    "# # Initialize W&B for RNN-Classifier\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project = project_name,\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config = dict(\n",
    "#     project_name = project_name,\n",
    "#     batch_size = 900,\n",
    "#     transaction_size = transaction_size,\n",
    "#     embedding_dim = 32,\n",
    "#     hidden_dim = 64,\n",
    "#     num_layers = 1,\n",
    "#     device = device,\n",
    "#     batch_first = True,\n",
    "#     fc_hidden_dim = 64,\n",
    "#     num_classes = 1,\n",
    "#     num_epochs = 150,\n",
    "#     learning_rate = 1e-4,\n",
    "#     weight_decay = 0.0, \n",
    "#     lr_update_step = None,\n",
    "#     log_step = 20,\n",
    "#     lr_step_decay = False,\n",
    "#     gamma = 0.95,\n",
    "#     #pos_weight = num_neg_seq/num_pos_seq,\n",
    "#     pos_weight = 6.0,\n",
    "#     grad_clip = 1.0,\n",
    "#     apply_grad_clip = False,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # initialize config\n",
    "# config = wandb.config\n",
    "\n",
    "# model = RNN(config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "#             num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = 'GRU-Classifier'\n",
    "\n",
    "# # Initialize W&B for GRU-Classifier\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project = project_name,\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config = dict(\n",
    "#     project_name = project_name,\n",
    "#     batch_size = 900,\n",
    "#     transaction_size = transaction_size,\n",
    "#     embedding_dim = 32,\n",
    "#     hidden_dim = 64,\n",
    "#     num_layers = 1,\n",
    "#     device = device,\n",
    "#     batch_first = True,\n",
    "#     fc_hidden_dim = 64,\n",
    "#     num_classes = 1,\n",
    "#     num_epochs = 25,\n",
    "#     learning_rate = 1e-2,\n",
    "#     weight_decay = 0.0, \n",
    "#     lr_update_step = None,\n",
    "#     log_step = 20,\n",
    "#     lr_step_decay = False,\n",
    "#     gamma = 0.8,\n",
    "#     pos_weight = num_neg_seq/num_pos_seq,\n",
    "#     # pos_weight = 6.0,\n",
    "#     grad_clip = 1.0,\n",
    "#     apply_grad_clip = False,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # initialize config\n",
    "# config = wandb.config\n",
    "\n",
    "# model = GRU(config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "#             num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "# model.to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# project_name = 'LSTM-Classifier'\n",
    "\n",
    "# # Initialize W&B for LSTM-Classifier\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project = project_name,\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config= dict(\n",
    "#     project_name = project_name,\n",
    "#     batch_size = 900,\n",
    "#     transaction_size = transaction_size,\n",
    "#     embedding_dim = 64,\n",
    "#     hidden_dim = 128,\n",
    "#     num_layers = 5,\n",
    "#     device = device,\n",
    "#     batch_first = True,\n",
    "#     fc_hidden_dim = 128,\n",
    "#     num_classes = 1,\n",
    "#     num_epochs = 25,\n",
    "#     learning_rate = 1e-3,\n",
    "#     weight_decay = 0.0, \n",
    "#     lr_update_step = None,\n",
    "#     log_step = 10,\n",
    "#     lr_step_decay = False,\n",
    "#     gamma = 0.90,\n",
    "#     pos_weight = num_neg_seq/num_pos_seq,\n",
    "#     # pos_weight = 6.0,\n",
    "#     grad_clip = 1.0,\n",
    "#     apply_grad_clip = False,\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # initialize config\n",
    "# config = wandb.config\n",
    "\n",
    "# model = LSTM(config.batch_size, config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "#             num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "# model.to(device)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRANSFORMER-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33monat-inak\u001b[0m (\u001b[33monat-inak-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/onatinak/workspace/transaction_analysis/wandb/run-20240701_033708-arira6ci</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/onat-inak-/Transformer-Classifier/runs/arira6ci' target=\"_blank\">colorful-feather-65</a></strong> to <a href='https://wandb.ai/onat-inak-/Transformer-Classifier' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/onat-inak-/Transformer-Classifier' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/onat-inak-/Transformer-Classifier/runs/arira6ci' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier/runs/arira6ci</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerClassifier(\n",
      "  (input_embedding): Embedding(312, 64)\n",
      "  (position_embedding): Embedding(10, 64)\n",
      "  (transformer_encoder_layer): TransformerEncoderLayer(\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (dropout1): Dropout(p=0.5, inplace=False)\n",
      "    (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (dropout): Dropout(p=0.5, inplace=False)\n",
      "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.5, inplace=False)\n",
      "        (dropout2): Dropout(p=0.5, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv1d): Conv1d(640, 64, kernel_size=(1,), stride=(1,))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (9): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onatinak/anaconda3/envs/case_study/lib/python3.8/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "project_name = 'Transformer-Classifier'\n",
    "\n",
    "# Initialize W&B for LSTM-Classifier\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project = project_name,\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config= dict(\n",
    "    project_name = project_name,\n",
    "    batch_size = 900,\n",
    "    transaction_size = transaction_size,\n",
    "    seq_len = 10,\n",
    "    embedding_dim = 64,\n",
    "    nhead = 4,\n",
    "    dim_feedforward_encoder = 64,\n",
    "    fc_hidden_dim = 64,\n",
    "    dropout = 0.5,\n",
    "    activation = \"relu\",\n",
    "    batch_first = True,\n",
    "    norm_first = True,\n",
    "    bias = True,\n",
    "    num_layers = 1,\n",
    "    is_causal = False,\n",
    "    learn_pos_embed = True,\n",
    "    device = device,\n",
    "    num_classes = 1,\n",
    "    num_epochs = 50,\n",
    "    learning_rate = 5e-4,\n",
    "    weight_decay = 0.0, \n",
    "    lr_update_step = None,\n",
    "    log_step = 10,\n",
    "    lr_step_decay = False,\n",
    "    gamma = 0.85,\n",
    "    pos_weight = num_neg_seq/num_pos_seq,\n",
    "    # pos_weight = 9.0,\n",
    "    grad_clip = 1.0,\n",
    "    apply_grad_clip = False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# initialize config\n",
    "config = wandb.config\n",
    "\n",
    "model = TransformerClassifier(batch_size = config.batch_size,\n",
    "                              transaction_size = config.transaction_size,\n",
    "                              seq_len = config.seq_len,\n",
    "                              embedding_dim = config.embedding_dim,\n",
    "                              nhead = config.nhead,\n",
    "                              dim_feedforward_encoder = config.dim_feedforward_encoder,\n",
    "                              fc_hidden_dim = config.fc_hidden_dim,\n",
    "                              dropout = config.dropout,\n",
    "                              device = config.device,\n",
    "                              activation = config.activation,\n",
    "                              batch_first = config.batch_first,\n",
    "                              norm_first = config.norm_first,\n",
    "                              bias = config.bias,\n",
    "                              num_layers = config.num_layers,\n",
    "                              is_causal = config.is_causal,\n",
    "                              learn_pos_embed = config.learn_pos_embed)\n",
    "model.to(device)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if 'weight' in name and param.data.dim() == 2:\n",
    "#         nn.init.kaiming_uniform_(param)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIALIZE DATA LOADERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embedding.weight tensor([[ 0.1940,  2.1614, -0.1721,  ..., -0.8146,  0.2502, -0.4273],\n",
      "        [ 1.1044, -1.1028,  0.5543,  ..., -1.2072, -0.2438, -0.6784],\n",
      "        [ 0.1973,  0.9782, -0.0287,  ..., -0.1266, -0.4321, -1.3943],\n",
      "        ...,\n",
      "        [ 1.9494, -0.7560, -1.7060,  ...,  2.3970,  0.1141, -0.3979],\n",
      "        [ 0.0863, -1.4670, -1.0068,  ...,  1.1312, -1.8843,  0.9926],\n",
      "        [-1.0923, -2.0909, -0.2095,  ..., -0.4394, -1.0592, -0.5276]],\n",
      "       device='cuda:0')\n",
      "position_embedding.weight tensor([[ 1.9269e+00,  1.4873e+00,  9.0072e-01, -2.1055e+00,  6.7842e-01,\n",
      "         -1.2345e+00, -4.3067e-02, -1.6047e+00, -7.5214e-01,  1.6487e+00,\n",
      "         -3.9248e-01, -1.4036e+00, -7.2788e-01, -5.5943e-01, -7.6884e-01,\n",
      "          7.6245e-01,  1.6423e+00, -1.5960e-01, -4.9740e-01,  4.3959e-01,\n",
      "         -7.5813e-01,  1.0783e+00,  8.0080e-01,  1.6806e+00,  1.2791e+00,\n",
      "          1.2964e+00,  6.1047e-01,  1.3347e+00, -2.3162e-01,  4.1759e-02,\n",
      "         -2.5158e-01,  8.5986e-01, -1.3847e+00, -8.7124e-01, -2.2337e-01,\n",
      "          1.7174e+00,  3.1888e-01, -4.2452e-01,  3.0572e-01, -7.7459e-01,\n",
      "         -1.5576e+00,  9.9564e-01, -8.7979e-01, -6.0114e-01, -1.2742e+00,\n",
      "          2.1228e+00, -1.2347e+00, -4.8791e-01, -9.1382e-01, -6.5814e-01,\n",
      "          7.8024e-02,  5.2581e-01, -4.8799e-01,  1.1914e+00, -8.1401e-01,\n",
      "         -7.3599e-01, -1.4032e+00,  3.6004e-02, -6.3477e-02,  6.7561e-01,\n",
      "         -9.7807e-02,  1.8446e+00, -1.1845e+00,  1.3835e+00],\n",
      "        [ 1.4451e+00,  8.5641e-01,  2.2181e+00,  5.2317e-01,  3.4665e-01,\n",
      "         -1.9733e-01, -1.0546e+00,  1.2780e+00, -1.7219e-01,  5.2379e-01,\n",
      "          5.6622e-02,  4.2630e-01,  5.7501e-01, -6.4172e-01, -2.2064e+00,\n",
      "         -7.5080e-01,  1.0868e-02, -3.3874e-01, -1.3407e+00, -5.8537e-01,\n",
      "          5.3619e-01,  5.2462e-01,  1.1412e+00,  5.1644e-02,  7.4395e-01,\n",
      "         -4.8158e-01, -1.0495e+00,  6.0390e-01, -1.7223e+00, -8.2777e-01,\n",
      "          1.3347e+00,  4.8354e-01, -2.5095e+00,  4.8800e-01,  7.8459e-01,\n",
      "          2.8647e-02,  6.4076e-01,  5.8325e-01,  1.0669e+00, -4.5015e-01,\n",
      "         -1.8527e-01,  7.5276e-01,  4.0476e-01,  1.7847e-01,  2.6491e-01,\n",
      "          1.2732e+00, -1.3109e-03, -3.0360e-01, -1.4570e+00, -1.0234e-01,\n",
      "         -5.9915e-01,  4.7706e-01,  7.2618e-01,  9.1152e-02, -3.8907e-01,\n",
      "          5.2792e-01, -1.2685e-02,  2.4084e-01,  1.3254e-01,  7.6424e-01,\n",
      "          1.0950e+00,  3.3989e-01,  7.1997e-01,  4.1141e-01],\n",
      "        [ 1.9312e+00,  1.0119e+00, -1.4364e+00, -1.1299e+00, -1.3603e-01,\n",
      "          1.6354e+00,  6.5474e-01,  5.7600e-01,  1.1415e+00,  1.8565e-02,\n",
      "         -1.8058e+00,  9.2543e-01, -3.7534e-01,  1.0331e+00, -6.8665e-01,\n",
      "          6.3681e-01, -9.7267e-01,  9.5846e-01,  1.6192e+00,  1.4506e+00,\n",
      "          2.6948e-01, -2.1038e-01, -7.3280e-01,  1.0430e-01,  3.4875e-01,\n",
      "          9.6759e-01, -4.6569e-01,  1.6048e+00, -2.4801e+00, -4.1754e-01,\n",
      "         -1.1955e+00,  8.1234e-01, -1.9006e+00,  2.2858e-01,  2.4859e-02,\n",
      "         -3.4595e-01,  2.8683e-01, -7.3084e-01,  1.7482e-01, -1.0939e+00,\n",
      "         -1.6022e+00,  1.3529e+00,  1.2888e+00,  5.2295e-02, -1.5469e+00,\n",
      "          7.5671e-01,  7.7552e-01,  2.0265e+00,  3.5818e-02,  1.2059e-01,\n",
      "         -8.0566e-01, -2.0758e-01, -9.3195e-01, -1.5910e+00, -1.1360e+00,\n",
      "         -5.2260e-01, -5.1877e-01, -1.5013e+00, -1.9267e+00,  1.2785e-01,\n",
      "          1.0229e+00, -5.5580e-01,  7.0427e-01,  7.0988e-01],\n",
      "        [ 1.7744e+00, -9.2155e-01,  9.6245e-01, -3.3702e-01, -1.1753e+00,\n",
      "          3.5806e-01,  4.7877e-01,  1.3537e+00,  5.2606e-01,  2.1120e+00,\n",
      "         -5.2076e-01, -9.3201e-01,  1.8516e-01,  1.0687e+00,  1.3065e+00,\n",
      "          4.5983e-01, -8.1463e-01, -1.0212e+00, -4.9492e-01, -5.9225e-01,\n",
      "          1.5432e-01,  4.4077e-01, -1.4829e-01, -2.3184e+00, -3.9800e-01,\n",
      "          1.0805e+00, -1.7809e+00,  1.5080e+00,  3.0943e-01, -5.0031e-01,\n",
      "          1.0350e+00,  1.6896e+00, -4.5051e-03,  1.6668e+00,  1.5392e-01,\n",
      "         -1.0603e+00, -5.7266e-01,  8.3568e-02,  3.9991e-01,  1.9892e+00,\n",
      "         -7.1988e-02, -9.0609e-01, -2.0487e+00, -1.0811e+00,  1.7623e-02,\n",
      "          7.8226e-02,  1.9316e-01,  4.0967e-01, -9.2913e-01,  2.7619e-01,\n",
      "         -5.3888e-01,  4.6258e-01, -8.7189e-01, -2.7118e-02, -3.5325e-01,\n",
      "          1.4639e+00,  1.2554e+00, -7.1496e-01,  8.5392e-01,  5.1299e-01,\n",
      "          5.3973e-01,  5.6551e-01,  5.0579e-01,  2.2245e-01],\n",
      "        [-6.8548e-01,  5.6356e-01, -1.5072e+00, -1.6107e+00, -1.4790e+00,\n",
      "          4.3227e-01, -1.2503e-01,  7.8212e-01, -1.5988e+00, -1.0913e-01,\n",
      "          7.1520e-01,  3.9139e-02,  1.3059e+00,  2.4659e-01, -1.9776e+00,\n",
      "          1.7896e-02, -1.3793e+00,  6.2580e-01, -2.5850e+00, -2.4000e-02,\n",
      "         -1.2219e-01, -7.4700e-01,  1.7093e+00,  5.7923e-02,  1.1930e+00,\n",
      "          1.9373e+00,  7.2871e-01,  9.8089e-01,  4.1459e-01,  1.1566e+00,\n",
      "          2.6905e-01, -3.6629e-02,  9.7329e-01, -1.0151e+00, -5.4192e-01,\n",
      "         -4.4102e-01, -3.1362e-01, -1.2925e-01, -7.1496e-01, -4.7562e-02,\n",
      "          2.0207e+00,  2.5392e-01,  9.3644e-01,  7.1224e-01, -3.1766e-02,\n",
      "          1.0164e-01,  1.3433e+00,  7.1327e-01,  4.0380e-01, -7.1398e-01,\n",
      "          8.3373e-01, -9.5855e-01,  4.5363e-01,  1.2461e+00, -2.3065e+00,\n",
      "         -1.2869e+00,  1.7989e-01, -2.1268e+00, -1.3408e-01, -1.0408e+00,\n",
      "         -7.6472e-01, -5.5283e-02,  1.2049e+00, -9.8247e-01],\n",
      "        [ 4.3344e-01, -7.1719e-01,  1.0554e+00, -1.4534e+00,  4.6515e-01,\n",
      "          3.7139e-01, -4.6568e-03,  7.9549e-02,  3.7818e-01,  7.0511e-01,\n",
      "         -1.7237e+00, -8.4348e-01,  4.3514e-01,  2.6589e-01, -5.8710e-01,\n",
      "          8.2689e-02,  8.8538e-01,  1.8244e-01,  7.8638e-01, -5.7920e-02,\n",
      "          5.6667e-01, -7.0976e-01, -4.8751e-01,  5.0096e-02,  6.0841e-01,\n",
      "          1.6309e+00, -8.4723e-02,  1.0844e+00,  9.4777e-01, -6.7663e-01,\n",
      "         -5.7302e-01, -3.3032e-01, -7.9394e-01,  3.7523e-01,  8.7910e-02,\n",
      "         -1.2415e+00, -3.2025e-01, -8.4438e-01, -5.5135e-01,  1.9890e+00,\n",
      "          1.9003e+00,  1.6951e+00,  2.8090e-02, -1.7537e-01, -1.7735e+00,\n",
      "         -7.0464e-01, -3.9465e-01,  1.8868e+00, -2.1844e-01,  1.6630e-01,\n",
      "          2.1442e+00,  1.7046e+00,  3.4590e-01,  6.4248e-01, -2.0395e-01,\n",
      "          6.8537e-01, -1.3969e-01, -1.1808e+00, -1.2829e+00,  4.4849e-01,\n",
      "         -5.9074e-01,  8.5406e-01, -4.9007e-01, -3.5946e-01],\n",
      "        [ 6.6637e-01, -7.4265e-02, -2.0960e-01,  1.6632e-01,  1.4703e+00,\n",
      "         -9.3909e-01, -6.0132e-01, -9.9640e-02, -9.8515e-01, -2.4885e+00,\n",
      "         -3.3132e-01,  8.4358e-01,  9.8745e-01, -3.3197e-01, -8.0762e-01,\n",
      "          8.2436e-01,  2.4700e-02, -1.0641e+00, -7.6019e-01, -4.0751e-01,\n",
      "          9.6236e-01, -1.4264e-01,  1.5271e-01, -3.8802e-02,  9.4461e-01,\n",
      "         -1.5824e+00,  9.8713e-01,  1.1457e+00, -1.4181e-01, -2.7634e-01,\n",
      "         -1.9321e-01,  7.7678e-01,  6.8388e-01, -1.3246e+00, -5.1608e-01,\n",
      "          6.0018e-01, -4.7022e-01, -6.0864e-01, -4.6192e-02, -1.6457e+00,\n",
      "         -4.8333e-01, -7.4029e-01,  3.1428e-01,  1.4156e-01,  1.0348e+00,\n",
      "         -6.2644e-01, -5.1509e-01,  6.9029e-01, -4.9400e-01,  1.1366e+00,\n",
      "         -4.6184e-01,  1.4200e+00,  8.4852e-01, -4.7891e-02,  6.6856e-01,\n",
      "          1.0430e+00,  6.8990e-01, -1.3129e+00,  3.7804e-02, -1.1702e+00,\n",
      "         -1.0319e-01,  1.1895e+00,  7.6069e-01, -7.4630e-01],\n",
      "        [-1.3839e+00,  4.8687e-01, -1.0020e+00,  3.2949e-02, -4.2920e-01,\n",
      "         -9.8180e-01, -6.4206e-01,  8.2659e-01,  1.5914e+00, -1.2081e-01,\n",
      "         -4.8302e-01,  1.1330e-01,  7.7151e-02, -9.2281e-01, -1.2620e+00,\n",
      "          1.0861e+00,  1.0966e+00, -6.8369e-01,  6.6043e-02, -7.7380e-04,\n",
      "          1.6206e-01,  1.1960e+00, -1.3062e+00, -1.4040e+00, -1.0597e+00,\n",
      "          3.0573e-01,  4.1506e-01, -7.1741e-01,  2.8340e+00,  1.9535e+00,\n",
      "          2.0487e+00, -1.0880e+00,  1.6217e+00,  8.5127e-01, -4.0047e-01,\n",
      "         -6.0883e-01, -5.0810e-01, -6.1849e-01, -1.6470e+00, -1.0362e+00,\n",
      "         -4.5031e-01, -7.2966e-02, -5.4795e-01, -1.1426e+00, -4.4875e-01,\n",
      "         -3.0454e-02,  3.8303e-01, -4.4770e-02,  1.1799e+00, -3.3143e-01,\n",
      "          6.4950e-01,  9.4959e-02, -7.5259e-01, -6.4723e-01, -1.2823e+00,\n",
      "          1.9653e+00, -9.6385e-01, -2.5668e+00,  7.0961e-01,  8.1984e-01,\n",
      "          6.2145e-01,  4.2319e-01, -3.3890e-01,  5.1797e-01],\n",
      "        [-1.3638e+00,  1.9296e-01, -6.1033e-01,  1.6323e-01,  1.5102e+00,\n",
      "          2.1230e-01, -7.2520e-01, -9.5277e-01,  5.2169e-01, -4.6387e-01,\n",
      "          1.8238e-01, -3.8666e-01, -1.7907e+00,  9.3293e-02, -1.9153e+00,\n",
      "         -6.4218e-01,  1.3439e+00, -1.2922e+00,  7.6624e-01,  6.4540e-01,\n",
      "          3.5332e-01, -2.6475e+00, -1.4575e+00, -9.7124e-01,  2.5403e-01,\n",
      "         -1.7906e-01,  1.1993e+00, -4.2922e-01,  1.0103e+00,  6.1104e-01,\n",
      "          1.2208e+00, -6.0764e-01, -1.7376e+00, -1.2535e-01, -1.3658e+00,\n",
      "          1.1117e+00, -6.2280e-01, -7.8918e-01, -1.6782e-01,  1.6433e+00,\n",
      "          2.0071e+00, -1.2531e+00,  1.1189e+00,  1.7733e+00, -2.0717e+00,\n",
      "         -4.1253e-01, -9.7696e-01, -3.3634e-02,  1.8595e+00,  2.6221e+00,\n",
      "          3.6905e-01,  3.8030e-01,  1.9898e-01, -2.3609e-01,  3.0341e-01,\n",
      "         -4.5008e-01,  4.7390e-01,  6.5034e-01,  1.1662e+00,  1.6936e-02,\n",
      "          5.3259e-01, -6.0354e-01, -1.7426e-01,  6.0921e-01],\n",
      "        [-8.0322e-01, -1.1209e+00,  1.9564e-01, -7.8152e-01, -1.7899e+00,\n",
      "         -2.6157e-01, -4.4025e-01,  2.1848e+00, -4.8010e-01, -1.2872e+00,\n",
      "          7.3888e-01,  3.3895e-02, -3.1229e-01, -2.5418e-01, -1.2055e+00,\n",
      "         -9.5421e-01,  6.1277e-02,  8.5261e-02,  7.4813e-01, -1.6356e-01,\n",
      "         -9.0856e-01,  3.1300e-01,  8.0505e-01, -1.1134e+00,  4.9816e-01,\n",
      "         -1.2000e+00,  1.2711e-01,  4.4037e-01,  6.3777e-01,  1.5979e-01,\n",
      "          1.7698e+00,  6.2682e-01, -1.8737e+00,  2.3259e+00, -9.2039e-01,\n",
      "          6.6611e-01, -4.4026e-01, -2.3180e+00,  1.2946e+00,  2.2267e-01,\n",
      "         -8.4834e-01,  1.6489e+00,  1.6006e+00, -7.8589e-02,  4.3105e-01,\n",
      "          3.6835e-01,  7.6380e-01,  1.1792e+00, -4.1379e-01,  5.1841e-01,\n",
      "         -7.0154e-01, -4.3234e-01,  1.4148e-01,  7.1104e-02,  5.6335e-01,\n",
      "         -5.7864e-01, -1.0838e+00, -3.8893e-01,  8.1261e-01,  1.4981e+00,\n",
      "          4.3896e-02,  1.4443e+00,  2.3203e-01,  5.0650e-01]], device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.in_proj_weight tensor([[ 0.0703, -0.1165, -0.1375,  ...,  0.0212, -0.1052,  0.0192],\n",
      "        [ 0.0411, -0.0424,  0.0953,  ..., -0.0461, -0.0236, -0.1036],\n",
      "        [ 0.0287,  0.1327,  0.0658,  ...,  0.1354, -0.0870, -0.0376],\n",
      "        ...,\n",
      "        [ 0.0707,  0.0237,  0.1266,  ...,  0.0025,  0.1408, -0.1374],\n",
      "        [ 0.0490, -0.0774, -0.1178,  ...,  0.0374, -0.1118,  0.1434],\n",
      "        [ 0.1466, -0.1332, -0.1414,  ...,  0.0549, -0.0502, -0.0088]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.in_proj_bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.out_proj.weight tensor([[ 0.1219, -0.0928,  0.0155,  ...,  0.0371,  0.1024, -0.0195],\n",
      "        [-0.0048,  0.1221,  0.0839,  ..., -0.0830, -0.0791,  0.0733],\n",
      "        [ 0.0856, -0.1222,  0.0995,  ..., -0.0639,  0.0036, -0.0691],\n",
      "        ...,\n",
      "        [ 0.0203,  0.1174,  0.1159,  ...,  0.0586,  0.0476, -0.1011],\n",
      "        [ 0.0005,  0.0868,  0.0465,  ...,  0.0547, -0.1133, -0.0812],\n",
      "        [-0.1178, -0.1099, -0.0940,  ..., -0.0874, -0.0822, -0.0179]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.out_proj.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear1.weight tensor([[-0.0434,  0.1242, -0.1073,  ..., -0.0213, -0.0520, -0.0027],\n",
      "        [-0.0805, -0.0889, -0.0868,  ...,  0.0488,  0.1191, -0.0554],\n",
      "        [ 0.1245, -0.1052, -0.0716,  ...,  0.0981,  0.0616, -0.0802],\n",
      "        ...,\n",
      "        [ 0.0621,  0.1016,  0.1089,  ...,  0.0488, -0.0239, -0.0645],\n",
      "        [-0.0704, -0.0956,  0.0587,  ...,  0.0442, -0.1043,  0.0921],\n",
      "        [-0.0672,  0.0947,  0.0921,  ..., -0.1023,  0.0511,  0.0835]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear1.bias tensor([-0.0463, -0.0482,  0.0280, -0.0686, -0.0988, -0.0673, -0.0733, -0.1123,\n",
      "        -0.0716,  0.0571,  0.1165, -0.0762, -0.0144, -0.1066, -0.0080,  0.1027,\n",
      "        -0.0611,  0.0536,  0.1062,  0.0387, -0.0515, -0.0200, -0.0940,  0.0399,\n",
      "         0.0301, -0.0883,  0.1065,  0.0029, -0.1145,  0.0316, -0.0973,  0.0511,\n",
      "         0.0636, -0.0207, -0.0341, -0.0081, -0.0718,  0.0273,  0.0333,  0.0698,\n",
      "        -0.0483, -0.0552,  0.0185, -0.1165, -0.1079,  0.0433, -0.0879, -0.0317,\n",
      "         0.0933,  0.0965,  0.0585, -0.0246, -0.0218, -0.0746, -0.1119,  0.0030,\n",
      "         0.0556, -0.0229, -0.0357,  0.0844, -0.0087, -0.0609, -0.0826, -0.0518],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear2.weight tensor([[-0.0686,  0.0209, -0.0159,  ...,  0.1047,  0.0429, -0.1198],\n",
      "        [ 0.0638, -0.0108, -0.0980,  ..., -0.0836,  0.0308, -0.0767],\n",
      "        [-0.1114,  0.0920,  0.0930,  ...,  0.0805, -0.1108,  0.0153],\n",
      "        ...,\n",
      "        [-0.1014,  0.1001,  0.0917,  ...,  0.0105, -0.0279,  0.1165],\n",
      "        [-0.1101, -0.0055, -0.0524,  ..., -0.1234, -0.0976, -0.0842],\n",
      "        [ 0.0119,  0.0240,  0.0157,  ...,  0.0464,  0.0430,  0.0867]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear2.bias tensor([-0.0354, -0.0503, -0.0427, -0.0258,  0.0338, -0.0213, -0.0616,  0.0753,\n",
      "        -0.0554,  0.1166,  0.0627,  0.1206,  0.0918,  0.0535, -0.0687, -0.0986,\n",
      "        -0.0976,  0.0492, -0.0991, -0.0068,  0.1092, -0.0254, -0.0342, -0.0542,\n",
      "         0.0081, -0.0508,  0.0273, -0.1021, -0.0283,  0.0683, -0.0380,  0.0252,\n",
      "        -0.0947, -0.0005, -0.1238, -0.1210, -0.0306, -0.0176,  0.0583,  0.1139,\n",
      "        -0.1039,  0.0451, -0.0414, -0.0518,  0.0741,  0.0518,  0.0111,  0.0484,\n",
      "        -0.0086, -0.1226, -0.1163, -0.0482, -0.1223,  0.0913, -0.0175, -0.1109,\n",
      "        -0.0113, -0.0682, -0.0551,  0.0859,  0.0088,  0.0958, -0.1084,  0.0657],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.norm1.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder_layer.norm1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.norm2.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder_layer.norm2.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_weight tensor([[ 0.0703, -0.1165, -0.1375,  ...,  0.0212, -0.1052,  0.0192],\n",
      "        [ 0.0411, -0.0424,  0.0953,  ..., -0.0461, -0.0236, -0.1036],\n",
      "        [ 0.0287,  0.1327,  0.0658,  ...,  0.1354, -0.0870, -0.0376],\n",
      "        ...,\n",
      "        [ 0.0707,  0.0237,  0.1266,  ...,  0.0025,  0.1408, -0.1374],\n",
      "        [ 0.0490, -0.0774, -0.1178,  ...,  0.0374, -0.1118,  0.1434],\n",
      "        [ 0.1466, -0.1332, -0.1414,  ...,  0.0549, -0.0502, -0.0088]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight tensor([[ 0.1219, -0.0928,  0.0155,  ...,  0.0371,  0.1024, -0.0195],\n",
      "        [-0.0048,  0.1221,  0.0839,  ..., -0.0830, -0.0791,  0.0733],\n",
      "        [ 0.0856, -0.1222,  0.0995,  ..., -0.0639,  0.0036, -0.0691],\n",
      "        ...,\n",
      "        [ 0.0203,  0.1174,  0.1159,  ...,  0.0586,  0.0476, -0.1011],\n",
      "        [ 0.0005,  0.0868,  0.0465,  ...,  0.0547, -0.1133, -0.0812],\n",
      "        [-0.1178, -0.1099, -0.0940,  ..., -0.0874, -0.0822, -0.0179]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.weight tensor([[-0.0434,  0.1242, -0.1073,  ..., -0.0213, -0.0520, -0.0027],\n",
      "        [-0.0805, -0.0889, -0.0868,  ...,  0.0488,  0.1191, -0.0554],\n",
      "        [ 0.1245, -0.1052, -0.0716,  ...,  0.0981,  0.0616, -0.0802],\n",
      "        ...,\n",
      "        [ 0.0621,  0.1016,  0.1089,  ...,  0.0488, -0.0239, -0.0645],\n",
      "        [-0.0704, -0.0956,  0.0587,  ...,  0.0442, -0.1043,  0.0921],\n",
      "        [-0.0672,  0.0947,  0.0921,  ..., -0.1023,  0.0511,  0.0835]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.bias tensor([-0.0463, -0.0482,  0.0280, -0.0686, -0.0988, -0.0673, -0.0733, -0.1123,\n",
      "        -0.0716,  0.0571,  0.1165, -0.0762, -0.0144, -0.1066, -0.0080,  0.1027,\n",
      "        -0.0611,  0.0536,  0.1062,  0.0387, -0.0515, -0.0200, -0.0940,  0.0399,\n",
      "         0.0301, -0.0883,  0.1065,  0.0029, -0.1145,  0.0316, -0.0973,  0.0511,\n",
      "         0.0636, -0.0207, -0.0341, -0.0081, -0.0718,  0.0273,  0.0333,  0.0698,\n",
      "        -0.0483, -0.0552,  0.0185, -0.1165, -0.1079,  0.0433, -0.0879, -0.0317,\n",
      "         0.0933,  0.0965,  0.0585, -0.0246, -0.0218, -0.0746, -0.1119,  0.0030,\n",
      "         0.0556, -0.0229, -0.0357,  0.0844, -0.0087, -0.0609, -0.0826, -0.0518],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.weight tensor([[-0.0686,  0.0209, -0.0159,  ...,  0.1047,  0.0429, -0.1198],\n",
      "        [ 0.0638, -0.0108, -0.0980,  ..., -0.0836,  0.0308, -0.0767],\n",
      "        [-0.1114,  0.0920,  0.0930,  ...,  0.0805, -0.1108,  0.0153],\n",
      "        ...,\n",
      "        [-0.1014,  0.1001,  0.0917,  ...,  0.0105, -0.0279,  0.1165],\n",
      "        [-0.1101, -0.0055, -0.0524,  ..., -0.1234, -0.0976, -0.0842],\n",
      "        [ 0.0119,  0.0240,  0.0157,  ...,  0.0464,  0.0430,  0.0867]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.bias tensor([-0.0354, -0.0503, -0.0427, -0.0258,  0.0338, -0.0213, -0.0616,  0.0753,\n",
      "        -0.0554,  0.1166,  0.0627,  0.1206,  0.0918,  0.0535, -0.0687, -0.0986,\n",
      "        -0.0976,  0.0492, -0.0991, -0.0068,  0.1092, -0.0254, -0.0342, -0.0542,\n",
      "         0.0081, -0.0508,  0.0273, -0.1021, -0.0283,  0.0683, -0.0380,  0.0252,\n",
      "        -0.0947, -0.0005, -0.1238, -0.1210, -0.0306, -0.0176,  0.0583,  0.1139,\n",
      "        -0.1039,  0.0451, -0.0414, -0.0518,  0.0741,  0.0518,  0.0111,  0.0484,\n",
      "        -0.0086, -0.1226, -0.1163, -0.0482, -0.1223,  0.0913, -0.0175, -0.1109,\n",
      "        -0.0113, -0.0682, -0.0551,  0.0859,  0.0088,  0.0958, -0.1084,  0.0657],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "conv1d.weight tensor([[[ 0.0038],\n",
      "         [ 0.0141],\n",
      "         [-0.0085],\n",
      "         ...,\n",
      "         [-0.0036],\n",
      "         [ 0.0197],\n",
      "         [ 0.0283]],\n",
      "\n",
      "        [[-0.0163],\n",
      "         [-0.0101],\n",
      "         [ 0.0124],\n",
      "         ...,\n",
      "         [ 0.0275],\n",
      "         [-0.0231],\n",
      "         [ 0.0271]],\n",
      "\n",
      "        [[-0.0187],\n",
      "         [ 0.0006],\n",
      "         [ 0.0301],\n",
      "         ...,\n",
      "         [ 0.0051],\n",
      "         [-0.0382],\n",
      "         [-0.0365]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0217],\n",
      "         [-0.0226],\n",
      "         [-0.0195],\n",
      "         ...,\n",
      "         [-0.0080],\n",
      "         [-0.0204],\n",
      "         [-0.0190]],\n",
      "\n",
      "        [[-0.0127],\n",
      "         [ 0.0053],\n",
      "         [ 0.0138],\n",
      "         ...,\n",
      "         [-0.0316],\n",
      "         [ 0.0017],\n",
      "         [-0.0220]],\n",
      "\n",
      "        [[ 0.0146],\n",
      "         [ 0.0166],\n",
      "         [ 0.0305],\n",
      "         ...,\n",
      "         [-0.0305],\n",
      "         [ 0.0358],\n",
      "         [-0.0010]]], device='cuda:0')\n",
      "conv1d.bias tensor([-1.8472e-02, -3.3557e-02, -1.6331e-02,  1.9784e-04,  2.7500e-03,\n",
      "         8.5102e-06, -3.0190e-02, -1.9152e-02,  3.3277e-02, -3.4147e-02,\n",
      "         4.6923e-03, -1.7847e-02, -9.8912e-03,  2.1519e-02, -6.4126e-03,\n",
      "         1.8312e-02,  3.1748e-03,  2.5110e-02, -3.1591e-02, -1.0388e-02,\n",
      "        -1.0326e-02, -2.2462e-02,  3.8821e-02,  2.7882e-02, -4.1372e-03,\n",
      "         1.7457e-02,  3.1906e-02, -2.6164e-02,  2.3668e-02, -2.1636e-02,\n",
      "        -1.0737e-02, -6.0408e-03,  1.6248e-02,  3.2399e-02,  1.6552e-02,\n",
      "        -1.1423e-02,  3.7610e-02,  2.1017e-03,  3.7001e-03,  1.2535e-02,\n",
      "         7.3908e-03, -3.4918e-02, -1.7059e-02,  1.6962e-02,  3.6828e-03,\n",
      "         3.2356e-02,  2.1142e-02,  2.5013e-02, -3.8319e-02, -7.4356e-03,\n",
      "        -8.0683e-03, -8.1947e-05,  3.9080e-02, -6.2053e-03,  3.7695e-02,\n",
      "        -3.2804e-02,  3.6398e-03,  2.8133e-03, -3.4132e-02, -8.8591e-03,\n",
      "         1.1089e-02,  1.1957e-02,  3.5045e-02,  1.5559e-02], device='cuda:0')\n",
      "fc.0.weight tensor([[ 0.0062, -0.0371, -0.0879,  ...,  0.0436,  0.0074, -0.1190],\n",
      "        [ 0.1095, -0.0917, -0.1223,  ...,  0.0334,  0.0564, -0.0482],\n",
      "        [ 0.1176, -0.1214, -0.1078,  ...,  0.0708,  0.0334,  0.0591],\n",
      "        ...,\n",
      "        [ 0.0672, -0.0197,  0.0083,  ..., -0.0173, -0.0031, -0.1063],\n",
      "        [ 0.0816, -0.0665, -0.1170,  ..., -0.0422, -0.1137,  0.1227],\n",
      "        [-0.0910, -0.0492,  0.0472,  ...,  0.0051, -0.0825, -0.0019]],\n",
      "       device='cuda:0')\n",
      "fc.0.bias tensor([-0.0432,  0.0161, -0.0674, -0.0556, -0.0611,  0.0886,  0.0486, -0.1055,\n",
      "        -0.0691,  0.0138,  0.0785, -0.0232,  0.1073,  0.0579, -0.0389, -0.0083,\n",
      "        -0.0151, -0.1137,  0.1150,  0.0747, -0.0772,  0.1060, -0.0935, -0.0497,\n",
      "        -0.0434,  0.0574,  0.0251, -0.0247, -0.0951, -0.1176, -0.1012, -0.0917,\n",
      "        -0.0698, -0.1053, -0.0798, -0.0185, -0.0294, -0.0644, -0.0983, -0.0408,\n",
      "         0.1220, -0.0927, -0.0488,  0.1099,  0.0542, -0.0028, -0.0797, -0.1026,\n",
      "        -0.1177, -0.1238, -0.0306, -0.0610,  0.0129,  0.0184, -0.0546,  0.0333,\n",
      "        -0.0849,  0.1092,  0.0005, -0.0320, -0.0399,  0.0385,  0.0401,  0.0436],\n",
      "       device='cuda:0')\n",
      "fc.3.weight tensor([[-0.0298,  0.0718,  0.0746,  ..., -0.0645, -0.0535, -0.0538],\n",
      "        [ 0.1143, -0.0847, -0.1057,  ...,  0.0791,  0.1202,  0.0441],\n",
      "        [ 0.0628,  0.0724,  0.0665,  ..., -0.0241,  0.0353,  0.0179],\n",
      "        ...,\n",
      "        [-0.0875, -0.1235, -0.1121,  ...,  0.0933, -0.1015,  0.0946],\n",
      "        [ 0.0087,  0.0447,  0.0107,  ...,  0.0041, -0.0585,  0.0971],\n",
      "        [-0.0832, -0.0498,  0.0465,  ...,  0.0430, -0.1226, -0.0200]],\n",
      "       device='cuda:0')\n",
      "fc.3.bias tensor([-0.0701, -0.1094,  0.0623,  0.0079, -0.1202,  0.0540,  0.0694,  0.0147,\n",
      "         0.0041,  0.0605, -0.0930,  0.1147, -0.0851, -0.0771,  0.0821,  0.0248,\n",
      "         0.1235, -0.0021, -0.0799, -0.0393, -0.1097,  0.0308, -0.0392, -0.0367,\n",
      "         0.0596, -0.0185, -0.0190, -0.0543,  0.0186, -0.0249, -0.0432, -0.0797,\n",
      "        -0.0116, -0.0607, -0.0716,  0.0148,  0.0926,  0.0192,  0.0138,  0.0041,\n",
      "        -0.1045,  0.0031, -0.0266, -0.0198, -0.0155, -0.0590, -0.0343, -0.1152,\n",
      "        -0.0857,  0.0759, -0.0169, -0.0111, -0.0377,  0.0077,  0.0755,  0.1142,\n",
      "        -0.0735, -0.0369,  0.0463, -0.0018, -0.0672, -0.0414,  0.0429, -0.0814,\n",
      "        -0.0048,  0.0267, -0.0821,  0.0964,  0.1042, -0.0793,  0.0269, -0.0384,\n",
      "         0.0482,  0.0720, -0.0868, -0.0747, -0.0622, -0.0950,  0.0572, -0.0401,\n",
      "         0.0170, -0.0791,  0.0263,  0.0247,  0.0454, -0.0346, -0.0893, -0.1116,\n",
      "         0.0576,  0.0905,  0.0789, -0.0109,  0.0495,  0.1210, -0.1130,  0.0433,\n",
      "         0.0804,  0.0266, -0.0558,  0.0593, -0.0605,  0.0820,  0.0061,  0.0837,\n",
      "         0.1232,  0.0933,  0.0779,  0.0292,  0.0290,  0.0015, -0.0314,  0.0532,\n",
      "         0.0698,  0.0513,  0.0106, -0.0166,  0.0208, -0.1073, -0.0387,  0.0813,\n",
      "         0.1072,  0.0934,  0.0485,  0.0982, -0.0314,  0.0414, -0.0410,  0.0514],\n",
      "       device='cuda:0')\n",
      "fc.6.weight tensor([[-0.0426, -0.0868, -0.0524,  ...,  0.0416, -0.0280, -0.0561],\n",
      "        [-0.0079, -0.0045, -0.0843,  ...,  0.0298,  0.0584,  0.0883],\n",
      "        [ 0.0013, -0.0024,  0.0235,  ..., -0.0785,  0.0579,  0.0484],\n",
      "        ...,\n",
      "        [ 0.0300, -0.0690, -0.0839,  ...,  0.0197, -0.0201, -0.0878],\n",
      "        [-0.0880,  0.0112, -0.0442,  ...,  0.0149,  0.0509, -0.0072],\n",
      "        [ 0.0030, -0.0797,  0.0548,  ...,  0.0730, -0.0809,  0.0286]],\n",
      "       device='cuda:0')\n",
      "fc.6.bias tensor([ 0.0429,  0.0670, -0.0839, -0.0704, -0.0790,  0.0878,  0.0630,  0.0368,\n",
      "         0.0486,  0.0637, -0.0406, -0.0135, -0.0727, -0.0331,  0.0839, -0.0083,\n",
      "        -0.0149,  0.0807,  0.0280, -0.0157,  0.0212,  0.0671,  0.0572,  0.0005,\n",
      "        -0.0855, -0.0713,  0.0090,  0.0750, -0.0472,  0.0163, -0.0423,  0.0255,\n",
      "        -0.0752,  0.0586, -0.0262,  0.0205, -0.0358,  0.0596,  0.0228, -0.0253,\n",
      "        -0.0785, -0.0063,  0.0842, -0.0440, -0.0442, -0.0303, -0.0087,  0.0696,\n",
      "        -0.0481,  0.0432,  0.0285, -0.0701,  0.0079,  0.0641,  0.0855,  0.0372,\n",
      "         0.0410,  0.0695,  0.0549,  0.0676, -0.0233, -0.0262,  0.0654, -0.0647],\n",
      "       device='cuda:0')\n",
      "fc.8.weight tensor([[ 0.1051, -0.1120, -0.0529,  0.0566, -0.1059,  0.0431,  0.1037,  0.0466,\n",
      "          0.0907, -0.1180,  0.0574, -0.0008, -0.0337, -0.0655,  0.1054,  0.1201,\n",
      "          0.1097,  0.0968, -0.0778, -0.0112,  0.0371, -0.1100, -0.0213, -0.0722,\n",
      "         -0.0993,  0.0507, -0.0992, -0.0783,  0.1062, -0.0017, -0.0794, -0.0098,\n",
      "         -0.1093, -0.0273,  0.1058,  0.1042, -0.0863, -0.0415, -0.1046, -0.0375,\n",
      "          0.0735, -0.0007,  0.0920, -0.0972, -0.0499, -0.0190,  0.0966, -0.0561,\n",
      "         -0.0028,  0.0248,  0.0160,  0.1004, -0.0363, -0.0878, -0.0564,  0.0086,\n",
      "         -0.1157, -0.0619,  0.0678, -0.0731,  0.0962,  0.0914,  0.0916,  0.0063]],\n",
      "       device='cuda:0')\n",
      "fc.8.bias tensor([-0.1008], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Divide train and test dataset into batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Check whether data is splitted correctly -> X_.shape: (batch, seq, encoding), y_.shape: (batch)\n",
    "# for i, (X_, y_) in enumerate(train_loader): \n",
    "#     print(X_.shape, y_.shape)\n",
    "#     print(X_[:10,:])\n",
    "#     print(y_[:10])\n",
    "\n",
    "# Print trainable model parameters with their corresponding initial values\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on 10800 transactions in test data: 89.712963%\n",
      "f1_score_test:  0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95      9689\n",
      "         1.0       0.00      0.00      0.00      1111\n",
      "\n",
      "    accuracy                           0.90     10800\n",
      "   macro avg       0.45      0.50      0.47     10800\n",
      "weighted avg       0.80      0.90      0.85     10800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onatinak/anaconda3/envs/case_study/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/onatinak/anaconda3/envs/case_study/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/onatinak/anaconda3/envs/case_study/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (input_embedding): Embedding(312, 64)\n",
       "  (position_embedding): Embedding(10, 64)\n",
       "  (transformer_encoder_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.5, inplace=False)\n",
       "    (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (dropout): Dropout(p=0.5, inplace=False)\n",
       "        (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.5, inplace=False)\n",
       "        (dropout2): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv1d): Conv1d(640, 64, kernel_size=(1,), stride=(1,))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=128, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (9): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Observe initial performance of the model without any training\n",
    "model.eval()\n",
    "test(model, test_loader, device, config.project_name, save_model = False)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer:  AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0.01\n",
      ")\n",
      "\n",
      "loss function:  BCEWithLogitsLoss()\n",
      "\n",
      "scheduler <torch.optim.lr_scheduler.ExponentialLR object at 0x7f86de527f70>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(float(config.pos_weight)).to(dtype=torch.long, device=config.device) * torch.ones([1]).to(config.device))\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight = 5 * torch.ones([1]).to(config.device))\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.999),\n",
    "#                              eps=1e-8, weight_decay=config.weight_decay, amsgrad=False)  \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01,\n",
    "                             amsgrad=False, maximize=False, foreach=None, capturable=False, \n",
    "                             differentiable=False, fused=None)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.gamma)\n",
    "\n",
    "print('optimizer: ', optimizer)\n",
    "print('')\n",
    "print('loss function: ', criterion)\n",
    "print('')\n",
    "print('scheduler', scheduler)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2b33fbf5ab4400bf3a900b3ae89a0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      1.00      0.94      8817\n",
      "         1.0       0.00      0.00      0.00      1083\n",
      "\n",
      "    accuracy                           0.89      9900\n",
      "   macro avg       0.45      0.50      0.47      9900\n",
      "weighted avg       0.79      0.89      0.84      9900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onatinak/anaconda3/envs/case_study/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/onatinak/anaconda3/envs/case_study/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/onatinak/anaconda3/envs/case_study/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [10/100], Train Loss: 1.2364\n",
      "learning_rate:  0.001\n",
      "Train Loss after 10 batches: 1.236\n",
      "Epoch [1/50], Step [20/100], Train Loss: 1.2233\n",
      "learning_rate:  0.001\n",
      "Train Loss after 20 batches: 1.223\n",
      "Epoch [1/50], Step [30/100], Train Loss: 1.2290\n",
      "learning_rate:  0.001\n",
      "Train Loss after 30 batches: 1.229\n",
      "Epoch [1/50], Step [40/100], Train Loss: 1.1823\n",
      "learning_rate:  0.001\n",
      "Train Loss after 40 batches: 1.182\n",
      "Epoch [1/50], Step [50/100], Train Loss: 1.1822\n",
      "learning_rate:  0.001\n",
      "Train Loss after 50 batches: 1.182\n",
      "Epoch [1/50], Step [60/100], Train Loss: 1.1611\n",
      "learning_rate:  0.001\n",
      "Train Loss after 60 batches: 1.161\n",
      "Epoch [1/50], Step [70/100], Train Loss: 1.1199\n",
      "learning_rate:  0.001\n",
      "Train Loss after 70 batches: 1.120\n",
      "Epoch [1/50], Step [80/100], Train Loss: 1.0868\n",
      "learning_rate:  0.001\n",
      "Train Loss after 80 batches: 1.087\n",
      "Epoch [1/50], Step [90/100], Train Loss: 1.0795\n",
      "learning_rate:  0.001\n",
      "Train Loss after 90 batches: 1.079\n",
      "Epoch [1/50], Step [100/100], Train Loss: 1.0871\n",
      "learning_rate:  0.001\n",
      "Train Loss after 100 batches: 1.087\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.79      0.87      8818\n",
      "         1.0       0.31      0.75      0.44      1082\n",
      "\n",
      "    accuracy                           0.79      9900\n",
      "   macro avg       0.64      0.77      0.65      9900\n",
      "weighted avg       0.89      0.79      0.82      9900\n",
      "\n",
      "Validation Accuracy: 78.959596%\n",
      "\n",
      "Epoch [2/50], Step [10/100], Train Loss: 1.0995\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 110 batches: 1.099\n",
      "Epoch [2/50], Step [20/100], Train Loss: 1.0794\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 120 batches: 1.079\n",
      "Epoch [2/50], Step [30/100], Train Loss: 1.0635\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 130 batches: 1.064\n",
      "Epoch [2/50], Step [40/100], Train Loss: 1.0864\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 140 batches: 1.086\n",
      "Epoch [2/50], Step [50/100], Train Loss: 1.0936\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 150 batches: 1.094\n",
      "Epoch [2/50], Step [60/100], Train Loss: 1.0941\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 160 batches: 1.094\n",
      "Epoch [2/50], Step [70/100], Train Loss: 1.0757\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 170 batches: 1.076\n",
      "Epoch [2/50], Step [80/100], Train Loss: 1.0762\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 180 batches: 1.076\n",
      "Epoch [2/50], Step [90/100], Train Loss: 1.0654\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 190 batches: 1.065\n",
      "Epoch [2/50], Step [100/100], Train Loss: 1.0778\n",
      "learning_rate:  0.00085\n",
      "Train Loss after 200 batches: 1.078\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.84      0.90      8818\n",
      "         1.0       0.35      0.73      0.48      1082\n",
      "\n",
      "    accuracy                           0.83      9900\n",
      "   macro avg       0.66      0.78      0.69      9900\n",
      "weighted avg       0.90      0.83      0.85      9900\n",
      "\n",
      "Validation Accuracy: 82.525253%\n",
      "\n",
      "Epoch [3/50], Step [10/100], Train Loss: 1.0721\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 210 batches: 1.072\n",
      "Epoch [3/50], Step [20/100], Train Loss: 1.0451\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 220 batches: 1.045\n",
      "Epoch [3/50], Step [30/100], Train Loss: 1.0705\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 230 batches: 1.071\n",
      "Epoch [3/50], Step [40/100], Train Loss: 1.0570\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 240 batches: 1.057\n",
      "Epoch [3/50], Step [50/100], Train Loss: 1.0792\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 250 batches: 1.079\n",
      "Epoch [3/50], Step [60/100], Train Loss: 1.0762\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 260 batches: 1.076\n",
      "Epoch [3/50], Step [70/100], Train Loss: 1.0514\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 270 batches: 1.051\n",
      "Epoch [3/50], Step [80/100], Train Loss: 1.0592\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 280 batches: 1.059\n",
      "Epoch [3/50], Step [90/100], Train Loss: 1.0679\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 290 batches: 1.068\n",
      "Epoch [3/50], Step [100/100], Train Loss: 1.0529\n",
      "learning_rate:  0.0007224999999999999\n",
      "Train Loss after 300 batches: 1.053\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8821\n",
      "         1.0       0.38      0.70      0.49      1079\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.67      0.78      0.70      9900\n",
      "weighted avg       0.90      0.85      0.86      9900\n",
      "\n",
      "Validation Accuracy: 84.515154%\n",
      "\n",
      "Epoch [4/50], Step [10/100], Train Loss: 1.0647\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 310 batches: 1.065\n",
      "Epoch [4/50], Step [20/100], Train Loss: 1.0471\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 320 batches: 1.047\n",
      "Epoch [4/50], Step [30/100], Train Loss: 1.0682\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 330 batches: 1.068\n",
      "Epoch [4/50], Step [40/100], Train Loss: 1.0559\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 340 batches: 1.056\n",
      "Epoch [4/50], Step [50/100], Train Loss: 1.0344\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 350 batches: 1.034\n",
      "Epoch [4/50], Step [60/100], Train Loss: 1.0600\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 360 batches: 1.060\n",
      "Epoch [4/50], Step [70/100], Train Loss: 1.0604\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 370 batches: 1.060\n",
      "Epoch [4/50], Step [80/100], Train Loss: 1.0560\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 380 batches: 1.056\n",
      "Epoch [4/50], Step [90/100], Train Loss: 1.0685\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 390 batches: 1.068\n",
      "Epoch [4/50], Step [100/100], Train Loss: 1.0634\n",
      "learning_rate:  0.000614125\n",
      "Train Loss after 400 batches: 1.063\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8820\n",
      "         1.0       0.39      0.70      0.50      1080\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.78      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 84.858590%\n",
      "\n",
      "Epoch [5/50], Step [10/100], Train Loss: 1.0507\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 410 batches: 1.051\n",
      "Epoch [5/50], Step [20/100], Train Loss: 1.0444\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 420 batches: 1.044\n",
      "Epoch [5/50], Step [30/100], Train Loss: 1.0530\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 430 batches: 1.053\n",
      "Epoch [5/50], Step [40/100], Train Loss: 1.0554\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 440 batches: 1.055\n",
      "Epoch [5/50], Step [50/100], Train Loss: 1.0573\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 450 batches: 1.057\n",
      "Epoch [5/50], Step [60/100], Train Loss: 1.0506\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 460 batches: 1.051\n",
      "Epoch [5/50], Step [70/100], Train Loss: 1.0515\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 470 batches: 1.051\n",
      "Epoch [5/50], Step [80/100], Train Loss: 1.0625\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 480 batches: 1.063\n",
      "Epoch [5/50], Step [90/100], Train Loss: 1.0498\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 490 batches: 1.050\n",
      "Epoch [5/50], Step [100/100], Train Loss: 1.0631\n",
      "learning_rate:  0.00052200625\n",
      "Train Loss after 500 batches: 1.063\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8827\n",
      "         1.0       0.39      0.70      0.50      1073\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.67      0.78      0.70      9900\n",
      "weighted avg       0.90      0.85      0.86      9900\n",
      "\n",
      "Validation Accuracy: 84.666669%\n",
      "\n",
      "Epoch [6/50], Step [10/100], Train Loss: 1.0422\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 510 batches: 1.042\n",
      "Epoch [6/50], Step [20/100], Train Loss: 1.0635\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 520 batches: 1.064\n",
      "Epoch [6/50], Step [30/100], Train Loss: 1.0499\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 530 batches: 1.050\n",
      "Epoch [6/50], Step [40/100], Train Loss: 1.0583\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 540 batches: 1.058\n",
      "Epoch [6/50], Step [50/100], Train Loss: 1.0505\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 550 batches: 1.050\n",
      "Epoch [6/50], Step [60/100], Train Loss: 1.0523\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 560 batches: 1.052\n",
      "Epoch [6/50], Step [70/100], Train Loss: 1.0476\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 570 batches: 1.048\n",
      "Epoch [6/50], Step [80/100], Train Loss: 1.0504\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 580 batches: 1.050\n",
      "Epoch [6/50], Step [90/100], Train Loss: 1.0578\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 590 batches: 1.058\n",
      "Epoch [6/50], Step [100/100], Train Loss: 1.0496\n",
      "learning_rate:  0.00044370531249999997\n",
      "Train Loss after 600 batches: 1.050\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8814\n",
      "         1.0       0.39      0.72      0.51      1086\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 84.717172%\n",
      "\n",
      "Epoch [7/50], Step [10/100], Train Loss: 1.0555\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 610 batches: 1.056\n",
      "Epoch [7/50], Step [20/100], Train Loss: 1.0523\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 620 batches: 1.052\n",
      "Epoch [7/50], Step [30/100], Train Loss: 1.0528\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 630 batches: 1.053\n",
      "Epoch [7/50], Step [40/100], Train Loss: 1.0549\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 640 batches: 1.055\n",
      "Epoch [7/50], Step [50/100], Train Loss: 1.0587\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 650 batches: 1.059\n",
      "Epoch [7/50], Step [60/100], Train Loss: 1.0393\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 660 batches: 1.039\n",
      "Epoch [7/50], Step [70/100], Train Loss: 1.0429\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 670 batches: 1.043\n",
      "Epoch [7/50], Step [80/100], Train Loss: 1.0443\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 680 batches: 1.044\n",
      "Epoch [7/50], Step [90/100], Train Loss: 1.0374\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 690 batches: 1.037\n",
      "Epoch [7/50], Step [100/100], Train Loss: 1.0549\n",
      "learning_rate:  0.00037714951562499996\n",
      "Train Loss after 700 batches: 1.055\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8815\n",
      "         1.0       0.39      0.71      0.51      1085\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 84.848487%\n",
      "\n",
      "Epoch [8/50], Step [10/100], Train Loss: 1.0546\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 710 batches: 1.055\n",
      "Epoch [8/50], Step [20/100], Train Loss: 1.0678\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 720 batches: 1.068\n",
      "Epoch [8/50], Step [30/100], Train Loss: 1.0344\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 730 batches: 1.034\n",
      "Epoch [8/50], Step [40/100], Train Loss: 1.0447\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 740 batches: 1.045\n",
      "Epoch [8/50], Step [50/100], Train Loss: 1.0508\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 750 batches: 1.051\n",
      "Epoch [8/50], Step [60/100], Train Loss: 1.0642\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 760 batches: 1.064\n",
      "Epoch [8/50], Step [70/100], Train Loss: 1.0559\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 770 batches: 1.056\n",
      "Epoch [8/50], Step [80/100], Train Loss: 1.0292\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 780 batches: 1.029\n",
      "Epoch [8/50], Step [90/100], Train Loss: 1.0457\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 790 batches: 1.046\n",
      "Epoch [8/50], Step [100/100], Train Loss: 1.0618\n",
      "learning_rate:  0.00032057708828124994\n",
      "Train Loss after 800 batches: 1.062\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.85      0.90      8816\n",
      "         1.0       0.37      0.74      0.49      1084\n",
      "\n",
      "    accuracy                           0.83      9900\n",
      "   macro avg       0.67      0.79      0.70      9900\n",
      "weighted avg       0.90      0.83      0.86      9900\n",
      "\n",
      "Validation Accuracy: 83.373737%\n",
      "\n",
      "Epoch [9/50], Step [10/100], Train Loss: 1.0460\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 810 batches: 1.046\n",
      "Epoch [9/50], Step [20/100], Train Loss: 1.0661\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 820 batches: 1.066\n",
      "Epoch [9/50], Step [30/100], Train Loss: 1.0399\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 830 batches: 1.040\n",
      "Epoch [9/50], Step [40/100], Train Loss: 1.0335\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 840 batches: 1.033\n",
      "Epoch [9/50], Step [50/100], Train Loss: 1.0758\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 850 batches: 1.076\n",
      "Epoch [9/50], Step [60/100], Train Loss: 1.0406\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 860 batches: 1.041\n",
      "Epoch [9/50], Step [70/100], Train Loss: 1.0607\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 870 batches: 1.061\n",
      "Epoch [9/50], Step [80/100], Train Loss: 1.0468\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 880 batches: 1.047\n",
      "Epoch [9/50], Step [90/100], Train Loss: 1.0375\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 890 batches: 1.038\n",
      "Epoch [9/50], Step [100/100], Train Loss: 1.0525\n",
      "learning_rate:  0.0002724905250390624\n",
      "Train Loss after 900 batches: 1.053\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8808\n",
      "         1.0       0.38      0.73      0.50      1092\n",
      "\n",
      "    accuracy                           0.84      9900\n",
      "   macro avg       0.67      0.79      0.70      9900\n",
      "weighted avg       0.90      0.84      0.86      9900\n",
      "\n",
      "Validation Accuracy: 84.111112%\n",
      "\n",
      "Epoch [10/50], Step [10/100], Train Loss: 1.0533\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 910 batches: 1.053\n",
      "Epoch [10/50], Step [20/100], Train Loss: 1.0479\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 920 batches: 1.048\n",
      "Epoch [10/50], Step [30/100], Train Loss: 1.0659\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 930 batches: 1.066\n",
      "Epoch [10/50], Step [40/100], Train Loss: 1.0343\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 940 batches: 1.034\n",
      "Epoch [10/50], Step [50/100], Train Loss: 1.0436\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 950 batches: 1.044\n",
      "Epoch [10/50], Step [60/100], Train Loss: 1.0536\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 960 batches: 1.054\n",
      "Epoch [10/50], Step [70/100], Train Loss: 1.0462\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 970 batches: 1.046\n",
      "Epoch [10/50], Step [80/100], Train Loss: 1.0443\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 980 batches: 1.044\n",
      "Epoch [10/50], Step [90/100], Train Loss: 1.0490\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 990 batches: 1.049\n",
      "Epoch [10/50], Step [100/100], Train Loss: 1.0392\n",
      "learning_rate:  0.00023161694628320305\n",
      "Train Loss after 1000 batches: 1.039\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.89      0.92      8816\n",
      "         1.0       0.43      0.67      0.52      1084\n",
      "\n",
      "    accuracy                           0.87      9900\n",
      "   macro avg       0.69      0.78      0.72      9900\n",
      "weighted avg       0.90      0.87      0.88      9900\n",
      "\n",
      "Validation Accuracy: 86.717176%\n",
      "\n",
      "Epoch [11/50], Step [10/100], Train Loss: 1.0416\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1010 batches: 1.042\n",
      "Epoch [11/50], Step [20/100], Train Loss: 1.0446\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1020 batches: 1.045\n",
      "Epoch [11/50], Step [30/100], Train Loss: 1.0588\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1030 batches: 1.059\n",
      "Epoch [11/50], Step [40/100], Train Loss: 1.0456\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1040 batches: 1.046\n",
      "Epoch [11/50], Step [50/100], Train Loss: 1.0444\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1050 batches: 1.044\n",
      "Epoch [11/50], Step [60/100], Train Loss: 1.0494\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1060 batches: 1.049\n",
      "Epoch [11/50], Step [70/100], Train Loss: 1.0410\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1070 batches: 1.041\n",
      "Epoch [11/50], Step [80/100], Train Loss: 1.0473\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1080 batches: 1.047\n",
      "Epoch [11/50], Step [90/100], Train Loss: 1.0555\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1090 batches: 1.055\n",
      "Epoch [11/50], Step [100/100], Train Loss: 1.0424\n",
      "learning_rate:  0.0001968744043407226\n",
      "Train Loss after 1100 batches: 1.042\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.92      8813\n",
      "         1.0       0.41      0.70      0.52      1087\n",
      "\n",
      "    accuracy                           0.86      9900\n",
      "   macro avg       0.68      0.79      0.72      9900\n",
      "weighted avg       0.90      0.86      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.575759%\n",
      "\n",
      "Epoch [12/50], Step [10/100], Train Loss: 1.0399\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1110 batches: 1.040\n",
      "Epoch [12/50], Step [20/100], Train Loss: 1.0432\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1120 batches: 1.043\n",
      "Epoch [12/50], Step [30/100], Train Loss: 1.0402\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1130 batches: 1.040\n",
      "Epoch [12/50], Step [40/100], Train Loss: 1.0577\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1140 batches: 1.058\n",
      "Epoch [12/50], Step [50/100], Train Loss: 1.0475\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1150 batches: 1.047\n",
      "Epoch [12/50], Step [60/100], Train Loss: 1.0379\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1160 batches: 1.038\n",
      "Epoch [12/50], Step [70/100], Train Loss: 1.0433\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1170 batches: 1.043\n",
      "Epoch [12/50], Step [80/100], Train Loss: 1.0540\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1180 batches: 1.054\n",
      "Epoch [12/50], Step [90/100], Train Loss: 1.0481\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1190 batches: 1.048\n",
      "Epoch [12/50], Step [100/100], Train Loss: 1.0491\n",
      "learning_rate:  0.0001673432436896142\n",
      "Train Loss after 1200 batches: 1.049\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8812\n",
      "         1.0       0.41      0.70      0.52      1088\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.72      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.494953%\n",
      "\n",
      "Epoch [13/50], Step [10/100], Train Loss: 1.0307\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1210 batches: 1.031\n",
      "Epoch [13/50], Step [20/100], Train Loss: 1.0578\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1220 batches: 1.058\n",
      "Epoch [13/50], Step [30/100], Train Loss: 1.0451\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1230 batches: 1.045\n",
      "Epoch [13/50], Step [40/100], Train Loss: 1.0427\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1240 batches: 1.043\n",
      "Epoch [13/50], Step [50/100], Train Loss: 1.0387\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1250 batches: 1.039\n",
      "Epoch [13/50], Step [60/100], Train Loss: 1.0450\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1260 batches: 1.045\n",
      "Epoch [13/50], Step [70/100], Train Loss: 1.0598\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1270 batches: 1.060\n",
      "Epoch [13/50], Step [80/100], Train Loss: 1.0423\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1280 batches: 1.042\n",
      "Epoch [13/50], Step [90/100], Train Loss: 1.0440\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1290 batches: 1.044\n",
      "Epoch [13/50], Step [100/100], Train Loss: 1.0384\n",
      "learning_rate:  0.00014224175713617207\n",
      "Train Loss after 1300 batches: 1.038\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8819\n",
      "         1.0       0.40      0.71      0.52      1081\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.373741%\n",
      "\n",
      "Epoch [14/50], Step [10/100], Train Loss: 1.0329\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1310 batches: 1.033\n",
      "Epoch [14/50], Step [20/100], Train Loss: 1.0460\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1320 batches: 1.046\n",
      "Epoch [14/50], Step [30/100], Train Loss: 1.0446\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1330 batches: 1.045\n",
      "Epoch [14/50], Step [40/100], Train Loss: 1.0521\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1340 batches: 1.052\n",
      "Epoch [14/50], Step [50/100], Train Loss: 1.0463\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1350 batches: 1.046\n",
      "Epoch [14/50], Step [60/100], Train Loss: 1.0501\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1360 batches: 1.050\n",
      "Epoch [14/50], Step [70/100], Train Loss: 1.0521\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1370 batches: 1.052\n",
      "Epoch [14/50], Step [80/100], Train Loss: 1.0446\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1380 batches: 1.045\n",
      "Epoch [14/50], Step [90/100], Train Loss: 1.0390\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1390 batches: 1.039\n",
      "Epoch [14/50], Step [100/100], Train Loss: 1.0487\n",
      "learning_rate:  0.00012090549356574625\n",
      "Train Loss after 1400 batches: 1.049\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8818\n",
      "         1.0       0.40      0.71      0.51      1082\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.222226%\n",
      "\n",
      "Epoch [15/50], Step [10/100], Train Loss: 1.0473\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1410 batches: 1.047\n",
      "Epoch [15/50], Step [20/100], Train Loss: 1.0535\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1420 batches: 1.054\n",
      "Epoch [15/50], Step [30/100], Train Loss: 1.0477\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1430 batches: 1.048\n",
      "Epoch [15/50], Step [40/100], Train Loss: 1.0381\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1440 batches: 1.038\n",
      "Epoch [15/50], Step [50/100], Train Loss: 1.0365\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1450 batches: 1.037\n",
      "Epoch [15/50], Step [60/100], Train Loss: 1.0357\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1460 batches: 1.036\n",
      "Epoch [15/50], Step [70/100], Train Loss: 1.0474\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1470 batches: 1.047\n",
      "Epoch [15/50], Step [80/100], Train Loss: 1.0443\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1480 batches: 1.044\n",
      "Epoch [15/50], Step [90/100], Train Loss: 1.0530\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1490 batches: 1.053\n",
      "Epoch [15/50], Step [100/100], Train Loss: 1.0457\n",
      "learning_rate:  0.00010276966953088431\n",
      "Train Loss after 1500 batches: 1.046\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8812\n",
      "         1.0       0.39      0.72      0.50      1088\n",
      "\n",
      "    accuracy                           0.84      9900\n",
      "   macro avg       0.67      0.79      0.70      9900\n",
      "weighted avg       0.90      0.84      0.86      9900\n",
      "\n",
      "Validation Accuracy: 84.282833%\n",
      "\n",
      "Epoch [16/50], Step [10/100], Train Loss: 1.0306\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1510 batches: 1.031\n",
      "Epoch [16/50], Step [20/100], Train Loss: 1.0458\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1520 batches: 1.046\n",
      "Epoch [16/50], Step [30/100], Train Loss: 1.0541\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1530 batches: 1.054\n",
      "Epoch [16/50], Step [40/100], Train Loss: 1.0433\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1540 batches: 1.043\n",
      "Epoch [16/50], Step [50/100], Train Loss: 1.0451\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1550 batches: 1.045\n",
      "Epoch [16/50], Step [60/100], Train Loss: 1.0412\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1560 batches: 1.041\n",
      "Epoch [16/50], Step [70/100], Train Loss: 1.0623\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1570 batches: 1.062\n",
      "Epoch [16/50], Step [80/100], Train Loss: 1.0397\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1580 batches: 1.040\n",
      "Epoch [16/50], Step [90/100], Train Loss: 1.0502\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1590 batches: 1.050\n",
      "Epoch [16/50], Step [100/100], Train Loss: 1.0270\n",
      "learning_rate:  8.735421910125166e-05\n",
      "Train Loss after 1600 batches: 1.027\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8816\n",
      "         1.0       0.40      0.70      0.51      1084\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.333335%\n",
      "\n",
      "Epoch [17/50], Step [10/100], Train Loss: 1.0457\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1610 batches: 1.046\n",
      "Epoch [17/50], Step [20/100], Train Loss: 1.0386\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1620 batches: 1.039\n",
      "Epoch [17/50], Step [30/100], Train Loss: 1.0437\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1630 batches: 1.044\n",
      "Epoch [17/50], Step [40/100], Train Loss: 1.0428\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1640 batches: 1.043\n",
      "Epoch [17/50], Step [50/100], Train Loss: 1.0489\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1650 batches: 1.049\n",
      "Epoch [17/50], Step [60/100], Train Loss: 1.0470\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1660 batches: 1.047\n",
      "Epoch [17/50], Step [70/100], Train Loss: 1.0532\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1670 batches: 1.053\n",
      "Epoch [17/50], Step [80/100], Train Loss: 1.0626\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1680 batches: 1.063\n",
      "Epoch [17/50], Step [90/100], Train Loss: 1.0433\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1690 batches: 1.043\n",
      "Epoch [17/50], Step [100/100], Train Loss: 1.0282\n",
      "learning_rate:  7.425108623606391e-05\n",
      "Train Loss after 1700 batches: 1.028\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8814\n",
      "         1.0       0.39      0.71      0.51      1086\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 84.707075%\n",
      "\n",
      "Epoch [18/50], Step [10/100], Train Loss: 1.0492\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1710 batches: 1.049\n",
      "Epoch [18/50], Step [20/100], Train Loss: 1.0405\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1720 batches: 1.040\n",
      "Epoch [18/50], Step [30/100], Train Loss: 1.0557\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1730 batches: 1.056\n",
      "Epoch [18/50], Step [40/100], Train Loss: 1.0525\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1740 batches: 1.053\n",
      "Epoch [18/50], Step [50/100], Train Loss: 1.0521\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1750 batches: 1.052\n",
      "Epoch [18/50], Step [60/100], Train Loss: 1.0289\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1760 batches: 1.029\n",
      "Epoch [18/50], Step [70/100], Train Loss: 1.0350\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1770 batches: 1.035\n",
      "Epoch [18/50], Step [80/100], Train Loss: 1.0589\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1780 batches: 1.059\n",
      "Epoch [18/50], Step [90/100], Train Loss: 1.0324\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1790 batches: 1.032\n",
      "Epoch [18/50], Step [100/100], Train Loss: 1.0434\n",
      "learning_rate:  6.311342330065433e-05\n",
      "Train Loss after 1800 batches: 1.043\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.88      0.92      8814\n",
      "         1.0       0.41      0.70      0.52      1086\n",
      "\n",
      "    accuracy                           0.86      9900\n",
      "   macro avg       0.68      0.79      0.72      9900\n",
      "weighted avg       0.90      0.86      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.606062%\n",
      "\n",
      "Epoch [19/50], Step [10/100], Train Loss: 1.0564\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1810 batches: 1.056\n",
      "Epoch [19/50], Step [20/100], Train Loss: 1.0482\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1820 batches: 1.048\n",
      "Epoch [19/50], Step [30/100], Train Loss: 1.0279\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1830 batches: 1.028\n",
      "Epoch [19/50], Step [40/100], Train Loss: 1.0365\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1840 batches: 1.036\n",
      "Epoch [19/50], Step [50/100], Train Loss: 1.0488\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1850 batches: 1.049\n",
      "Epoch [19/50], Step [60/100], Train Loss: 1.0393\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1860 batches: 1.039\n",
      "Epoch [19/50], Step [70/100], Train Loss: 1.0360\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1870 batches: 1.036\n",
      "Epoch [19/50], Step [80/100], Train Loss: 1.0453\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1880 batches: 1.045\n",
      "Epoch [19/50], Step [90/100], Train Loss: 1.0442\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1890 batches: 1.044\n",
      "Epoch [19/50], Step [100/100], Train Loss: 1.0562\n",
      "learning_rate:  5.3646409805556176e-05\n",
      "Train Loss after 1900 batches: 1.056\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      8818\n",
      "         1.0       0.39      0.71      0.50      1082\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.86      9900\n",
      "\n",
      "Validation Accuracy: 84.656566%\n",
      "\n",
      "Epoch [20/50], Step [10/100], Train Loss: 1.0347\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1910 batches: 1.035\n",
      "Epoch [20/50], Step [20/100], Train Loss: 1.0336\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1920 batches: 1.034\n",
      "Epoch [20/50], Step [30/100], Train Loss: 1.0469\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1930 batches: 1.047\n",
      "Epoch [20/50], Step [40/100], Train Loss: 1.0408\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1940 batches: 1.041\n",
      "Epoch [20/50], Step [50/100], Train Loss: 1.0487\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1950 batches: 1.049\n",
      "Epoch [20/50], Step [60/100], Train Loss: 1.0386\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1960 batches: 1.039\n",
      "Epoch [20/50], Step [70/100], Train Loss: 1.0517\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1970 batches: 1.052\n",
      "Epoch [20/50], Step [80/100], Train Loss: 1.0509\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1980 batches: 1.051\n",
      "Epoch [20/50], Step [90/100], Train Loss: 1.0605\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 1990 batches: 1.060\n",
      "Epoch [20/50], Step [100/100], Train Loss: 1.0394\n",
      "learning_rate:  4.559944833472275e-05\n",
      "Train Loss after 2000 batches: 1.039\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8826\n",
      "         1.0       0.40      0.70      0.51      1074\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.252529%\n",
      "\n",
      "Epoch [21/50], Step [10/100], Train Loss: 1.0444\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2010 batches: 1.044\n",
      "Epoch [21/50], Step [20/100], Train Loss: 1.0449\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2020 batches: 1.045\n",
      "Epoch [21/50], Step [30/100], Train Loss: 1.0306\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2030 batches: 1.031\n",
      "Epoch [21/50], Step [40/100], Train Loss: 1.0407\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2040 batches: 1.041\n",
      "Epoch [21/50], Step [50/100], Train Loss: 1.0413\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2050 batches: 1.041\n",
      "Epoch [21/50], Step [60/100], Train Loss: 1.0358\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2060 batches: 1.036\n",
      "Epoch [21/50], Step [70/100], Train Loss: 1.0402\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2070 batches: 1.040\n",
      "Epoch [21/50], Step [80/100], Train Loss: 1.0505\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2080 batches: 1.050\n",
      "Epoch [21/50], Step [90/100], Train Loss: 1.0431\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2090 batches: 1.043\n",
      "Epoch [21/50], Step [100/100], Train Loss: 1.0610\n",
      "learning_rate:  3.875953108451434e-05\n",
      "Train Loss after 2100 batches: 1.061\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8821\n",
      "         1.0       0.40      0.71      0.51      1079\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.343438%\n",
      "\n",
      "Epoch [22/50], Step [10/100], Train Loss: 1.0579\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2110 batches: 1.058\n",
      "Epoch [22/50], Step [20/100], Train Loss: 1.0271\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2120 batches: 1.027\n",
      "Epoch [22/50], Step [30/100], Train Loss: 1.0590\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2130 batches: 1.059\n",
      "Epoch [22/50], Step [40/100], Train Loss: 1.0449\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2140 batches: 1.045\n",
      "Epoch [22/50], Step [50/100], Train Loss: 1.0400\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2150 batches: 1.040\n",
      "Epoch [22/50], Step [60/100], Train Loss: 1.0429\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2160 batches: 1.043\n",
      "Epoch [22/50], Step [70/100], Train Loss: 1.0345\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2170 batches: 1.035\n",
      "Epoch [22/50], Step [80/100], Train Loss: 1.0366\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2180 batches: 1.037\n",
      "Epoch [22/50], Step [90/100], Train Loss: 1.0463\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2190 batches: 1.046\n",
      "Epoch [22/50], Step [100/100], Train Loss: 1.0461\n",
      "learning_rate:  3.294560142183719e-05\n",
      "Train Loss after 2200 batches: 1.046\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8822\n",
      "         1.0       0.40      0.70      0.51      1078\n",
      "\n",
      "    accuracy                           0.86      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.86      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.515153%\n",
      "\n",
      "Epoch [23/50], Step [10/100], Train Loss: 1.0585\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2210 batches: 1.059\n",
      "Epoch [23/50], Step [20/100], Train Loss: 1.0305\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2220 batches: 1.030\n",
      "Epoch [23/50], Step [30/100], Train Loss: 1.0388\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2230 batches: 1.039\n",
      "Epoch [23/50], Step [40/100], Train Loss: 1.0526\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2240 batches: 1.053\n",
      "Epoch [23/50], Step [50/100], Train Loss: 1.0389\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2250 batches: 1.039\n",
      "Epoch [23/50], Step [60/100], Train Loss: 1.0369\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2260 batches: 1.037\n",
      "Epoch [23/50], Step [70/100], Train Loss: 1.0424\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2270 batches: 1.042\n",
      "Epoch [23/50], Step [80/100], Train Loss: 1.0485\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2280 batches: 1.049\n",
      "Epoch [23/50], Step [90/100], Train Loss: 1.0404\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2290 batches: 1.040\n",
      "Epoch [23/50], Step [100/100], Train Loss: 1.0452\n",
      "learning_rate:  2.800376120856161e-05\n",
      "Train Loss after 2300 batches: 1.045\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8820\n",
      "         1.0       0.41      0.70      0.51      1080\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.484850%\n",
      "\n",
      "Epoch [24/50], Step [10/100], Train Loss: 1.0435\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2310 batches: 1.044\n",
      "Epoch [24/50], Step [20/100], Train Loss: 1.0338\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2320 batches: 1.034\n",
      "Epoch [24/50], Step [30/100], Train Loss: 1.0377\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2330 batches: 1.038\n",
      "Epoch [24/50], Step [40/100], Train Loss: 1.0310\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2340 batches: 1.031\n",
      "Epoch [24/50], Step [50/100], Train Loss: 1.0328\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2350 batches: 1.033\n",
      "Epoch [24/50], Step [60/100], Train Loss: 1.0498\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2360 batches: 1.050\n",
      "Epoch [24/50], Step [70/100], Train Loss: 1.0575\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2370 batches: 1.057\n",
      "Epoch [24/50], Step [80/100], Train Loss: 1.0406\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2380 batches: 1.041\n",
      "Epoch [24/50], Step [90/100], Train Loss: 1.0701\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2390 batches: 1.070\n",
      "Epoch [24/50], Step [100/100], Train Loss: 1.0407\n",
      "learning_rate:  2.380319702727737e-05\n",
      "Train Loss after 2400 batches: 1.041\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8811\n",
      "         1.0       0.40      0.71      0.51      1089\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.151517%\n",
      "\n",
      "Epoch [25/50], Step [10/100], Train Loss: 1.0354\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2410 batches: 1.035\n",
      "Epoch [25/50], Step [20/100], Train Loss: 1.0556\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2420 batches: 1.056\n",
      "Epoch [25/50], Step [30/100], Train Loss: 1.0566\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2430 batches: 1.057\n",
      "Epoch [25/50], Step [40/100], Train Loss: 1.0443\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2440 batches: 1.044\n",
      "Epoch [25/50], Step [50/100], Train Loss: 1.0244\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2450 batches: 1.024\n",
      "Epoch [25/50], Step [60/100], Train Loss: 1.0403\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2460 batches: 1.040\n",
      "Epoch [25/50], Step [70/100], Train Loss: 1.0369\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2470 batches: 1.037\n",
      "Epoch [25/50], Step [80/100], Train Loss: 1.0355\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2480 batches: 1.036\n",
      "Epoch [25/50], Step [90/100], Train Loss: 1.0524\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2490 batches: 1.052\n",
      "Epoch [25/50], Step [100/100], Train Loss: 1.0504\n",
      "learning_rate:  2.0232717473185764e-05\n",
      "Train Loss after 2500 batches: 1.050\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.92      8819\n",
      "         1.0       0.41      0.70      0.52      1081\n",
      "\n",
      "    accuracy                           0.86      9900\n",
      "   macro avg       0.68      0.79      0.72      9900\n",
      "weighted avg       0.90      0.86      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.555559%\n",
      "\n",
      "Epoch [26/50], Step [10/100], Train Loss: 1.0540\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2510 batches: 1.054\n",
      "Epoch [26/50], Step [20/100], Train Loss: 1.0318\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2520 batches: 1.032\n",
      "Epoch [26/50], Step [30/100], Train Loss: 1.0592\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2530 batches: 1.059\n",
      "Epoch [26/50], Step [40/100], Train Loss: 1.0380\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2540 batches: 1.038\n",
      "Epoch [26/50], Step [50/100], Train Loss: 1.0364\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2550 batches: 1.036\n",
      "Epoch [26/50], Step [60/100], Train Loss: 1.0377\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2560 batches: 1.038\n",
      "Epoch [26/50], Step [70/100], Train Loss: 1.0425\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2570 batches: 1.043\n",
      "Epoch [26/50], Step [80/100], Train Loss: 1.0502\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2580 batches: 1.050\n",
      "Epoch [26/50], Step [90/100], Train Loss: 1.0478\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2590 batches: 1.048\n",
      "Epoch [26/50], Step [100/100], Train Loss: 1.0337\n",
      "learning_rate:  1.71978098522079e-05\n",
      "Train Loss after 2600 batches: 1.034\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8812\n",
      "         1.0       0.40      0.71      0.51      1088\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.222226%\n",
      "\n",
      "Epoch [27/50], Step [10/100], Train Loss: 1.0253\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2610 batches: 1.025\n",
      "Epoch [27/50], Step [20/100], Train Loss: 1.0539\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2620 batches: 1.054\n",
      "Epoch [27/50], Step [30/100], Train Loss: 1.0419\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2630 batches: 1.042\n",
      "Epoch [27/50], Step [40/100], Train Loss: 1.0402\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2640 batches: 1.040\n",
      "Epoch [27/50], Step [50/100], Train Loss: 1.0492\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2650 batches: 1.049\n",
      "Epoch [27/50], Step [60/100], Train Loss: 1.0607\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2660 batches: 1.061\n",
      "Epoch [27/50], Step [70/100], Train Loss: 1.0326\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2670 batches: 1.033\n",
      "Epoch [27/50], Step [80/100], Train Loss: 1.0556\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2680 batches: 1.056\n",
      "Epoch [27/50], Step [90/100], Train Loss: 1.0339\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2690 batches: 1.034\n",
      "Epoch [27/50], Step [100/100], Train Loss: 1.0448\n",
      "learning_rate:  1.4618138374376715e-05\n",
      "Train Loss after 2700 batches: 1.045\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8817\n",
      "         1.0       0.40      0.71      0.51      1083\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.202020%\n",
      "\n",
      "Epoch [28/50], Step [10/100], Train Loss: 1.0499\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2710 batches: 1.050\n",
      "Epoch [28/50], Step [20/100], Train Loss: 1.0418\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2720 batches: 1.042\n",
      "Epoch [28/50], Step [30/100], Train Loss: 1.0462\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2730 batches: 1.046\n",
      "Epoch [28/50], Step [40/100], Train Loss: 1.0295\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2740 batches: 1.030\n",
      "Epoch [28/50], Step [50/100], Train Loss: 1.0426\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2750 batches: 1.043\n",
      "Epoch [28/50], Step [60/100], Train Loss: 1.0457\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2760 batches: 1.046\n",
      "Epoch [28/50], Step [70/100], Train Loss: 1.0525\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2770 batches: 1.052\n",
      "Epoch [28/50], Step [80/100], Train Loss: 1.0494\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2780 batches: 1.049\n",
      "Epoch [28/50], Step [90/100], Train Loss: 1.0390\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2790 batches: 1.039\n",
      "Epoch [28/50], Step [100/100], Train Loss: 1.0284\n",
      "learning_rate:  1.2425417618220208e-05\n",
      "Train Loss after 2800 batches: 1.028\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8821\n",
      "         1.0       0.40      0.71      0.51      1079\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.262626%\n",
      "\n",
      "Epoch [29/50], Step [10/100], Train Loss: 1.0416\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2810 batches: 1.042\n",
      "Epoch [29/50], Step [20/100], Train Loss: 1.0430\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2820 batches: 1.043\n",
      "Epoch [29/50], Step [30/100], Train Loss: 1.0306\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2830 batches: 1.031\n",
      "Epoch [29/50], Step [40/100], Train Loss: 1.0412\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2840 batches: 1.041\n",
      "Epoch [29/50], Step [50/100], Train Loss: 1.0544\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2850 batches: 1.054\n",
      "Epoch [29/50], Step [60/100], Train Loss: 1.0414\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2860 batches: 1.041\n",
      "Epoch [29/50], Step [70/100], Train Loss: 1.0506\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2870 batches: 1.051\n",
      "Epoch [29/50], Step [80/100], Train Loss: 1.0341\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2880 batches: 1.034\n",
      "Epoch [29/50], Step [90/100], Train Loss: 1.0352\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2890 batches: 1.035\n",
      "Epoch [29/50], Step [100/100], Train Loss: 1.0572\n",
      "learning_rate:  1.0561604975487176e-05\n",
      "Train Loss after 2900 batches: 1.057\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8816\n",
      "         1.0       0.40      0.70      0.51      1084\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.101014%\n",
      "\n",
      "Epoch [30/50], Step [10/100], Train Loss: 1.0441\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 2910 batches: 1.044\n",
      "Epoch [30/50], Step [20/100], Train Loss: 1.0436\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 2920 batches: 1.044\n",
      "Epoch [30/50], Step [30/100], Train Loss: 1.0386\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 2930 batches: 1.039\n",
      "Epoch [30/50], Step [40/100], Train Loss: 1.0308\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 2940 batches: 1.031\n",
      "Epoch [30/50], Step [50/100], Train Loss: 1.0635\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 2950 batches: 1.063\n",
      "Epoch [30/50], Step [60/100], Train Loss: 1.0440\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 2960 batches: 1.044\n",
      "Epoch [30/50], Step [70/100], Train Loss: 1.0181\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 2970 batches: 1.018\n",
      "Epoch [30/50], Step [80/100], Train Loss: 1.0602\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 2980 batches: 1.060\n",
      "Epoch [30/50], Step [90/100], Train Loss: 1.0362\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 2990 batches: 1.036\n",
      "Epoch [30/50], Step [100/100], Train Loss: 1.0555\n",
      "learning_rate:  8.9773642291641e-06\n",
      "Train Loss after 3000 batches: 1.056\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8824\n",
      "         1.0       0.40      0.70      0.51      1076\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.141414%\n",
      "\n",
      "Epoch [31/50], Step [10/100], Train Loss: 1.0486\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3010 batches: 1.049\n",
      "Epoch [31/50], Step [20/100], Train Loss: 1.0435\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3020 batches: 1.043\n",
      "Epoch [31/50], Step [30/100], Train Loss: 1.0256\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3030 batches: 1.026\n",
      "Epoch [31/50], Step [40/100], Train Loss: 1.0636\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3040 batches: 1.064\n",
      "Epoch [31/50], Step [50/100], Train Loss: 1.0494\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3050 batches: 1.049\n",
      "Epoch [31/50], Step [60/100], Train Loss: 1.0372\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3060 batches: 1.037\n",
      "Epoch [31/50], Step [70/100], Train Loss: 1.0320\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3070 batches: 1.032\n",
      "Epoch [31/50], Step [80/100], Train Loss: 1.0475\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3080 batches: 1.047\n",
      "Epoch [31/50], Step [90/100], Train Loss: 1.0437\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3090 batches: 1.044\n",
      "Epoch [31/50], Step [100/100], Train Loss: 1.0386\n",
      "learning_rate:  7.630759594789484e-06\n",
      "Train Loss after 3100 batches: 1.039\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8821\n",
      "         1.0       0.40      0.70      0.51      1079\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.242426%\n",
      "\n",
      "Epoch [32/50], Step [10/100], Train Loss: 1.0521\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3110 batches: 1.052\n",
      "Epoch [32/50], Step [20/100], Train Loss: 1.0592\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3120 batches: 1.059\n",
      "Epoch [32/50], Step [30/100], Train Loss: 1.0356\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3130 batches: 1.036\n",
      "Epoch [32/50], Step [40/100], Train Loss: 1.0397\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3140 batches: 1.040\n",
      "Epoch [32/50], Step [50/100], Train Loss: 1.0337\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3150 batches: 1.034\n",
      "Epoch [32/50], Step [60/100], Train Loss: 1.0354\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3160 batches: 1.035\n",
      "Epoch [32/50], Step [70/100], Train Loss: 1.0664\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3170 batches: 1.066\n",
      "Epoch [32/50], Step [80/100], Train Loss: 1.0402\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3180 batches: 1.040\n",
      "Epoch [32/50], Step [90/100], Train Loss: 1.0374\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3190 batches: 1.037\n",
      "Epoch [32/50], Step [100/100], Train Loss: 1.0269\n",
      "learning_rate:  6.486145655571062e-06\n",
      "Train Loss after 3200 batches: 1.027\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8808\n",
      "         1.0       0.40      0.71      0.51      1092\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.060608%\n",
      "\n",
      "Epoch [33/50], Step [10/100], Train Loss: 1.0472\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3210 batches: 1.047\n",
      "Epoch [33/50], Step [20/100], Train Loss: 1.0434\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3220 batches: 1.043\n",
      "Epoch [33/50], Step [30/100], Train Loss: 1.0501\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3230 batches: 1.050\n",
      "Epoch [33/50], Step [40/100], Train Loss: 1.0611\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3240 batches: 1.061\n",
      "Epoch [33/50], Step [50/100], Train Loss: 1.0330\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3250 batches: 1.033\n",
      "Epoch [33/50], Step [60/100], Train Loss: 1.0358\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3260 batches: 1.036\n",
      "Epoch [33/50], Step [70/100], Train Loss: 1.0520\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3270 batches: 1.052\n",
      "Epoch [33/50], Step [80/100], Train Loss: 1.0364\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3280 batches: 1.036\n",
      "Epoch [33/50], Step [90/100], Train Loss: 1.0395\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3290 batches: 1.040\n",
      "Epoch [33/50], Step [100/100], Train Loss: 1.0317\n",
      "learning_rate:  5.513223807235402e-06\n",
      "Train Loss after 3300 batches: 1.032\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8819\n",
      "         1.0       0.40      0.71      0.51      1081\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.090911%\n",
      "\n",
      "Epoch [34/50], Step [10/100], Train Loss: 1.0334\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3310 batches: 1.033\n",
      "Epoch [34/50], Step [20/100], Train Loss: 1.0389\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3320 batches: 1.039\n",
      "Epoch [34/50], Step [30/100], Train Loss: 1.0612\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3330 batches: 1.061\n",
      "Epoch [34/50], Step [40/100], Train Loss: 1.0386\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3340 batches: 1.039\n",
      "Epoch [34/50], Step [50/100], Train Loss: 1.0359\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3350 batches: 1.036\n",
      "Epoch [34/50], Step [60/100], Train Loss: 1.0485\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3360 batches: 1.048\n",
      "Epoch [34/50], Step [70/100], Train Loss: 1.0488\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3370 batches: 1.049\n",
      "Epoch [34/50], Step [80/100], Train Loss: 1.0415\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3380 batches: 1.042\n",
      "Epoch [34/50], Step [90/100], Train Loss: 1.0471\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3390 batches: 1.047\n",
      "Epoch [34/50], Step [100/100], Train Loss: 1.0337\n",
      "learning_rate:  4.686240236150092e-06\n",
      "Train Loss after 3400 batches: 1.034\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8827\n",
      "         1.0       0.40      0.71      0.51      1073\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.111111%\n",
      "\n",
      "Epoch [35/50], Step [10/100], Train Loss: 1.0458\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3410 batches: 1.046\n",
      "Epoch [35/50], Step [20/100], Train Loss: 1.0524\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3420 batches: 1.052\n",
      "Epoch [35/50], Step [30/100], Train Loss: 1.0532\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3430 batches: 1.053\n",
      "Epoch [35/50], Step [40/100], Train Loss: 1.0339\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3440 batches: 1.034\n",
      "Epoch [35/50], Step [50/100], Train Loss: 1.0532\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3450 batches: 1.053\n",
      "Epoch [35/50], Step [60/100], Train Loss: 1.0237\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3460 batches: 1.024\n",
      "Epoch [35/50], Step [70/100], Train Loss: 1.0389\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3470 batches: 1.039\n",
      "Epoch [35/50], Step [80/100], Train Loss: 1.0481\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3480 batches: 1.048\n",
      "Epoch [35/50], Step [90/100], Train Loss: 1.0545\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3490 batches: 1.055\n",
      "Epoch [35/50], Step [100/100], Train Loss: 1.0262\n",
      "learning_rate:  3.983304200727578e-06\n",
      "Train Loss after 3500 batches: 1.026\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8811\n",
      "         1.0       0.40      0.71      0.51      1089\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.181820%\n",
      "\n",
      "Epoch [36/50], Step [10/100], Train Loss: 1.0454\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3510 batches: 1.045\n",
      "Epoch [36/50], Step [20/100], Train Loss: 1.0405\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3520 batches: 1.041\n",
      "Epoch [36/50], Step [30/100], Train Loss: 1.0410\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3530 batches: 1.041\n",
      "Epoch [36/50], Step [40/100], Train Loss: 1.0460\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3540 batches: 1.046\n",
      "Epoch [36/50], Step [50/100], Train Loss: 1.0394\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3550 batches: 1.039\n",
      "Epoch [36/50], Step [60/100], Train Loss: 1.0328\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3560 batches: 1.033\n",
      "Epoch [36/50], Step [70/100], Train Loss: 1.0376\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3570 batches: 1.038\n",
      "Epoch [36/50], Step [80/100], Train Loss: 1.0502\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3580 batches: 1.050\n",
      "Epoch [36/50], Step [90/100], Train Loss: 1.0601\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3590 batches: 1.060\n",
      "Epoch [36/50], Step [100/100], Train Loss: 1.0394\n",
      "learning_rate:  3.3858085706184414e-06\n",
      "Train Loss after 3600 batches: 1.039\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8816\n",
      "         1.0       0.40      0.71      0.51      1084\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.262626%\n",
      "\n",
      "Epoch [37/50], Step [10/100], Train Loss: 1.0450\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3610 batches: 1.045\n",
      "Epoch [37/50], Step [20/100], Train Loss: 1.0450\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3620 batches: 1.045\n",
      "Epoch [37/50], Step [30/100], Train Loss: 1.0470\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3630 batches: 1.047\n",
      "Epoch [37/50], Step [40/100], Train Loss: 1.0372\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3640 batches: 1.037\n",
      "Epoch [37/50], Step [50/100], Train Loss: 1.0437\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3650 batches: 1.044\n",
      "Epoch [37/50], Step [60/100], Train Loss: 1.0573\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3660 batches: 1.057\n",
      "Epoch [37/50], Step [70/100], Train Loss: 1.0361\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3670 batches: 1.036\n",
      "Epoch [37/50], Step [80/100], Train Loss: 1.0309\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3680 batches: 1.031\n",
      "Epoch [37/50], Step [90/100], Train Loss: 1.0477\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3690 batches: 1.048\n",
      "Epoch [37/50], Step [100/100], Train Loss: 1.0474\n",
      "learning_rate:  2.8779372850256753e-06\n",
      "Train Loss after 3700 batches: 1.047\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8816\n",
      "         1.0       0.40      0.70      0.51      1084\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.272729%\n",
      "\n",
      "Epoch [38/50], Step [10/100], Train Loss: 1.0427\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3710 batches: 1.043\n",
      "Epoch [38/50], Step [20/100], Train Loss: 1.0468\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3720 batches: 1.047\n",
      "Epoch [38/50], Step [30/100], Train Loss: 1.0538\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3730 batches: 1.054\n",
      "Epoch [38/50], Step [40/100], Train Loss: 1.0417\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3740 batches: 1.042\n",
      "Epoch [38/50], Step [50/100], Train Loss: 1.0391\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3750 batches: 1.039\n",
      "Epoch [38/50], Step [60/100], Train Loss: 1.0345\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3760 batches: 1.035\n",
      "Epoch [38/50], Step [70/100], Train Loss: 1.0397\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3770 batches: 1.040\n",
      "Epoch [38/50], Step [80/100], Train Loss: 1.0488\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3780 batches: 1.049\n",
      "Epoch [38/50], Step [90/100], Train Loss: 1.0415\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3790 batches: 1.041\n",
      "Epoch [38/50], Step [100/100], Train Loss: 1.0395\n",
      "learning_rate:  2.446246692271824e-06\n",
      "Train Loss after 3800 batches: 1.039\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8809\n",
      "         1.0       0.40      0.70      0.51      1091\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.242426%\n",
      "\n",
      "Epoch [39/50], Step [10/100], Train Loss: 1.0543\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3810 batches: 1.054\n",
      "Epoch [39/50], Step [20/100], Train Loss: 1.0507\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3820 batches: 1.051\n",
      "Epoch [39/50], Step [30/100], Train Loss: 1.0231\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3830 batches: 1.023\n",
      "Epoch [39/50], Step [40/100], Train Loss: 1.0367\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3840 batches: 1.037\n",
      "Epoch [39/50], Step [50/100], Train Loss: 1.0313\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3850 batches: 1.031\n",
      "Epoch [39/50], Step [60/100], Train Loss: 1.0496\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3860 batches: 1.050\n",
      "Epoch [39/50], Step [70/100], Train Loss: 1.0509\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3870 batches: 1.051\n",
      "Epoch [39/50], Step [80/100], Train Loss: 1.0475\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3880 batches: 1.048\n",
      "Epoch [39/50], Step [90/100], Train Loss: 1.0561\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3890 batches: 1.056\n",
      "Epoch [39/50], Step [100/100], Train Loss: 1.0356\n",
      "learning_rate:  2.07930968843105e-06\n",
      "Train Loss after 3900 batches: 1.036\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8822\n",
      "         1.0       0.40      0.70      0.51      1078\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.282832%\n",
      "\n",
      "Epoch [40/50], Step [10/100], Train Loss: 1.0690\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 3910 batches: 1.069\n",
      "Epoch [40/50], Step [20/100], Train Loss: 1.0469\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 3920 batches: 1.047\n",
      "Epoch [40/50], Step [30/100], Train Loss: 1.0489\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 3930 batches: 1.049\n",
      "Epoch [40/50], Step [40/100], Train Loss: 1.0344\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 3940 batches: 1.034\n",
      "Epoch [40/50], Step [50/100], Train Loss: 1.0307\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 3950 batches: 1.031\n",
      "Epoch [40/50], Step [60/100], Train Loss: 1.0437\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 3960 batches: 1.044\n",
      "Epoch [40/50], Step [70/100], Train Loss: 1.0329\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 3970 batches: 1.033\n",
      "Epoch [40/50], Step [80/100], Train Loss: 1.0511\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 3980 batches: 1.051\n",
      "Epoch [40/50], Step [90/100], Train Loss: 1.0394\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 3990 batches: 1.039\n",
      "Epoch [40/50], Step [100/100], Train Loss: 1.0370\n",
      "learning_rate:  1.7674132351663925e-06\n",
      "Train Loss after 4000 batches: 1.037\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8817\n",
      "         1.0       0.40      0.71      0.51      1083\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.181820%\n",
      "\n",
      "Epoch [41/50], Step [10/100], Train Loss: 1.0335\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4010 batches: 1.034\n",
      "Epoch [41/50], Step [20/100], Train Loss: 1.0323\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4020 batches: 1.032\n",
      "Epoch [41/50], Step [30/100], Train Loss: 1.0524\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4030 batches: 1.052\n",
      "Epoch [41/50], Step [40/100], Train Loss: 1.0381\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4040 batches: 1.038\n",
      "Epoch [41/50], Step [50/100], Train Loss: 1.0362\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4050 batches: 1.036\n",
      "Epoch [41/50], Step [60/100], Train Loss: 1.0542\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4060 batches: 1.054\n",
      "Epoch [41/50], Step [70/100], Train Loss: 1.0451\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4070 batches: 1.045\n",
      "Epoch [41/50], Step [80/100], Train Loss: 1.0488\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4080 batches: 1.049\n",
      "Epoch [41/50], Step [90/100], Train Loss: 1.0541\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4090 batches: 1.054\n",
      "Epoch [41/50], Step [100/100], Train Loss: 1.0332\n",
      "learning_rate:  1.5023012498914335e-06\n",
      "Train Loss after 4100 batches: 1.033\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8816\n",
      "         1.0       0.40      0.70      0.51      1084\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.242426%\n",
      "\n",
      "Epoch [42/50], Step [10/100], Train Loss: 1.0441\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4110 batches: 1.044\n",
      "Epoch [42/50], Step [20/100], Train Loss: 1.0450\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4120 batches: 1.045\n",
      "Epoch [42/50], Step [30/100], Train Loss: 1.0484\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4130 batches: 1.048\n",
      "Epoch [42/50], Step [40/100], Train Loss: 1.0430\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4140 batches: 1.043\n",
      "Epoch [42/50], Step [50/100], Train Loss: 1.0353\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4150 batches: 1.035\n",
      "Epoch [42/50], Step [60/100], Train Loss: 1.0406\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4160 batches: 1.041\n",
      "Epoch [42/50], Step [70/100], Train Loss: 1.0334\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4170 batches: 1.033\n",
      "Epoch [42/50], Step [80/100], Train Loss: 1.0534\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4180 batches: 1.053\n",
      "Epoch [42/50], Step [90/100], Train Loss: 1.0402\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4190 batches: 1.040\n",
      "Epoch [42/50], Step [100/100], Train Loss: 1.0412\n",
      "learning_rate:  1.2769560624077185e-06\n",
      "Train Loss after 4200 batches: 1.041\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8824\n",
      "         1.0       0.40      0.70      0.51      1076\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.141414%\n",
      "\n",
      "Epoch [43/50], Step [10/100], Train Loss: 1.0483\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4210 batches: 1.048\n",
      "Epoch [43/50], Step [20/100], Train Loss: 1.0509\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4220 batches: 1.051\n",
      "Epoch [43/50], Step [30/100], Train Loss: 1.0711\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4230 batches: 1.071\n",
      "Epoch [43/50], Step [40/100], Train Loss: 1.0291\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4240 batches: 1.029\n",
      "Epoch [43/50], Step [50/100], Train Loss: 1.0427\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4250 batches: 1.043\n",
      "Epoch [43/50], Step [60/100], Train Loss: 1.0295\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4260 batches: 1.029\n",
      "Epoch [43/50], Step [70/100], Train Loss: 1.0431\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4270 batches: 1.043\n",
      "Epoch [43/50], Step [80/100], Train Loss: 1.0369\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4280 batches: 1.037\n",
      "Epoch [43/50], Step [90/100], Train Loss: 1.0408\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4290 batches: 1.041\n",
      "Epoch [43/50], Step [100/100], Train Loss: 1.0333\n",
      "learning_rate:  1.0854126530465608e-06\n",
      "Train Loss after 4300 batches: 1.033\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8817\n",
      "         1.0       0.40      0.70      0.51      1083\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.191923%\n",
      "\n",
      "Epoch [44/50], Step [10/100], Train Loss: 1.0325\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4310 batches: 1.032\n",
      "Epoch [44/50], Step [20/100], Train Loss: 1.0527\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4320 batches: 1.053\n",
      "Epoch [44/50], Step [30/100], Train Loss: 1.0372\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4330 batches: 1.037\n",
      "Epoch [44/50], Step [40/100], Train Loss: 1.0426\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4340 batches: 1.043\n",
      "Epoch [44/50], Step [50/100], Train Loss: 1.0441\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4350 batches: 1.044\n",
      "Epoch [44/50], Step [60/100], Train Loss: 1.0588\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4360 batches: 1.059\n",
      "Epoch [44/50], Step [70/100], Train Loss: 1.0388\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4370 batches: 1.039\n",
      "Epoch [44/50], Step [80/100], Train Loss: 1.0346\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4380 batches: 1.035\n",
      "Epoch [44/50], Step [90/100], Train Loss: 1.0428\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4390 batches: 1.043\n",
      "Epoch [44/50], Step [100/100], Train Loss: 1.0431\n",
      "learning_rate:  9.226007550895766e-07\n",
      "Train Loss after 4400 batches: 1.043\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8813\n",
      "         1.0       0.40      0.71      0.51      1087\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.181820%\n",
      "\n",
      "Epoch [45/50], Step [10/100], Train Loss: 1.0553\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4410 batches: 1.055\n",
      "Epoch [45/50], Step [20/100], Train Loss: 1.0575\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4420 batches: 1.057\n",
      "Epoch [45/50], Step [30/100], Train Loss: 1.0484\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4430 batches: 1.048\n",
      "Epoch [45/50], Step [40/100], Train Loss: 1.0364\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4440 batches: 1.036\n",
      "Epoch [45/50], Step [50/100], Train Loss: 1.0338\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4450 batches: 1.034\n",
      "Epoch [45/50], Step [60/100], Train Loss: 1.0361\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4460 batches: 1.036\n",
      "Epoch [45/50], Step [70/100], Train Loss: 1.0356\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4470 batches: 1.036\n",
      "Epoch [45/50], Step [80/100], Train Loss: 1.0430\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4480 batches: 1.043\n",
      "Epoch [45/50], Step [90/100], Train Loss: 1.0384\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4490 batches: 1.038\n",
      "Epoch [45/50], Step [100/100], Train Loss: 1.0513\n",
      "learning_rate:  7.842106418261401e-07\n",
      "Train Loss after 4500 batches: 1.051\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8818\n",
      "         1.0       0.40      0.71      0.51      1082\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.242426%\n",
      "\n",
      "Epoch [46/50], Step [10/100], Train Loss: 1.0468\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4510 batches: 1.047\n",
      "Epoch [46/50], Step [20/100], Train Loss: 1.0392\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4520 batches: 1.039\n",
      "Epoch [46/50], Step [30/100], Train Loss: 1.0403\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4530 batches: 1.040\n",
      "Epoch [46/50], Step [40/100], Train Loss: 1.0499\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4540 batches: 1.050\n",
      "Epoch [46/50], Step [50/100], Train Loss: 1.0457\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4550 batches: 1.046\n",
      "Epoch [46/50], Step [60/100], Train Loss: 1.0552\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4560 batches: 1.055\n",
      "Epoch [46/50], Step [70/100], Train Loss: 1.0500\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4570 batches: 1.050\n",
      "Epoch [46/50], Step [80/100], Train Loss: 1.0273\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4580 batches: 1.027\n",
      "Epoch [46/50], Step [90/100], Train Loss: 1.0316\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4590 batches: 1.032\n",
      "Epoch [46/50], Step [100/100], Train Loss: 1.0416\n",
      "learning_rate:  6.665790455522191e-07\n",
      "Train Loss after 4600 batches: 1.042\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8820\n",
      "         1.0       0.40      0.70      0.51      1080\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.262626%\n",
      "\n",
      "Epoch [47/50], Step [10/100], Train Loss: 1.0458\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4610 batches: 1.046\n",
      "Epoch [47/50], Step [20/100], Train Loss: 1.0433\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4620 batches: 1.043\n",
      "Epoch [47/50], Step [30/100], Train Loss: 1.0286\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4630 batches: 1.029\n",
      "Epoch [47/50], Step [40/100], Train Loss: 1.0405\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4640 batches: 1.040\n",
      "Epoch [47/50], Step [50/100], Train Loss: 1.0364\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4650 batches: 1.036\n",
      "Epoch [47/50], Step [60/100], Train Loss: 1.0492\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4660 batches: 1.049\n",
      "Epoch [47/50], Step [70/100], Train Loss: 1.0374\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4670 batches: 1.037\n",
      "Epoch [47/50], Step [80/100], Train Loss: 1.0277\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4680 batches: 1.028\n",
      "Epoch [47/50], Step [90/100], Train Loss: 1.0487\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4690 batches: 1.049\n",
      "Epoch [47/50], Step [100/100], Train Loss: 1.0605\n",
      "learning_rate:  5.665921887193862e-07\n",
      "Train Loss after 4700 batches: 1.061\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8812\n",
      "         1.0       0.40      0.70      0.51      1088\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.181820%\n",
      "\n",
      "Epoch [48/50], Step [10/100], Train Loss: 1.0504\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4710 batches: 1.050\n",
      "Epoch [48/50], Step [20/100], Train Loss: 1.0420\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4720 batches: 1.042\n",
      "Epoch [48/50], Step [30/100], Train Loss: 1.0332\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4730 batches: 1.033\n",
      "Epoch [48/50], Step [40/100], Train Loss: 1.0429\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4740 batches: 1.043\n",
      "Epoch [48/50], Step [50/100], Train Loss: 1.0301\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4750 batches: 1.030\n",
      "Epoch [48/50], Step [60/100], Train Loss: 1.0566\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4760 batches: 1.057\n",
      "Epoch [48/50], Step [70/100], Train Loss: 1.0329\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4770 batches: 1.033\n",
      "Epoch [48/50], Step [80/100], Train Loss: 1.0542\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4780 batches: 1.054\n",
      "Epoch [48/50], Step [90/100], Train Loss: 1.0409\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4790 batches: 1.041\n",
      "Epoch [48/50], Step [100/100], Train Loss: 1.0492\n",
      "learning_rate:  4.816033604114782e-07\n",
      "Train Loss after 4800 batches: 1.049\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8812\n",
      "         1.0       0.40      0.70      0.51      1088\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.262626%\n",
      "\n",
      "Epoch [49/50], Step [10/100], Train Loss: 1.0371\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4810 batches: 1.037\n",
      "Epoch [49/50], Step [20/100], Train Loss: 1.0387\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4820 batches: 1.039\n",
      "Epoch [49/50], Step [30/100], Train Loss: 1.0363\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4830 batches: 1.036\n",
      "Epoch [49/50], Step [40/100], Train Loss: 1.0382\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4840 batches: 1.038\n",
      "Epoch [49/50], Step [50/100], Train Loss: 1.0642\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4850 batches: 1.064\n",
      "Epoch [49/50], Step [60/100], Train Loss: 1.0401\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4860 batches: 1.040\n",
      "Epoch [49/50], Step [70/100], Train Loss: 1.0519\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4870 batches: 1.052\n",
      "Epoch [49/50], Step [80/100], Train Loss: 1.0398\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4880 batches: 1.040\n",
      "Epoch [49/50], Step [90/100], Train Loss: 1.0362\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4890 batches: 1.036\n",
      "Epoch [49/50], Step [100/100], Train Loss: 1.0418\n",
      "learning_rate:  4.0936285634975647e-07\n",
      "Train Loss after 4900 batches: 1.042\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8813\n",
      "         1.0       0.40      0.71      0.51      1087\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.222226%\n",
      "\n",
      "Epoch [50/50], Step [10/100], Train Loss: 1.0441\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 4910 batches: 1.044\n",
      "Epoch [50/50], Step [20/100], Train Loss: 1.0368\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 4920 batches: 1.037\n",
      "Epoch [50/50], Step [30/100], Train Loss: 1.0419\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 4930 batches: 1.042\n",
      "Epoch [50/50], Step [40/100], Train Loss: 1.0555\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 4940 batches: 1.056\n",
      "Epoch [50/50], Step [50/100], Train Loss: 1.0374\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 4950 batches: 1.037\n",
      "Epoch [50/50], Step [60/100], Train Loss: 1.0427\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 4960 batches: 1.043\n",
      "Epoch [50/50], Step [70/100], Train Loss: 1.0499\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 4970 batches: 1.050\n",
      "Epoch [50/50], Step [80/100], Train Loss: 1.0333\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 4980 batches: 1.033\n",
      "Epoch [50/50], Step [90/100], Train Loss: 1.0441\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 4990 batches: 1.044\n",
      "Epoch [50/50], Step [100/100], Train Loss: 1.0475\n",
      "learning_rate:  3.47958427897293e-07\n",
      "Train Loss after 5000 batches: 1.047\n",
      "\n",
      " Validation:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91      8814\n",
      "         1.0       0.40      0.71      0.51      1086\n",
      "\n",
      "    accuracy                           0.85      9900\n",
      "   macro avg       0.68      0.79      0.71      9900\n",
      "weighted avg       0.90      0.85      0.87      9900\n",
      "\n",
      "Validation Accuracy: 85.262626%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train(model, train_loader, val_loader, criterion, optimizer, scheduler, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on 10800 transactions in test data: 84.750000%\n",
      "f1_score_test:  0.48547328959700087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.86      0.91      9687\n",
      "         1.0       0.37      0.70      0.49      1113\n",
      "\n",
      "    accuracy                           0.85     10800\n",
      "   macro avg       0.67      0.78      0.70     10800\n",
      "weighted avg       0.90      0.85      0.87     10800\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model -> no need to compute gradients (for memory efficiency)\n",
    "test(model, test_loader, device, config.project_name, save_model = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_embedding.weight tensor([[ 0.2817,  2.1020, -0.2418,  ..., -0.7328,  0.3054, -0.4162],\n",
      "        [ 1.1522, -1.1712,  0.5084,  ..., -1.1771, -0.2050, -0.6818],\n",
      "        [ 0.2275,  0.9952, -0.0166,  ..., -0.1564, -0.4006, -1.3212],\n",
      "        ...,\n",
      "        [ 2.0941, -0.8441, -1.7690,  ...,  2.4999,  0.3088, -0.4117],\n",
      "        [-0.0197, -1.3732, -0.8576,  ...,  1.0638, -1.9571,  0.9633],\n",
      "        [-1.0167, -2.0919, -0.2583,  ..., -0.3903, -0.9911, -0.5274]],\n",
      "       device='cuda:0')\n",
      "position_embedding.weight tensor([[ 1.8930,  1.4601,  0.8983, -2.0662,  0.6538, -1.2094, -0.0223, -1.5702,\n",
      "         -0.7280,  1.6065, -0.3768, -1.3739, -0.7004, -0.5274, -0.7234,  0.7738,\n",
      "          1.6030, -0.1514, -0.5070,  0.4295, -0.7706,  1.0571,  0.7990,  1.6420,\n",
      "          1.2504,  1.2738,  0.5892,  1.2978, -0.2440,  0.0205, -0.2529,  0.8407,\n",
      "         -1.3666, -0.8501, -0.2210,  1.6649,  0.3118, -0.3961,  0.3285, -0.7691,\n",
      "         -1.5416,  0.9680, -0.8495, -0.6127, -1.2355,  2.0922, -1.1920, -0.5014,\n",
      "         -0.9168, -0.6242,  0.0720,  0.4923, -0.4685,  1.1820, -0.8040, -0.7029,\n",
      "         -1.3961,  0.0387, -0.0426,  0.6505, -0.0767,  1.7942, -1.1347,  1.3691],\n",
      "        [ 1.4639,  0.8318,  2.1674,  0.4977,  0.3515, -0.2105, -1.0579,  1.2507,\n",
      "         -0.1481,  0.5606,  0.0142,  0.4512,  0.5927, -0.6521, -2.1579, -0.6880,\n",
      "          0.0224, -0.3256, -1.3223, -0.5808,  0.5254,  0.5013,  1.0961,  0.0861,\n",
      "          0.7052, -0.4135, -0.9984,  0.5765, -1.7294, -0.8024,  1.2467,  0.4927,\n",
      "         -2.5021,  0.4856,  0.8048,  0.0479,  0.6375,  0.5876,  1.0085, -0.4523,\n",
      "         -0.2130,  0.7217,  0.3814,  0.1901,  0.2744,  1.2470,  0.0168, -0.2927,\n",
      "         -1.4019, -0.1212, -0.5961,  0.4645,  0.6687,  0.1160, -0.3863,  0.5252,\n",
      "         -0.0131,  0.2055,  0.1366,  0.7768,  1.0991,  0.3532,  0.7247,  0.3982],\n",
      "        [ 1.9337,  1.0087, -1.4225, -1.1227, -0.1271,  1.5919,  0.6385,  0.5437,\n",
      "          1.0917,  0.0144, -1.8157,  0.9221, -0.3205,  0.9824, -0.6651,  0.6728,\n",
      "         -0.9200,  0.8905,  1.5506,  1.4246,  0.2696, -0.1954, -0.7101,  0.1268,\n",
      "          0.2896,  0.9702, -0.4529,  1.5742, -2.4839, -0.4329, -1.2107,  0.8040,\n",
      "         -1.8592,  0.2037,  0.0327, -0.3481,  0.2725, -0.6949,  0.1833, -1.0861,\n",
      "         -1.5720,  1.3000,  1.2697,  0.0444, -1.5216,  0.7718,  0.7805,  1.9900,\n",
      "          0.0185,  0.1076, -0.7922, -0.2120, -0.8908, -1.5540, -1.1086, -0.4769,\n",
      "         -0.5491, -1.5172, -1.9027,  0.1682,  1.0132, -0.5122,  0.7175,  0.6946],\n",
      "        [ 1.7882, -0.8421,  0.9680, -0.3203, -1.1296,  0.3277,  0.4534,  1.3401,\n",
      "          0.5100,  2.0981, -0.5489, -0.8876,  0.1680,  1.0444,  1.3148,  0.5075,\n",
      "         -0.7907, -1.0274, -0.4919, -0.5610,  0.1407,  0.4586, -0.1407, -2.3039,\n",
      "         -0.4175,  1.0988, -1.7052,  1.5001,  0.2383, -0.4933,  0.9709,  1.6765,\n",
      "          0.0083,  1.6478,  0.1630, -1.0521, -0.5450,  0.0739,  0.3693,  1.9381,\n",
      "         -0.0861, -0.9042, -2.0119, -1.0842,  0.0177,  0.0888,  0.1942,  0.3974,\n",
      "         -0.8954,  0.2718, -0.5295,  0.4598, -0.8697, -0.0156, -0.3629,  1.4846,\n",
      "          1.2310, -0.7543,  0.7999,  0.5442,  0.5789,  0.5265,  0.5362,  0.2218],\n",
      "        [-0.6452,  0.5798, -1.4899, -1.5808, -1.4748,  0.4270, -0.0940,  0.7283,\n",
      "         -1.5608, -0.1250,  0.7036,  0.0822,  1.2846,  0.2176, -1.9776,  0.0692,\n",
      "         -1.3493,  0.5295, -2.5484,  0.0150, -0.1109, -0.6874,  1.7072,  0.0301,\n",
      "          1.1968,  1.8933,  0.7426,  0.9669,  0.3605,  1.1049,  0.2566, -0.0526,\n",
      "          0.9848, -0.9852, -0.5418, -0.4354, -0.3058, -0.1317, -0.7005, -0.0465,\n",
      "          1.9439,  0.2163,  0.9167,  0.6790, -0.0100,  0.1238,  1.3334,  0.6911,\n",
      "          0.3903, -0.6625,  0.8034, -0.9674,  0.4727,  1.2463, -2.2748, -1.2706,\n",
      "          0.1366, -2.1396, -0.1144, -0.9698, -0.7230, -0.0040,  1.2247, -0.9465],\n",
      "        [ 0.5073, -0.6570,  1.0422, -1.3991,  0.4453,  0.3433, -0.0235,  0.0748,\n",
      "          0.3504,  0.6951, -1.7316, -0.7812,  0.4619,  0.1899, -0.5501,  0.1377,\n",
      "          0.9206,  0.1146,  0.7707, -0.0388,  0.5366, -0.7022, -0.4748,  0.0675,\n",
      "          0.5838,  1.6498, -0.0302,  1.0721,  0.8581, -0.6842, -0.6354, -0.2886,\n",
      "         -0.7819,  0.3731,  0.0873, -1.1964, -0.3179, -0.8224, -0.5623,  1.9089,\n",
      "          1.8259,  1.6898,  0.0318, -0.1594, -1.7446, -0.6931, -0.3655,  1.8651,\n",
      "         -0.2214,  0.1610,  2.0908,  1.6910,  0.3244,  0.6414, -0.2108,  0.6857,\n",
      "         -0.1745, -1.2300, -1.2656,  0.4894, -0.5267,  0.8493, -0.4388, -0.3560],\n",
      "        [ 0.6468,  0.0100, -0.2102,  0.1305,  1.4337, -0.9106, -0.5843, -0.1369,\n",
      "         -0.9532, -2.4428, -0.3781,  0.8586,  0.9749, -0.3522, -0.7796,  0.8350,\n",
      "          0.0421, -1.0565, -0.7782, -0.3734,  0.9273, -0.1002,  0.1589, -0.0428,\n",
      "          0.9154, -1.4873,  0.9737,  1.1376, -0.1740, -0.2745, -0.2172,  0.7302,\n",
      "          0.7124, -1.2917, -0.4990,  0.5601, -0.4167, -0.5827, -0.0494, -1.5890,\n",
      "         -0.4813, -0.7614,  0.2894,  0.1344,  1.0112, -0.5810, -0.5235,  0.6743,\n",
      "         -0.4873,  1.1009, -0.4262,  1.3943,  0.8548, -0.0112,  0.5906,  1.0649,\n",
      "          0.6649, -1.3168,  0.0298, -1.1234, -0.0804,  1.1568,  0.7691, -0.7028],\n",
      "        [-1.3255,  0.4975, -0.9852,  0.0081, -0.4211, -0.9750, -0.6378,  0.8173,\n",
      "          1.5352, -0.1102, -0.4417,  0.1257,  0.0900, -0.9007, -1.2219,  1.1046,\n",
      "          1.0731, -0.6734,  0.0374, -0.0140,  0.1571,  1.1696, -1.2920, -1.3448,\n",
      "         -1.0800,  0.3069,  0.3898, -0.6876,  2.7470,  1.8949,  1.9752, -1.0634,\n",
      "          1.5636,  0.8276, -0.3682, -0.5931, -0.4900, -0.6094, -1.6369, -1.0286,\n",
      "         -0.4727, -0.0855, -0.5134, -1.1239, -0.4455, -0.0198,  0.3611, -0.0395,\n",
      "          1.1618, -0.3243,  0.6600,  0.0773, -0.7519, -0.6135, -1.2257,  1.9122,\n",
      "         -0.9432, -2.4980,  0.6895,  0.8023,  0.5957,  0.4163, -0.2904,  0.5062],\n",
      "        [-1.3337,  0.1661, -0.5881,  0.1315,  1.4681,  0.2127, -0.6697, -0.9143,\n",
      "          0.4501, -0.4609,  0.1871, -0.4016, -1.7603,  0.1209, -1.8404, -0.6194,\n",
      "          1.3258, -1.2521,  0.7516,  0.6341,  0.3515, -2.5563, -1.3912, -0.9280,\n",
      "          0.2387, -0.1610,  1.1506, -0.4268,  0.9898,  0.5409,  1.1428, -0.5809,\n",
      "         -1.7203, -0.1083, -1.3068,  1.0866, -0.6246, -0.7394, -0.1682,  1.5747,\n",
      "          1.9152, -1.2269,  1.1009,  1.6920, -1.9718, -0.4088, -0.9632, -0.0211,\n",
      "          1.8304,  2.5289,  0.3678,  0.3399,  0.2270, -0.2478,  0.2834, -0.4252,\n",
      "          0.4798,  0.6386,  1.1304,  0.0274,  0.4783, -0.5926, -0.1706,  0.5755],\n",
      "        [-0.8089, -1.0792,  0.2166, -0.7581, -1.7723, -0.2364, -0.3644,  2.0918,\n",
      "         -0.4570, -1.2355,  0.7481,  0.0176, -0.3006, -0.2245, -1.1396, -0.9105,\n",
      "          0.0622,  0.0834,  0.7279, -0.1571, -0.8768,  0.3201,  0.8065, -1.0940,\n",
      "          0.4782, -1.1720,  0.1380,  0.4065,  0.6212,  0.1593,  1.6763,  0.6071,\n",
      "         -1.8367,  2.2493, -0.9189,  0.6615, -0.4164, -2.1654,  1.2791,  0.2083,\n",
      "         -0.8092,  1.5995,  1.5219, -0.0787,  0.4332,  0.3232,  0.7587,  1.1085,\n",
      "         -0.4174,  0.5233, -0.6531, -0.4211,  0.1395,  0.1109,  0.5275, -0.5642,\n",
      "         -1.0525, -0.3744,  0.7772,  1.4571,  0.0355,  1.3483,  0.2313,  0.4877]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.in_proj_weight tensor([[ 0.0703, -0.1165, -0.1375,  ...,  0.0212, -0.1052,  0.0192],\n",
      "        [ 0.0411, -0.0424,  0.0953,  ..., -0.0461, -0.0236, -0.1036],\n",
      "        [ 0.0287,  0.1327,  0.0658,  ...,  0.1354, -0.0870, -0.0376],\n",
      "        ...,\n",
      "        [ 0.0707,  0.0237,  0.1266,  ...,  0.0025,  0.1408, -0.1374],\n",
      "        [ 0.0490, -0.0774, -0.1178,  ...,  0.0374, -0.1118,  0.1434],\n",
      "        [ 0.1466, -0.1332, -0.1414,  ...,  0.0549, -0.0502, -0.0088]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.in_proj_bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.out_proj.weight tensor([[ 0.1219, -0.0928,  0.0155,  ...,  0.0371,  0.1024, -0.0195],\n",
      "        [-0.0048,  0.1221,  0.0839,  ..., -0.0830, -0.0791,  0.0733],\n",
      "        [ 0.0856, -0.1222,  0.0995,  ..., -0.0639,  0.0036, -0.0691],\n",
      "        ...,\n",
      "        [ 0.0203,  0.1174,  0.1159,  ...,  0.0586,  0.0476, -0.1011],\n",
      "        [ 0.0005,  0.0868,  0.0465,  ...,  0.0547, -0.1133, -0.0812],\n",
      "        [-0.1178, -0.1099, -0.0940,  ..., -0.0874, -0.0822, -0.0179]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.self_attn.out_proj.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear1.weight tensor([[-0.0434,  0.1242, -0.1073,  ..., -0.0213, -0.0520, -0.0027],\n",
      "        [-0.0805, -0.0889, -0.0868,  ...,  0.0488,  0.1191, -0.0554],\n",
      "        [ 0.1245, -0.1052, -0.0716,  ...,  0.0981,  0.0616, -0.0802],\n",
      "        ...,\n",
      "        [ 0.0621,  0.1016,  0.1089,  ...,  0.0488, -0.0239, -0.0645],\n",
      "        [-0.0704, -0.0956,  0.0587,  ...,  0.0442, -0.1043,  0.0921],\n",
      "        [-0.0672,  0.0947,  0.0921,  ..., -0.1023,  0.0511,  0.0835]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear1.bias tensor([-0.0463, -0.0482,  0.0280, -0.0686, -0.0988, -0.0673, -0.0733, -0.1123,\n",
      "        -0.0716,  0.0571,  0.1165, -0.0762, -0.0144, -0.1066, -0.0080,  0.1027,\n",
      "        -0.0611,  0.0536,  0.1062,  0.0387, -0.0515, -0.0200, -0.0940,  0.0399,\n",
      "         0.0301, -0.0883,  0.1065,  0.0029, -0.1145,  0.0316, -0.0973,  0.0511,\n",
      "         0.0636, -0.0207, -0.0341, -0.0081, -0.0718,  0.0273,  0.0333,  0.0698,\n",
      "        -0.0483, -0.0552,  0.0185, -0.1165, -0.1079,  0.0433, -0.0879, -0.0317,\n",
      "         0.0933,  0.0965,  0.0585, -0.0246, -0.0218, -0.0746, -0.1119,  0.0030,\n",
      "         0.0556, -0.0229, -0.0357,  0.0844, -0.0087, -0.0609, -0.0826, -0.0518],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear2.weight tensor([[-0.0686,  0.0209, -0.0159,  ...,  0.1047,  0.0429, -0.1198],\n",
      "        [ 0.0638, -0.0108, -0.0980,  ..., -0.0836,  0.0308, -0.0767],\n",
      "        [-0.1114,  0.0920,  0.0930,  ...,  0.0805, -0.1108,  0.0153],\n",
      "        ...,\n",
      "        [-0.1014,  0.1001,  0.0917,  ...,  0.0105, -0.0279,  0.1165],\n",
      "        [-0.1101, -0.0055, -0.0524,  ..., -0.1234, -0.0976, -0.0842],\n",
      "        [ 0.0119,  0.0240,  0.0157,  ...,  0.0464,  0.0430,  0.0867]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.linear2.bias tensor([-0.0354, -0.0503, -0.0427, -0.0258,  0.0338, -0.0213, -0.0616,  0.0753,\n",
      "        -0.0554,  0.1166,  0.0627,  0.1206,  0.0918,  0.0535, -0.0687, -0.0986,\n",
      "        -0.0976,  0.0492, -0.0991, -0.0068,  0.1092, -0.0254, -0.0342, -0.0542,\n",
      "         0.0081, -0.0508,  0.0273, -0.1021, -0.0283,  0.0683, -0.0380,  0.0252,\n",
      "        -0.0947, -0.0005, -0.1238, -0.1210, -0.0306, -0.0176,  0.0583,  0.1139,\n",
      "        -0.1039,  0.0451, -0.0414, -0.0518,  0.0741,  0.0518,  0.0111,  0.0484,\n",
      "        -0.0086, -0.1226, -0.1163, -0.0482, -0.1223,  0.0913, -0.0175, -0.1109,\n",
      "        -0.0113, -0.0682, -0.0551,  0.0859,  0.0088,  0.0958, -0.1084,  0.0657],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.norm1.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder_layer.norm1.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder_layer.norm2.weight tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0')\n",
      "transformer_encoder_layer.norm2.bias tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_weight tensor([[ 0.0219, -0.1135, -0.0762,  ..., -0.0118, -0.1166, -0.0196],\n",
      "        [-0.0322, -0.0298,  0.1813,  ..., -0.0910, -0.0337, -0.1324],\n",
      "        [ 0.0773,  0.1050,  0.0523,  ...,  0.1401, -0.0670, -0.0265],\n",
      "        ...,\n",
      "        [ 0.0976,  0.0326,  0.0978,  ...,  0.0387,  0.1296, -0.1433],\n",
      "        [ 0.0837, -0.0885, -0.1126,  ...,  0.0369, -0.0654,  0.1404],\n",
      "        [ 0.1547, -0.1119, -0.1743,  ...,  0.0437, -0.0442, -0.0103]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.in_proj_bias tensor([-2.0620e-02, -3.8369e-02,  9.0105e-02,  2.2040e-02,  3.2269e-02,\n",
      "         2.2436e-03, -2.5250e-02,  5.8059e-03, -2.9242e-02, -5.1693e-02,\n",
      "         7.5736e-03, -6.0978e-02,  3.2418e-03,  1.6821e-02,  1.6839e-02,\n",
      "         3.6258e-02, -9.0493e-03, -6.6342e-04, -1.8020e-02, -6.2657e-03,\n",
      "        -3.1052e-02, -1.1489e-02,  6.9000e-03,  1.4882e-02,  8.9320e-03,\n",
      "        -2.1193e-02, -4.5765e-02, -4.0652e-02,  7.2750e-03, -3.2420e-02,\n",
      "         4.5063e-02,  3.9777e-03, -1.0590e-01,  1.2903e-01, -8.8160e-02,\n",
      "        -8.2553e-02,  1.0974e-01,  3.3945e-02,  1.7398e-01, -8.1399e-02,\n",
      "         1.9091e-02, -6.9589e-02,  1.0181e-01, -1.2906e-01, -6.0900e-02,\n",
      "        -1.1396e-02,  9.7741e-02, -3.6884e-02, -5.9295e-03, -3.5938e-02,\n",
      "        -7.7268e-04,  1.3349e-02,  8.6225e-02, -3.3399e-02, -3.3085e-02,\n",
      "        -3.8965e-03, -3.6296e-03,  1.4874e-02,  1.3143e-02,  1.8588e-02,\n",
      "         4.5761e-02,  1.3973e-02, -4.3526e-03,  5.2283e-03,  1.8765e-04,\n",
      "        -8.4781e-05,  4.5591e-04,  3.5448e-05, -6.2092e-05,  3.7850e-04,\n",
      "        -3.4638e-04,  2.4754e-05, -1.3684e-05, -5.2441e-06, -2.5077e-04,\n",
      "        -3.8775e-05,  2.4588e-04, -2.9824e-04,  3.6739e-06, -1.0195e-04,\n",
      "        -5.1717e-05,  1.4115e-04,  6.3171e-05, -1.5162e-05,  2.6234e-04,\n",
      "        -9.6214e-05,  1.1677e-04,  4.8819e-06,  1.4632e-04,  7.3640e-05,\n",
      "        -1.6383e-04,  4.3717e-05,  7.6985e-05, -4.9707e-05,  1.3619e-04,\n",
      "         1.0701e-04,  2.4471e-04,  3.5193e-04,  3.8800e-04, -2.3796e-04,\n",
      "        -2.2651e-04,  1.3193e-04, -1.2730e-04, -1.8717e-05, -1.1274e-04,\n",
      "        -4.2141e-04,  2.0178e-04, -2.0408e-04,  2.1199e-04, -5.8545e-05,\n",
      "         5.4870e-05, -1.8299e-04,  2.5350e-04,  6.6249e-06, -1.3058e-05,\n",
      "        -8.7830e-05,  5.0515e-05,  2.7800e-07,  2.9952e-05,  8.4431e-05,\n",
      "         4.8581e-05,  1.1037e-04,  1.2833e-04, -2.2571e-04, -2.4100e-04,\n",
      "         2.1765e-04, -1.0382e-04, -2.0545e-05, -9.1337e-03, -3.7609e-02,\n",
      "        -1.9582e-02, -5.7764e-02,  6.8354e-02, -4.6879e-02,  1.0828e-02,\n",
      "        -1.2764e-02,  1.8284e-02,  6.5670e-02, -1.6579e-02,  3.2535e-02,\n",
      "         3.0307e-02,  6.1372e-02, -2.4969e-02,  9.5185e-03,  5.7307e-03,\n",
      "        -1.1796e-02, -1.7907e-02,  1.5558e-02,  1.4000e-02,  2.5095e-02,\n",
      "         1.2013e-02, -2.5711e-03,  9.6096e-03, -2.9811e-02, -6.3871e-03,\n",
      "         1.8334e-03,  1.7330e-02, -3.8202e-03,  8.2363e-03, -7.3100e-03,\n",
      "        -3.1776e-02, -4.6456e-03, -3.5899e-03,  1.0479e-02, -2.4202e-02,\n",
      "        -1.5171e-02, -8.8201e-03, -1.0561e-02, -1.3362e-02,  2.3925e-02,\n",
      "        -1.7261e-02, -2.3952e-03, -3.8705e-03,  1.2477e-02, -6.2823e-03,\n",
      "        -5.7027e-03,  2.3495e-02, -2.1281e-02,  2.1600e-02, -7.2838e-03,\n",
      "        -2.1995e-02,  3.0952e-03, -2.2504e-02, -6.4485e-03,  1.1322e-02,\n",
      "        -1.3056e-02, -9.2498e-03,  5.4356e-03,  1.5423e-02,  5.1666e-03,\n",
      "         1.0762e-02,  1.0869e-02], device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.weight tensor([[ 0.1144, -0.1247, -0.0334,  ...,  0.0689,  0.1238,  0.0132],\n",
      "        [-0.0085,  0.1739,  0.1057,  ..., -0.0671, -0.0804,  0.0397],\n",
      "        [ 0.0955, -0.1051,  0.1359,  ..., -0.0939, -0.0180, -0.0953],\n",
      "        ...,\n",
      "        [ 0.0092,  0.1091,  0.0906,  ...,  0.0768,  0.0743, -0.1117],\n",
      "        [ 0.0454,  0.0675,  0.0132,  ...,  0.0752, -0.1250, -0.0648],\n",
      "        [-0.0920, -0.1027, -0.1399,  ..., -0.0528, -0.0984, -0.0166]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.self_attn.out_proj.bias tensor([-0.0247,  0.0072,  0.0057,  0.0020,  0.0017,  0.0104, -0.0022, -0.0092,\n",
      "         0.0071,  0.0087,  0.0106,  0.0198,  0.0028,  0.0132,  0.0324,  0.0191,\n",
      "         0.0159,  0.0210, -0.0192, -0.0059, -0.0006,  0.0020,  0.0110, -0.0010,\n",
      "        -0.0281,  0.0395, -0.0005, -0.0285, -0.0105, -0.0077, -0.0211, -0.0240,\n",
      "        -0.0002, -0.0273, -0.0005, -0.0050,  0.0135,  0.0075, -0.0024,  0.0003,\n",
      "         0.0024, -0.0229,  0.0168,  0.0032,  0.0127,  0.0193, -0.0121,  0.0081,\n",
      "        -0.0059,  0.0065,  0.0167, -0.0157,  0.0011, -0.0068,  0.0080,  0.0164,\n",
      "         0.0026,  0.0046,  0.0050, -0.0168, -0.0248, -0.0109, -0.0038,  0.0030],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.weight tensor([[-0.0469,  0.1548, -0.0423,  ..., -0.0341, -0.0839,  0.0410],\n",
      "        [-0.0597, -0.0640, -0.0716,  ...,  0.0332,  0.1218, -0.0444],\n",
      "        [ 0.1289, -0.1067, -0.1044,  ...,  0.0845,  0.0328, -0.0847],\n",
      "        ...,\n",
      "        [ 0.0371,  0.0403,  0.1013,  ...,  0.0112, -0.0424, -0.0865],\n",
      "        [-0.0814, -0.0617,  0.1200,  ...,  0.0144, -0.1461,  0.0982],\n",
      "        [-0.0855,  0.0384,  0.0165,  ..., -0.1191,  0.0782,  0.0968]],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear1.bias tensor([-0.0656, -0.0862, -0.0154, -0.1002, -0.0856, -0.0838, -0.0854, -0.1262,\n",
      "        -0.0747,  0.0072,  0.0962, -0.0820, -0.0331, -0.1024, -0.0433,  0.0947,\n",
      "        -0.0647,  0.0527,  0.0956,  0.0287, -0.0473, -0.0468, -0.0919,  0.0245,\n",
      "         0.0152, -0.0944,  0.0850, -0.0167, -0.1340,  0.0105, -0.0998,  0.0272,\n",
      "         0.0492, -0.0224, -0.0611, -0.0405, -0.1089,  0.0176,  0.0436,  0.0681,\n",
      "        -0.0735, -0.0652,  0.0260, -0.1479, -0.1300,  0.0227, -0.1383, -0.0616,\n",
      "         0.0724,  0.0730,  0.0208, -0.0500, -0.0470, -0.0907, -0.1283, -0.0338,\n",
      "         0.0331, -0.0525, -0.0468,  0.0614, -0.0712, -0.0621, -0.1072, -0.0807],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.weight tensor([[-8.2265e-02, -1.2837e-02, -3.1344e-03,  ...,  7.5576e-02,\n",
      "          7.8232e-03, -5.9342e-02],\n",
      "        [ 5.7043e-02, -1.9631e-02, -9.5847e-02,  ..., -8.5674e-02,\n",
      "          4.5810e-02, -1.2482e-01],\n",
      "        [-9.5975e-02,  9.8930e-02,  8.4272e-02,  ...,  6.5731e-02,\n",
      "         -1.0343e-01, -3.6816e-02],\n",
      "        ...,\n",
      "        [-1.3443e-01,  8.8023e-02,  9.1546e-02,  ..., -1.9487e-02,\n",
      "         -7.2039e-02,  1.4945e-01],\n",
      "        [-1.3949e-01, -2.2424e-03, -4.2295e-02,  ..., -8.4816e-02,\n",
      "         -1.2360e-01, -1.0857e-04],\n",
      "        [-1.3873e-02,  1.1027e-02,  1.2915e-02,  ...,  2.9922e-02,\n",
      "          1.1243e-02,  1.2347e-01]], device='cuda:0')\n",
      "transformer_encoder.layers.0.linear2.bias tensor([-0.0713, -0.0603, -0.0407, -0.0470,  0.0225, -0.0091, -0.0488,  0.0471,\n",
      "        -0.0433,  0.1031,  0.0725,  0.0971,  0.0827,  0.0611, -0.0407, -0.0826,\n",
      "        -0.1177,  0.0686, -0.0994, -0.0135,  0.0932,  0.0278, -0.0506, -0.0475,\n",
      "         0.0060, -0.0292,  0.0283, -0.1124, -0.0276,  0.0240, -0.0516,  0.0159,\n",
      "        -0.0722,  0.0195, -0.1352, -0.1310, -0.0237, -0.0112,  0.0748,  0.1179,\n",
      "        -0.1295,  0.0243, -0.0341, -0.0447,  0.0724,  0.0524,  0.0060,  0.0454,\n",
      "        -0.0277, -0.1302, -0.1076, -0.0538, -0.0952,  0.0685, -0.0153, -0.0929,\n",
      "        -0.0293, -0.0645, -0.0483,  0.0685,  0.0079,  0.0832, -0.1007,  0.0562],\n",
      "       device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.weight tensor([1.0952, 1.0153, 1.0412, 0.9808, 1.0314, 1.0536, 1.0275, 0.9950, 1.0365,\n",
      "        1.0300, 1.0465, 1.0368, 1.0635, 1.0297, 0.9872, 1.0674, 1.1273, 1.0254,\n",
      "        1.0825, 1.0480, 1.0061, 1.0188, 1.0518, 1.0360, 1.0545, 1.0347, 1.0070,\n",
      "        1.0531, 0.9943, 0.9961, 0.9731, 1.0205, 1.0219, 0.9615, 1.0554, 1.0528,\n",
      "        1.0289, 0.9763, 1.0551, 0.9452, 0.9642, 0.9908, 1.0135, 1.0411, 0.9561,\n",
      "        1.0743, 1.0257, 1.0625, 1.0318, 1.0096, 0.9811, 1.0674, 1.0541, 1.0745,\n",
      "        1.0247, 1.0006, 0.9790, 1.0540, 1.0165, 1.0131, 0.9960, 0.9969, 0.9890,\n",
      "        1.0297], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm1.bias tensor([ 3.0607e-02,  2.8117e-02,  1.6130e-02,  7.2385e-03, -9.0397e-03,\n",
      "         1.1715e-02,  2.5939e-02, -8.0401e-03, -7.8154e-03,  9.8251e-04,\n",
      "        -2.1539e-02,  1.8317e-02,  2.3619e-03, -9.0604e-03,  2.2042e-02,\n",
      "         4.7094e-02,  1.9569e-02, -2.1019e-02, -1.8798e-02,  2.9974e-02,\n",
      "        -1.4160e-02,  2.9187e-02,  5.0739e-02, -1.4038e-03, -1.0175e-02,\n",
      "         3.5596e-02,  2.5602e-02,  8.3121e-03, -4.2072e-02, -6.9081e-02,\n",
      "        -7.7623e-02, -2.9682e-02,  1.1169e-02,  1.5463e-02,  1.0288e-02,\n",
      "         7.6376e-05,  5.0338e-02,  3.3438e-02, -1.4991e-03, -4.3756e-02,\n",
      "        -7.1488e-02, -2.8177e-02,  7.6219e-03, -1.3807e-02,  1.5485e-02,\n",
      "         1.1295e-02, -1.0177e-02, -4.9377e-03,  3.7482e-03,  1.8259e-02,\n",
      "         1.5581e-02, -1.1562e-02, -1.8208e-02,  3.4817e-02, -2.2951e-02,\n",
      "         5.9553e-03, -4.1836e-02, -4.3545e-02,  5.5344e-03,  6.3350e-02,\n",
      "         4.6991e-02,  2.0270e-02,  4.5907e-02,  3.0003e-02], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.weight tensor([0.9599, 0.9680, 1.0040, 1.0467, 1.0243, 1.0716, 0.9981, 0.9953, 0.9749,\n",
      "        0.9254, 0.9480, 0.9901, 0.9233, 1.0555, 0.9511, 1.0015, 1.0691, 0.9847,\n",
      "        1.0216, 1.0378, 0.9289, 1.0222, 0.9604, 0.9889, 0.9880, 0.9504, 0.9675,\n",
      "        0.9565, 1.0088, 0.9918, 0.9073, 0.9766, 1.0526, 0.9443, 1.0238, 1.0175,\n",
      "        0.9479, 0.9316, 1.0050, 0.9780, 1.0060, 0.9892, 0.9487, 0.9709, 0.9714,\n",
      "        0.9855, 0.9987, 1.0170, 0.9876, 0.9614, 0.9606, 0.9895, 0.9831, 0.9894,\n",
      "        0.9320, 0.9409, 1.0093, 0.9909, 1.0098, 0.9714, 0.9796, 0.9362, 0.9838,\n",
      "        0.9980], device='cuda:0')\n",
      "transformer_encoder.layers.0.norm2.bias tensor([ 1.8274e-02,  3.1846e-02, -1.4141e-03, -1.2353e-02, -1.8443e-02,\n",
      "         1.6165e-02,  2.6264e-02, -5.6627e-02,  8.5036e-03,  4.2801e-02,\n",
      "         6.7526e-03,  2.3258e-02, -1.4447e-03,  5.3663e-02,  1.3821e-02,\n",
      "         5.3828e-02, -3.0798e-03, -1.4397e-02, -8.7130e-03,  1.9225e-02,\n",
      "        -1.0204e-02,  3.6692e-03, -1.8897e-03,  1.0495e-02, -4.4630e-02,\n",
      "         2.4110e-02, -5.6780e-02,  9.4899e-03,  4.6568e-03,  7.6630e-03,\n",
      "         1.5549e-02, -3.2503e-02,  6.6053e-03, -6.8865e-02,  1.5229e-02,\n",
      "        -1.9160e-02,  9.4925e-03,  3.9488e-02,  5.3349e-03, -2.1861e-02,\n",
      "         4.2141e-04,  3.6281e-05, -3.0263e-03, -3.4271e-02,  2.9920e-02,\n",
      "         2.2420e-02, -1.2470e-02, -1.5548e-02,  1.9728e-02,  7.2063e-03,\n",
      "        -2.1747e-02, -2.5899e-02,  2.8335e-02,  2.8194e-02, -7.4177e-03,\n",
      "         4.2408e-02, -6.5016e-03,  4.6502e-02, -2.1006e-02, -7.8670e-04,\n",
      "        -3.6662e-02, -4.3802e-02, -2.4843e-03, -2.5695e-02], device='cuda:0')\n",
      "conv1d.weight tensor([[[ 8.2420e-03],\n",
      "         [ 2.0228e-02],\n",
      "         [-8.2918e-03],\n",
      "         ...,\n",
      "         [-2.4695e-02],\n",
      "         [-7.4198e-03],\n",
      "         [-3.0497e-02]],\n",
      "\n",
      "        [[-1.7689e-02],\n",
      "         [-1.6834e-02],\n",
      "         [-9.1540e-03],\n",
      "         ...,\n",
      "         [ 2.6454e-02],\n",
      "         [-9.0606e-03],\n",
      "         [ 1.6175e-03]],\n",
      "\n",
      "        [[ 4.2680e-03],\n",
      "         [ 2.1573e-02],\n",
      "         [ 2.3198e-02],\n",
      "         ...,\n",
      "         [ 3.0106e-03],\n",
      "         [-8.7097e-03],\n",
      "         [-3.8866e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 7.6810e-03],\n",
      "         [-4.8295e-02],\n",
      "         [-1.0699e-02],\n",
      "         ...,\n",
      "         [-3.7320e-02],\n",
      "         [-7.0348e-02],\n",
      "         [-8.5739e-02]],\n",
      "\n",
      "        [[-2.6874e-02],\n",
      "         [ 3.0045e-05],\n",
      "         [ 3.9422e-02],\n",
      "         ...,\n",
      "         [-3.0912e-02],\n",
      "         [-2.6130e-02],\n",
      "         [-4.1728e-02]],\n",
      "\n",
      "        [[ 1.9033e-02],\n",
      "         [ 3.5214e-02],\n",
      "         [ 1.7119e-02],\n",
      "         ...,\n",
      "         [-9.2047e-04],\n",
      "         [ 1.0614e-01],\n",
      "         [ 8.6754e-02]]], device='cuda:0')\n",
      "conv1d.bias tensor([-0.0113, -0.0348, -0.0177, -0.0046,  0.0005,  0.0019, -0.0294, -0.0154,\n",
      "         0.0339, -0.0293,  0.0011, -0.0209, -0.0101,  0.0217, -0.0023,  0.0246,\n",
      "         0.0114,  0.0195, -0.0280, -0.0101, -0.0124, -0.0250,  0.0345,  0.0268,\n",
      "        -0.0033,  0.0148,  0.0333, -0.0187,  0.0224, -0.0185, -0.0106, -0.0063,\n",
      "         0.0110,  0.0343,  0.0171, -0.0016,  0.0410, -0.0010, -0.0045,  0.0145,\n",
      "         0.0079, -0.0347, -0.0180,  0.0183, -0.0032,  0.0331,  0.0144,  0.0168,\n",
      "        -0.0426, -0.0008, -0.0076,  0.0017,  0.0385, -0.0065,  0.0334, -0.0297,\n",
      "         0.0039,  0.0054, -0.0338, -0.0083,  0.0090,  0.0089,  0.0335,  0.0200],\n",
      "       device='cuda:0')\n",
      "fc.0.weight tensor([[-0.0214, -0.0430, -0.1128,  ...,  0.0489,  0.0397, -0.1508],\n",
      "        [ 0.0658, -0.1222, -0.0927,  ..., -0.0408,  0.0373,  0.0126],\n",
      "        [ 0.0760, -0.1307, -0.1230,  ...,  0.0458,  0.0338,  0.1086],\n",
      "        ...,\n",
      "        [ 0.0980, -0.0192,  0.0236,  ..., -0.0124, -0.0084, -0.1115],\n",
      "        [ 0.0523, -0.0518, -0.1353,  ..., -0.0552, -0.1067,  0.1616],\n",
      "        [-0.0710, -0.0164,  0.0014,  ...,  0.0291, -0.0181, -0.0272]],\n",
      "       device='cuda:0')\n",
      "fc.0.bias tensor([-0.0393,  0.0258, -0.0746, -0.0664, -0.0883,  0.1037,  0.0468, -0.1079,\n",
      "        -0.0542, -0.0088,  0.0795, -0.0400,  0.1177,  0.0544, -0.0563,  0.0065,\n",
      "        -0.0098, -0.1091,  0.1104,  0.0783, -0.0735,  0.0935, -0.0891, -0.0539,\n",
      "        -0.0347,  0.0466, -0.0189, -0.0296, -0.1025, -0.1226, -0.0959, -0.1164,\n",
      "        -0.0799, -0.0969, -0.0742, -0.0602, -0.0289, -0.0660, -0.0891, -0.0510,\n",
      "         0.1219, -0.0854, -0.0538,  0.0985,  0.0281, -0.0097, -0.0982, -0.1394,\n",
      "        -0.1384, -0.1136, -0.0201, -0.0613,  0.0085,  0.0311, -0.0782,  0.0386,\n",
      "        -0.0820,  0.1050, -0.0334, -0.0277, -0.0515,  0.0220,  0.0367,  0.0403],\n",
      "       device='cuda:0')\n",
      "fc.3.weight tensor([[-0.0143,  0.0630,  0.0472,  ..., -0.0800, -0.0799, -0.0544],\n",
      "        [ 0.0847, -0.0823, -0.0460,  ...,  0.1022,  0.1122,  0.0395],\n",
      "        [ 0.0500,  0.1289,  0.0738,  ..., -0.0150,  0.0429, -0.0137],\n",
      "        ...,\n",
      "        [-0.0587, -0.1658, -0.1499,  ...,  0.0377, -0.1398,  0.1183],\n",
      "        [ 0.0190,  0.0927,  0.0416,  ...,  0.0586, -0.0340,  0.0671],\n",
      "        [-0.0622, -0.0193,  0.1512,  ...,  0.0847, -0.0758, -0.0199]],\n",
      "       device='cuda:0')\n",
      "fc.3.bias tensor([-0.0335, -0.1403,  0.0221,  0.0519, -0.1260,  0.1169,  0.0278,  0.0077,\n",
      "        -0.0581, -0.0087, -0.0442,  0.1782, -0.1019, -0.1034,  0.1040, -0.0031,\n",
      "         0.0881,  0.0445, -0.1070, -0.0715, -0.0562,  0.0075, -0.0566, -0.0743,\n",
      "         0.0368, -0.0269,  0.0223, -0.0165, -0.0182, -0.0415, -0.0601, -0.1076,\n",
      "        -0.0348, -0.0515, -0.0026,  0.0024,  0.1354, -0.0242, -0.0193, -0.0168,\n",
      "        -0.1352, -0.0247, -0.0500, -0.0467, -0.0360, -0.0360, -0.0618, -0.0697,\n",
      "        -0.1151,  0.0988, -0.0443, -0.0308, -0.0594, -0.0427,  0.0404,  0.1045,\n",
      "        -0.1235,  0.0113,  0.0037, -0.0257, -0.0615, -0.0846,  0.0053, -0.1232,\n",
      "        -0.0563, -0.0179, -0.0312,  0.0634,  0.0810, -0.1299, -0.0346, -0.0921,\n",
      "         0.0185,  0.1166, -0.1229, -0.0913, -0.0624, -0.1132,  0.0893, -0.0990,\n",
      "        -0.0153, -0.0976,  0.0150,  0.0660, -0.0103, -0.0777, -0.1205, -0.1546,\n",
      "         0.0306,  0.0708,  0.0327, -0.0475,  0.0592,  0.0868, -0.0589,  0.0888,\n",
      "         0.0295,  0.0529, -0.0407,  0.0272, -0.0818,  0.0418,  0.0060,  0.0989,\n",
      "         0.1271,  0.0582,  0.1310,  0.0118,  0.0006, -0.0532, -0.0627,  0.0244,\n",
      "         0.0375,  0.0623, -0.0238, -0.0542,  0.0101, -0.1025, -0.0664,  0.0490,\n",
      "         0.0717,  0.1292,  0.0039,  0.1346,  0.0116,  0.0716, -0.0313,  0.0174],\n",
      "       device='cuda:0')\n",
      "fc.6.weight tensor([[ 0.0042, -0.1304, -0.0104,  ...,  0.0977, -0.0762, -0.0664],\n",
      "        [-0.0163,  0.0127, -0.0561,  ...,  0.0074,  0.1065,  0.1040],\n",
      "        [-0.0216,  0.0126,  0.0522,  ..., -0.0906,  0.1109,  0.0664],\n",
      "        ...,\n",
      "        [ 0.0705, -0.1019, -0.0717,  ...,  0.0478, -0.0539, -0.1141],\n",
      "        [-0.0414, -0.0476, -0.0051,  ...,  0.0396, -0.0021, -0.0399],\n",
      "        [-0.0096, -0.0931,  0.0232,  ...,  0.0104, -0.0955,  0.0161]],\n",
      "       device='cuda:0')\n",
      "fc.6.bias tensor([ 0.1060,  0.0380, -0.1193, -0.0771, -0.1053,  0.1417,  0.1222,  0.1036,\n",
      "         0.0803,  0.0254,  0.0060, -0.0847, -0.1083, -0.0781,  0.1363,  0.0435,\n",
      "         0.0209,  0.1298, -0.0119,  0.0197,  0.0490,  0.0306,  0.0216, -0.0471,\n",
      "        -0.1086, -0.0376, -0.0331,  0.0477, -0.0014,  0.0694, -0.0807, -0.0246,\n",
      "        -0.1051,  0.0216, -0.0190,  0.0623, -0.0707,  0.0186, -0.0034, -0.0609,\n",
      "        -0.0591, -0.0548,  0.1381, -0.0728, -0.0720, -0.0709,  0.0161,  0.0259,\n",
      "        -0.0958,  0.0853,  0.0514, -0.0663, -0.0272,  0.0279,  0.0395,  0.0170,\n",
      "         0.0172,  0.0327,  0.0706,  0.0302,  0.0330,  0.0107,  0.1129, -0.0954],\n",
      "       device='cuda:0')\n",
      "fc.8.weight tensor([[ 0.1670, -0.1264, -0.0763,  0.0284, -0.1379,  0.0901,  0.1524,  0.1221,\n",
      "          0.1299, -0.1325,  0.1078, -0.0375, -0.0577, -0.0883,  0.1727,  0.1699,\n",
      "          0.1444,  0.1379, -0.0937,  0.0640,  0.0670, -0.1026, -0.0101, -0.0684,\n",
      "         -0.1321,  0.0902, -0.1194, -0.1003,  0.1654,  0.0663, -0.1056, -0.0226,\n",
      "         -0.1340, -0.0119,  0.1311,  0.1484, -0.0954, -0.0526, -0.1148, -0.0348,\n",
      "          0.0938, -0.0081,  0.1516, -0.1256, -0.0834, -0.0242,  0.1172, -0.0624,\n",
      "         -0.0299,  0.0785,  0.0277,  0.1154, -0.0462, -0.1009, -0.0604,  0.0016,\n",
      "         -0.1358, -0.0702,  0.0668, -0.0935,  0.1530,  0.1339,  0.1540, -0.0013]],\n",
      "       device='cuda:0')\n",
      "fc.8.bias tensor([-0.0590], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Print learnable parameter values after training\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fdf348489ac4c3a9047cf5bb561e237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.522 MB of 0.763 MB uploaded\\r'), FloatProgress(value=0.6837962644737582, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_score_test</td><td>▁█</td></tr><tr><td>f1_score_val</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>learning_rate</td><td>█▇▆▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>█▁</td></tr><tr><td>train_loss</td><td>█▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▂▂▂▁▁▂▂▁▁▁▂▂▂▁▁▂▂▂▁▁▂▁▁</td></tr><tr><td>val_accuracy</td><td>█▁▃▅▅▅▅▄▆▆▆▅▅▅▅▅▅▅▆▅▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>f1_score_test</td><td>0.48547</td></tr><tr><td>f1_score_val</td><td>0.51253</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>test_accuracy</td><td>0.8475</td></tr><tr><td>train_loss</td><td>1.04746</td></tr><tr><td>val_accuracy</td><td>0.85263</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">colorful-feather-65</strong> at: <a href='https://wandb.ai/onat-inak-/Transformer-Classifier/runs/arira6ci' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier/runs/arira6ci</a><br/> View project at: <a href='https://wandb.ai/onat-inak-/Transformer-Classifier' target=\"_blank\">https://wandb.ai/onat-inak-/Transformer-Classifier</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240701_033708-arira6ci/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
