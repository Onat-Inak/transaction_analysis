{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f091ffb1cb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from models.LSTM import LSTM\n",
    "from models.RNN import RNN\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset import Dataset\n",
    "import os\n",
    "from torchsummary import summary\n",
    "from tools.adjust_learning_rate import adjust_learning_rate\n",
    "from tools.train import train\n",
    "from tools.test import test\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import wandb\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
    "torch.manual_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of negative sequence array (OHE):  (100000, 10, 312)\n",
      "Shape of positive sequence array (OHE):  (11979, 10, 312)\n",
      "Shape of negative sequence array:  (100000, 10)\n",
      "Shape of positive sequence array:  (11979, 10)\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load 3D numpy matrices (user, time, transaction type)\n",
    "ns = np.load('data/neg_sequences.npy')\n",
    "ps = np.load('data/pos_sequences.npy')\n",
    "\n",
    "transaction_size = ns.shape[-1]\n",
    "\n",
    "# Take a look at the given data with OHE (one-hot encodings)\n",
    "print('Shape of negative sequence array (OHE): ', ns.shape)\n",
    "print('Shape of positive sequence array (OHE): ', ps.shape)\n",
    "\n",
    "# Convert one-hot encodings to integers:\n",
    "ns = np.argmax(ns, axis=2)\n",
    "ps = np.argmax(ps, axis=2)\n",
    "\n",
    "# Take a look at the given data\n",
    "print('Shape of negative sequence array: ', ns.shape)\n",
    "print('Shape of positive sequence array: ', ps.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data\n",
    "ns_label = np.zeros_like(ns[:,0])\n",
    "ps_label = np.ones_like(ps[:,0])\n",
    "\n",
    "# Concetenate positive sequences with negative sequences regarding users with correponding labels (axis=0)\n",
    "X = np.concatenate((ns, ps), axis=0)\n",
    "y = np.concatenate((ns_label, ps_label), axis=0) \n",
    "\n",
    "# Shuffle data and labels, for reproductivity set random_state=0\n",
    "# dataset, labels = shuffle(dataset, labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train, test and validation ratios\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.1\n",
    "val_ratio = 0.1\n",
    "\n",
    "# Split the data / Shuffle it and maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_ratio, random_state=42, shuffle=True)\n",
    "\n",
    "# Further split train_data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=val_ratio, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (90702, 10)  - y_train.shape:  (90702,)\n",
      "X_test.shape:  (11198, 10)  - y_test.shape:  (11198,)\n",
      "X_val.shape:  (10079, 10)  - y_val.shape:  (10079,)\n",
      "Training data -> num_pos_seq:  tensor([9724], device='cuda:0')  num_neg_seq:  tensor([80978], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Print train, test and validation dataset and label shapes\n",
    "print('X_train.shape: ', X_train.shape, ' - y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape, ' - y_test.shape: ', y_test.shape)\n",
    "print('X_val.shape: ', X_val.shape, ' - y_val.shape: ', y_val.shape)\n",
    "\n",
    "# Convert numpy arrays to torch.tensor\n",
    "X_train, y_train = torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
    "X_train, y_train = X_train.to(device, dtype=torch.int32), y_train.to(device, dtype=torch.float32)\n",
    "X_test, y_test = torch.from_numpy(X_test), torch.from_numpy(y_test)\n",
    "X_test, y_test = X_test.to(device, dtype=torch.int32), y_test.to(device, dtype=torch.float32)\n",
    "X_val, y_val = torch.from_numpy(X_val), torch.from_numpy(y_val)\n",
    "X_val, y_val = X_val.to(device, dtype=torch.int32), y_val.to(device, dtype=torch.float32)\n",
    "\n",
    "# Number of positive sequences in training data\n",
    "num_pos_seq = torch.count_nonzero(y_train).view(1)\n",
    "num_neg_seq = (y_train.shape[0] - num_pos_seq).view(1)\n",
    "print('Training data -> num_pos_seq: ', num_pos_seq, ' num_neg_seq: ', num_neg_seq)\n",
    "\n",
    "# Create a custom dataset\n",
    "train_dataset = Dataset(X_train, y_train, device)\n",
    "test_dataset = Dataset(X_test, y_test, device)\n",
    "val_dataset = Dataset(X_val, y_val, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (270467018.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 20\u001b[0;36m\u001b[0m\n\u001b[0;31m    num_classes = 1,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "##### RNN #####\n",
    "###############\n",
    "\n",
    "# Initialize W&B \n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project='RNN-Classifier',\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config= dict(\n",
    "    batch_size = 50,\n",
    "    transaction_size = transaction_size,\n",
    "    embedding_dim = 32,\n",
    "    hidden_dim = 64,\n",
    "    num_layers = 1,\n",
    "    device = device,\n",
    "    batch_first = True,\n",
    "    fc_hidden_dim = 64,\n",
    "    num_classes = 1,\n",
    "    num_epochs = 200,\n",
    "    learning_rate = 5e-4,\n",
    "    weight_decay = 0.0, \n",
    "    lr_update_step = None,\n",
    "    log_step = 100,\n",
    "    lr_step_decay = False,\n",
    "    gamma = 0.85,\n",
    "    )\n",
    ")\n",
    "# initialize config\n",
    "config = wandb.config\n",
    "\n",
    "# Divide train and test dataset into batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=X_test.shape[0], shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=X_val.shape[0], shuffle=True, drop_last=True)\n",
    "\n",
    "# Check whether data is splitted correctly -> X_.shape: (batch, seq, encoding), y_.shape: (batch)\n",
    "# for i, (X_, y_) in enumerate(train_loader): \n",
    "    # print(X_.shape, y_.shape)\n",
    "    # print(X_[:10,:])\n",
    "    # print(y_[:10])\n",
    "\n",
    "model = RNN(config.transaction_size, config.embedding_dim, config.hidden_dim, config.num_layers, config.device,\n",
    "            num_classes = config.num_classes, batch_first = config.batch_first, fc_hidden_dim = config.fc_hidden_dim)\n",
    "model.to(device)\n",
    "\n",
    "# for i, (X_, y_) in enumerate(train_loader):\n",
    "#     out = model(X_)\n",
    "#     break\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#         # print (name, param.data)\n",
    "#         print (name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe initial performance of the model without any training\n",
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LSTM settings\n",
    "# embedding_dim = 64\n",
    "# hidden_dim = 64\n",
    "# transaction_size = 312\n",
    "# num_layers = 2\n",
    "\n",
    "# # parameter setting\n",
    "# num_epochs = 50\n",
    "# batch_size = 100\n",
    "# use_gpu = (device.type == 'cuda')\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# # Divide training dataset into batches\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# # Check whether data is splitted correctly -> X_.shape: (batch, seq, encoding), y_.shape: (batch)\n",
    "# for i, (X_, y_) in enumerate(train_loader): \n",
    "#     print(X_.shape, y_.shape)\n",
    "#     print(i)\n",
    "\n",
    "# model = LSTM(embedding_dim, hidden_dim, transaction_size, num_layers, batch_size, use_gpu)\n",
    "\n",
    "# model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight = (num_neg_seq/num_pos_seq) * torch.ones([1]).to(config.device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate, betas=(0.9, 0.999),\n",
    "                             eps=1e-8, weight_decay=config.weight_decay, amsgrad=False)  \n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train(model, train_loader, val_loader, criterion, optimizer, scheduler, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model -> no need to compute gradients (for memory efficiency)\n",
    "test(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
